Abstract,Link
"This paper explores the link between education and the decision to start
smoking as well as the decision to quit smoking. Data is gathered from IPUMS
CPS and Centers for Disease Control and Prevention. Probit analysis (with the
use of probability weight and robust standard error) indicates that every
additional year of education will reduce the 2.3 percentage point of the
smoking probability and will add 3.53 percentage point in quitting likelihood,
holding home restriction, public restriction, cigarette price, family income,
age, gender, race, and ethnicity constant. I believe that tobacco epidemic is a
serious global issue that may be mitigated by using careful regulations on
smoking restriction and education.",http://arxiv.org/abs/2011.14834v1
"Anonymized electronic medical records are an increasingly popular source of
research data. However, these datasets often lack race and ethnicity
information. This creates problems for researchers modeling human disease, as
race and ethnicity are powerful confounders for many health exposures and
treatment outcomes; race and ethnicity are closely linked to
population-specific genetic variation. We showed that deep neural networks
generate more accurate estimates for missing racial and ethnic information than
competing methods (e.g., logistic regression, random forest). RIDDLE yielded
significantly better classification performance across all metrics that were
considered: accuracy, cross-entropy loss (error), and area under the curve for
receiver operating characteristic plots (all $p < 10^{-6}$). We made specific
efforts to interpret the trained neural network models to identify, quantify,
and visualize medical features which are predictive of race and ethnicity. We
used these characterizations of informative features to perform a systematic
comparison of differential disease patterns by race and ethnicity. The fact
that clinical histories are informative for imputing race and ethnicity could
reflect (1) a skewed distribution of blue- and white-collar professions across
racial and ethnic groups, (2) uneven accessibility and subjective importance of
prophylactic health, (3) possible variation in lifestyle, such as dietary
habits, and (4) differences in background genetic variation which predispose to
diseases.",http://arxiv.org/abs/1707.01623v2
"Open Source Software (OSS) projects are typically the result of collective
efforts performed by developers with different backgrounds. Although the
quality of developers' contributions should be the only factor influencing the
evaluation of the contributions to OSS projects, recent studies have shown that
diversity issues are correlated with the acceptance or rejection of developers'
contributions. This paper assists this emerging state-of-the-art body on
diversity research with the first empirical study that analyzes how developers'
perceptible race and ethnicity relates to the evaluation of the contributions
in OSS. We performed a large-scale quantitative study of OSS projects in
GitHub. We extracted the developers' perceptible race and ethnicity from their
names in GitHub using the Name-Prism tool and applied regression modeling of
contributions (i.e, pull requests) data from GHTorrent and GitHub. We observed
that among the developers whose perceptible race and ethnicity was captured by
the tool, only 16.56% were perceptible as Non-White developers; contributions
from perceptible White developers have about 6-10% higher odds of being
accepted when compared to contributions from perceptible Non-White developers;
and submitters with perceptible non-white races and ethnicities are more likely
to get their pull requests accepted when the integrator is estimated to be from
their same race and ethnicity rather than when the integrator is estimated to
be White. Our initial analysis shows a low number of Non-White developers
participating in OSS. Furthermore, the results from our regression analysis
lead us to believe that there may exist differences between the evaluation of
the contributions from different perceptible races and ethnicities. Thus, our
findings reinforce the need for further studies on racial and ethnic diversity
in software engineering to foster healthier OSS communities.",http://arxiv.org/abs/2104.06143v1
"To answer questions about racial inequality and fairness, we often need a way
to infer race and ethnicity from names. One way to infer race and ethnicity
from names is by relying on the Census Bureau's list of popular last names. The
list, however, suffers from at least three limitations: 1. it only contains
last names, 2. it only includes popular last names, and 3. it is updated once
every 10 years. To provide better generalization, and higher accuracy when
first names are available, we model the relationship between characters in a
name and race and ethnicity using various techniques. A model using Long
Short-Term Memory works best with out-of-sample accuracy of .85. The
best-performing last-name model achieves out-of-sample accuracy of .81. To
illustrate the utility of the models, we apply them to campaign finance data to
estimate the share of donations made by people of various racial groups, and to
news data to estimate the coverage of various races and ethnicities in the
news.",http://arxiv.org/abs/1805.02109v2
"A common practice in evidence-based decision-making uses estimates of
conditional probabilities P(y|x) obtained from research studies to predict
outcomes y on the basis of observed covariates x. Given this information,
decisions are then based on the predicted outcomes. Researchers commonly assume
that the predictors used in the generation of the evidence are the same as
those used in applying the evidence: i.e., the meaning of x in the two
circumstances is the same. This may not be the case in real-world settings.
Across a wide-range of settings, ranging from clinical practice or education
policy, demographic attributes (e.g., age, race, ethnicity) are often
classified differently in research studies than in decision settings. This
paper studies identification in such settings. We propose a formal framework
for prediction with what we term differential covariate classification (DCC).
Using this framework, we analyze partial identification of probabilistic
predictions and assess how various assumptions influence the identification
regions. We apply the findings to a range of settings, focusing mainly on
differential classification of individuals' race and ethnicity in clinical
medicine. We find that bounds on P(y|x) can be wide, and the information needed
to narrow them available only in special cases. These findings highlight an
important problem in using evidence in decision making, a problem that has not
yet been fully appreciated in debates on classification in public policy and
medicine.",http://arxiv.org/abs/2501.02318v1
"There is active debate over whether to consider patient race and ethnicity
when estimating disease risk. By accounting for race and ethnicity, it is
possible to improve the accuracy of risk predictions, but there is concern that
their use may encourage a racialized view of medicine. In diabetes risk models,
despite substantial gains in statistical accuracy from using race and
ethnicity, the gains in clinical utility are surprisingly modest. These modest
clinical gains stem from two empirical patterns: first, the vast majority of
individuals receive the same screening recommendation regardless of whether
race or ethnicity are included in risk models; and second, for those who do
receive different screening recommendations, the difference in utility between
screening and not screening is relatively small. Our results are based on broad
statistical principles, and so are likely to generalize to many other
risk-based clinical decisions.",http://arxiv.org/abs/2306.10220v1
"Computational social science studies often contextualize content analysis
within standard demographics. Since demographics are unavailable on many social
media platforms (e.g. Twitter) numerous studies have inferred demographics
automatically. Despite many studies presenting proof of concept inference of
race and ethnicity, training of practical systems remains elusive since there
are few annotated datasets. Existing datasets are small, inaccurate, or fail to
cover the four most common racial and ethnic groups in the United States. We
present a method to identify self-reports of race and ethnicity from Twitter
profile descriptions. Despite errors inherent in automated supervision, we
produce models with good performance when measured on gold standard self-report
survey data. The result is a reproducible method for creating large-scale
training resources for race and ethnicity.",http://arxiv.org/abs/2005.00635v2
"We use place of birth information from the Social Security Administration
linked to earnings data from the Longitudinal Employer-Household Dynamics
Program and detailed race and ethnicity data from the 2010 Census to study how
long-term earnings differentials vary by place of birth for different
self-identified race and ethnicity categories. We focus on foreign-born persons
from countries that are heavily Hispanic and from countries in the Middle East
and North Africa (MENA). We find substantial heterogeneity of long-term
earnings differentials within country of birth, some of which will be difficult
to detect when the reporting format changes from the current two-question
version to the new single-question version because they depend on
self-identifications that place the individual in two distinct categories
within the single-question format, specifically, Hispanic and White or Black,
and MENA and White or Black. We also study the USA-born children of these same
immigrants. Long-term earnings differences for the 2nd generation also vary as
a function of self-identified ethnicity and race in ways that changing to the
single-question format could affect.",http://arxiv.org/abs/2407.12775v1
"We model the COVID-19 coronavirus epidemic in China. We use early reported
case data to predict the cumulative number of reported cases to a final size.
The key features of our model are the timing of implementation of major public
policies restricting social movement, the identification and isolation of
unreported cases, and the impact of asymptomatic infectious cases.",http://arxiv.org/abs/2002.12298v1
"The multidisciplinary and socially anchored nature of Feminist Studies
presents unique challenges for bibliometric analysis, as this research area
transcends traditional disciplinary boundaries and reflects discussions from
feminist and LGBTQIA+ social movements. This paper proposes a novel approach
for identifying gender/sex related publications scattered across diverse
scientific disciplines. Using the Dimensions database, we employ bibliometric
techniques, natural language processing (NLP) and manual curation to compile a
dataset of scientific publications that allows for the analysis of Gender
Studies and its influence across different disciplines.
  This is achieved through a methodology that combines a core of specialized
journals with a comprehensive keyword search over titles. These keywords are
obtained by applying Topic Modeling (BERTopic) to the corpus of titles and
abstracts from the core. This methodological strategy, divided into two stages,
reflects the dynamic interaction between Gender Studies and its dialogue with
different disciplines. This hybrid system surpasses basic keyword search by
mitigating potential biases introduced through manual keyword enumeration.
  The resulting dataset comprises over 1.9 million scientific documents
published between 1668 and 2023, spanning four languages. This dataset enables
a characterization of Gender Studies in terms of addressed topics, citation and
collaboration dynamics, and institutional and regional participation. By
addressing the methodological challenges of studying ""more-than-disciplinary""
research areas, this approach could also be adapted to delineate other
conversations where disciplinary boundaries are difficult to disentangle.",http://arxiv.org/abs/2411.18306v1
"In 2018, the US Census Bureau designed a new data reconstruction and
re-identification attack and tested it against their 2010 data release. The
specific attack executed by the Bureau allows an attacker to infer the race and
ethnicity of respondents with average 75% precision for 85% of the respondents,
assuming that the attacker knows the correct age, sex, and address of the
respondents. They interpreted the attack as exceeding the Bureau's privacy
standards, and so introduced stronger privacy protections for the 2020 Census
in the form of the TopDown Algorithm (TDA). This paper demonstrates that race
and ethnicity can be inferred from the TDA-protected census data with
substantially better precision and recall, using less prior knowledge: only the
respondents' address. Race and ethnicity can be inferred with average 75%
precision for 98% of the respondents, and can be inferred with 100% precision
for 11% of the respondents. The inference is done by simply assuming that the
race/ethnicity of the respondent is that of the majority race/ethnicity for the
respondent's census block. The conclusion to draw from this simple
demonstration is NOT that the Bureau's data releases lack adequate privacy
protections. Indeed it is the purpose of the data releases to allow this kind
of inference. The problem, rather, is that the Bureau's criteria for measuring
privacy is flawed and overly pessimistic.",http://arxiv.org/abs/2202.04872v2
"The design of data-driven dashboards that inform municipalities on ongoing
changes in infections within their community is addressed in this research.
Daily reports of Covid-19 infections published by the state of Wisconsin as the
initial surge in the pandemic ensued during the October 2020 to September 2021
time frame is considered as a case study. Of particular interest is the
identification of regions and population groups distinguished by race and
ethnicity that may be experiencing a disproportional rate of infections over
time. This study integrates the municipality-level daily positive cases that
are disaggregated by race and ethnicity and population size data derived from
the US Census Bureau. The goal is to present timely data-driven information in
a manner that is accessible to the general population, is relatable to the
constituents and promotes community engagement in managing and mitigating the
infections. A statistical metric referred to as the rank difference and its
persistence over time is used to capture the disproportional incidence of
Covid-19 positive cases on particular race and ethnic groups in relation to
their population size. A persistence index is derived to identify regions that
continually exhibit positive rank differences on a daily time scale and
indicate disparity in disease incidence. The analysis leads to the
identification that several municipalities in Wisconsin that are located in
regions of low population and away from the denser urban centers are those that
continue to exhibit disparity in the infection rates for Black/African American
and Hispanic/Latino population groups. Examples of a dashboard that can be
utilized to capture both aggregate level and temporal patterns of Covid-19
infections are presented.",http://arxiv.org/abs/2211.12583v1
"The research question this report addresses is: how, and to what extent,
those directly involved with the design, development and employment of a
specific black box algorithm can be certain that it is not unlawfully
discriminating (directly and/or indirectly) against particular persons with
protected characteristics (e.g. gender, race and ethnicity)?",http://arxiv.org/abs/1604.07180v1
"Income inequality between different races in the U.S. is especially large.
This difference is even larger when gender is involved. In a complementary
study, we have developed a dynamic microeconomic model accurately describing
the evolution of male and female incomes since 1930. Here, we extend our
analysis and model the disparity between black and white population in the
U.S., separately for males and females. Unfortunately, income microdata
provided by the U.S. Census Bureau for other races and ethnic groups are not
time compatible or too short for modelling purposes. We are forced to constrain
our analysis to black and white population, but all principal results can be
extrapolated to other races and ethnicities. Our analysis shows that black
females and white males are two poles of the overall income inequality. The
prediction of income distribution for two extreme cases with one model is the
main challenge of this study.",http://arxiv.org/abs/2007.06530v1
"This paper presents raceBERT -- a transformer-based model for predicting race
and ethnicity from character sequences in names, and an accompanying python
package. Using a transformer-based model trained on a U.S. Florida voter
registration dataset, the model predicts the likelihood of a name belonging to
5 U.S. census race categories (White, Black, Hispanic, Asian & Pacific
Islander, American Indian & Alaskan Native). I build on Sood and Laohaprapanon
(2018) by replacing their LSTM model with transformer-based models (pre-trained
BERT model, and a roBERTa model trained from scratch), and compare the results.
To the best of my knowledge, raceBERT achieves state-of-the-art results in race
prediction using names, with an average f1-score of 0.86 -- a 4.1% improvement
over the previous state-of-the-art, and improvements between 15-17% for
non-white names.",http://arxiv.org/abs/2112.03807v3
"We provide the largest compiled publicly available dictionaries of first,
middle, and last names for the purpose of imputing race and ethnicity using,
for example, Bayesian Improved Surname Geocoding (BISG). The dictionaries are
based on the voter files of six Southern states that collect self-reported
racial data upon voter registration. Our data cover a much larger scope of
names than any comparable dataset, containing roughly one million first names,
1.1 million middle names, and 1.4 million surnames. Individuals are categorized
into five mutually exclusive racial and ethnic groups -- White, Black,
Hispanic, Asian, and Other -- and racial/ethnic counts by name are provided for
every name in each dictionary. Counts can then be normalized row-wise or
column-wise to obtain conditional probabilities of race given name or name
given race. These conditional probabilities can then be deployed for imputation
in a data analytic task for which ground truth racial and ethnic data is not
available.",http://arxiv.org/abs/2208.12443v1
"Public engagement (PE) initiatives can lead to a long term public support of
science. However most of the real impact of PE initiatives within the context
of long-term science policy is not completely understood. An examination of the
National Aeronautics and Space Administration's (NASA) Hubble Space Telescope,
James Webb Space Telescope, and International Sun-Earth Explorer 3 reveal how
large grassroots movements led by citizen scientists and space aficionados can
have profound effects on public policy. We explore the role and relevance of
public grassroots movements in the policy of space astronomy initiatives,
present some recent cases which illustrate policy decisions involving broader
interest groups, and consider new avenues of PE including crowdfunding and
crowdsourcing.",http://arxiv.org/abs/1408.4987v2
"Bayesian Improved Surname Geocoding (BISG) is a ubiquitous tool for
predicting race and ethnicity using an individual's geolocation and surname.
Here we demonstrate that statistical dependence of surname and geolocation
within racial/ethnic categories in the United States results in biases for
minority subpopulations, and we introduce a raking-based improvement. Our
method augments the data used by BISG--distributions of race by geolocation and
race by surname--with the distribution of surname by geolocation obtained from
state voter files. We validate our algorithm on state voter registration lists
that contain self-identified race/ethnicity.",http://arxiv.org/abs/2304.09126v3
"Fairness-aware statistical learning is critical for data-driven
decision-making to mitigate discrimination against protected attributes, such
as gender, race, and ethnicity. This is especially important for high-stake
decision-making, such as insurance underwriting and annuity pricing. This paper
proposes a new fairness-regularized principal component analysis - Fair PCA, in
the context of high-dimensional factor models. An efficient gradient descent
algorithm is constructed with adaptive selection criteria for hyperparameter
tuning. The Fair PCA is applied to mortality modelling to mitigate gender
discrimination in annuity pricing. The model performance has been validated
through both simulation studies and empirical data analysis.",http://arxiv.org/abs/2412.04663v1
"Health care decisions are increasingly informed by clinical decision support
algorithms, but these algorithms may perpetuate or increase racial and ethnic
disparities in access to and quality of health care. Further complicating the
problem, clinical data often have missing or poor quality racial and ethnic
information, which can lead to misleading assessments of algorithmic bias. We
present novel statistical methods that allow for the use of probabilities of
racial/ethnic group membership in assessments of algorithm performance and
quantify the statistical bias that results from error in these imputed group
probabilities. We propose a sensitivity analysis approach to estimating the
statistical bias that allows practitioners to assess disparities in algorithm
performance under a range of assumed levels of group probability error. We also
prove theoretical bounds on the statistical bias for a set of commonly used
fairness metrics and describe real-world scenarios where our theoretical
results are likely to apply. We present a case study using imputed race and
ethnicity from the Bayesian Improved Surname Geocoding (BISG) algorithm for
estimation of disparities in a clinical decision support algorithm used to
inform osteoporosis treatment. Our novel methods allow policy makers to
understand the range of potential disparities under a given algorithm even when
race and ethnicity information is missing and to make informed decisions
regarding the implementation of machine learning for clinical decision support.",http://arxiv.org/abs/2402.13391v2
"Using high-frequency donation records from a major medical crowdfunding site
and careful difference-in-difference analysis, we demonstrate that the 2020 BLM
surge decreased the fundraising gap between Black and non-Black beneficiaries
by around 50\%. The reduction is largely attributed to non-Black donors. Those
beneficiaries in counties with moderate BLM activities were most impacted. We
construct innovative instrumental variable approaches that utilize weekends and
rainfall to identify the global and local effects of BLM protests. Results
suggest a broad social movement has a greater influence on charitable-giving
behavior than a local event. Social media significantly magnifies the impact of
protests.",http://arxiv.org/abs/2310.14590v2
"Purpose: The Medical Imaging and Data Resource Center (MIDRC) open data
commons was launched to accelerate the development of artificial intelligence
(AI) algorithms to help address the COVID-19 pandemic. The purpose of this
study was to quantify longitudinal representativeness of the demographic
characteristics of the primary imaging dataset compared to the United States
general population (US Census) and COVID-19 positive case counts from the
Centers for Disease Control and Prevention (CDC). Approach: The Jensen Shannon
distance (JSD) was used to longitudinally measure the similarity of the
distribution of (1) all unique patients in the MIDRC data to the 2020 US Census
and (2) all unique COVID-19 positive patients in the MIDRC data to the case
counts reported by the CDC. The distributions were evaluated in the demographic
categories of age at index, sex, race, ethnicity, and the intersection of race
and ethnicity. Results: Representativeness the MIDRC data by ethnicity and the
intersection of race and ethnicity was impacted by the percentage of CDC case
counts for which data in these categories is not reported. The distributions by
sex and race have retained their level of representativeness over time.
Conclusion: The representativeness of the open medical imaging datasets in the
curated public data commons at MIDRC has evolved over time as both the number
of contributing institutions and overall number of subjects has grown. The use
of metrics such as the JSD support measurement of representativeness, one step
needed for fair and generalizable AI algorithm development.",http://arxiv.org/abs/2303.10501v1
"Using only 34 published tables, we reconstruct five variables (census block,
sex, age, race, and ethnicity) in the confidential 2010 Census person records.
Using the 38-bin age variable tabulated at the census block level, at most
20.1% of reconstructed records can differ from their confidential source on
even a single value for these five variables. Using only published data, an
attacker can verify that all records in 70% of all census blocks (97 million
people) are perfectly reconstructed. The tabular publications in Summary File 1
thus have prohibited disclosure risk similar to the unreleased confidential
microdata. Reidentification studies confirm that an attacker can, within blocks
with perfect reconstruction accuracy, correctly infer the actual census
response on race and ethnicity for 3.4 million vulnerable population uniques
(persons with nonmodal characteristics) with 95% accuracy, the same precision
as the confidential data achieve and far greater than statistical baselines.
The flaw in the 2010 Census framework was the assumption that aggregation
prevented accurate microdata reconstruction, justifying weaker disclosure
limitation methods than were applied to 2010 Census public microdata. The
framework used for 2020 Census publications defends against attacks that are
based on reconstruction, as we also demonstrate here. Finally, we show that
alternatives to the 2020 Census Disclosure Avoidance System with similar
accuracy (enhanced swapping) also fail to protect confidentiality, and those
that partially defend against reconstruction attacks (incomplete suppression
implementations) destroy the primary statutory use case: data for redistricting
all legislatures in the country in compliance with the 1965 Voting Rights Act.",http://arxiv.org/abs/2312.11283v1
"The ethical implications and social impacts of artificial intelligence have
become topics of compelling interest to industry, researchers in academia, and
the public. However, current analyses of AI in a global context are biased
toward perspectives held in the U.S., and limited by a lack of research,
especially outside the U.S. and Western Europe.
  This article summarizes the key findings of a literature review of recent
social science scholarship on the social impacts of AI and related technologies
in five global regions. Our team of social science researchers reviewed more
than 800 academic journal articles and monographs in over a dozen languages.
  Our review of the literature suggests that AI is likely to have markedly
different social impacts depending on geographical setting. Likewise,
perceptions and understandings of AI are likely to be profoundly shaped by
local cultural and social context.
  Recent research in U.S. settings demonstrates that AI-driven technologies
have a pattern of entrenching social divides and exacerbating social
inequality, particularly among historically-marginalized groups. Our literature
review indicates that this pattern exists on a global scale, and suggests that
low- and middle-income countries may be more vulnerable to the negative social
impacts of AI and less likely to benefit from the attendant gains.
  We call for rigorous ethnographic research to better understand the social
impacts of AI around the world. Global, on-the-ground research is particularly
critical to identify AI systems that may amplify social inequality in order to
mitigate potential harms. Deeper understanding of the social impacts of AI in
diverse social settings is a necessary precursor to the development,
implementation, and monitoring of responsible and beneficial AI technologies,
and forms the basis for meaningful regulation of these technologies.",http://arxiv.org/abs/1907.07892v1
"Comparing how different populations have suffered under COVID-19 is a core
part of ongoing investigations into how public policy and social inequalities
influence the number of and severity of COVID-19 cases. But COVID-19 incidence
can vary multifold from one subpopulation to another, including between
neighborhoods of the same city, making comparisons of case rates deceptive. At
the same time, although epidemiological heterogeneities are increasingly
well-represented in mathematical models of disease spread, fitting these models
to real data on case numbers presents a tremendous challenge, as does
interpreting the models to answer questions such as: Which public health
policies achieve the best outcomes? Which social sacrifices are most worth
making? Here we compare COVID-19 case-curves between different US states, by
clustering case surges between March 2020 and March 2021 into groups with
similar dynamics. We advance the hypothesis that each surge is driven by a
subpopulation of COVID-19 contacting individuals, and make detecting the size
of that population a step within our clustering algorithm. Clustering reveals
that case trajectories in each state conform to one of a small number (4-6) of
archetypal dynamics. Our results suggest that while the spread of COVID-19 in
different states is heterogeneous, there are underlying universalities in the
spread of the disease that may yet be predictable by models with reduced
mathematical complexity. These universalities also prove to be surprisingly
robust to school closures, which we choose as a common, but high social cost,
public health measure.",http://arxiv.org/abs/2211.09010v1
"We examine the way race and racial categories are adopted in algorithmic
fairness frameworks. Current methodologies fail to adequately account for the
socially constructed nature of race, instead adopting a conceptualization of
race as a fixed attribute. Treating race as an attribute, rather than a
structural, institutional, and relational phenomenon, can serve to minimize the
structural aspects of algorithmic unfairness. In this work, we focus on the
history of racial categories and turn to critical race theory and sociological
work on race and ethnicity to ground conceptualizations of race for fairness
research, drawing on lessons from public health, biomedical research, and
social survey research. We argue that algorithmic fairness researchers need to
take into account the multidimensionality of race, take seriously the processes
of conceptualizing and operationalizing race, focus on social processes which
produce racial inequality, and consider perspectives of those most affected by
sociotechnical systems.",http://arxiv.org/abs/1912.03593v1
"In a recent study by Ginther et al., the probability of receiving a U.S.
National Institutes of Health (NIH) RO1 award was related to the applicant's
race/ethnicity. The results indicate black/African-American applicants were 10%
less likely than white peers to receive an award, after controlling for
background and qualifications. It has generated a widespread debate regarding
the unfairness of the NIH grant review process and its correction. In this
paper, the work by Ginther et al. was augmented by pairing analysis,
axiomatically-individualized productivity and normalized funding success
measurement. Although there are racial differences in R01 grant success rates,
normalized figures of merit for funding success explain the discrepancy. The
suggested ""leverage points for policy intervention"" are in question and require
deeper and more thorough investigations. Further adjustments in policies to
remove racial disparity should be made more systematically for equal
opportunity, rather than being limited to the NIH review process.",http://arxiv.org/abs/1112.3944v1
"Feature selection is a prevalent data preprocessing paradigm for various
learning tasks. Due to the expensive cost of acquiring supervision information,
unsupervised feature selection sparks great interests recently. However,
existing unsupervised feature selection algorithms do not have fairness
considerations and suffer from a high risk of amplifying discrimination by
selecting features that are over associated with protected attributes such as
gender, race, and ethnicity. In this paper, we make an initial investigation of
the fairness-aware unsupervised feature selection problem and develop a
principled framework, which leverages kernel alignment to find a subset of
high-quality features that can best preserve the information in the original
feature space while being minimally correlated with protected attributes.
Specifically, different from the mainstream in-processing debiasing methods,
our proposed framework can be regarded as a model-agnostic debiasing strategy
that eliminates biases and discrimination before downstream learning algorithms
are involved. Experimental results on multiple real-world datasets demonstrate
that our framework achieves a good trade-off between utility maximization and
fairness promotion.",http://arxiv.org/abs/2106.02216v1
"Systemic bias with respect to gender, race and ethnicity, often unconscious,
is prevalent in datasets involving choices among individuals. Consequently,
society has found it challenging to alleviate bias and achieve diversity in a
way that maintains meritocracy in such settings. We propose (a) a novel
optimization approach based on optimally flipping outcome labels and training
classification models simultaneously to discover changes to be made in the
selection process so as to achieve diversity without significantly affecting
meritocracy, and (b) a novel implementation tool employing optimal
classification trees to provide insights on which attributes of individuals
lead to flipping of their labels, and to help make changes in the current
selection processes in a manner understandable by human decision makers. We
present case studies on three real-world datasets consisting of parole,
admissions to the bar and lending decisions, and demonstrate that the price of
diversity is low and sometimes negative, that is we can modify our selection
processes in a way that enhances diversity without affecting meritocracy
significantly, and sometimes improving it.",http://arxiv.org/abs/2107.03900v1
"In recent years, monitoring hate speech and offensive language on social
media platforms has become paramount due to its widespread usage among all age
groups, races, and ethnicities. Consequently, there have been substantial
research efforts towards automated detection of such content using Natural
Language Processing (NLP). While successfully filtering textual data, no
research has focused on detecting hateful content in multimedia data. With
increased ease of data storage and the exponential growth of social media
platforms, multimedia content proliferates the internet as much as text data.
Nevertheless, it escapes the automatic filtering systems. Hate speech and
offensiveness can be detected in multimedia primarily via three modalities,
i.e., visual, acoustic, and verbal. Our preliminary study concluded that the
most essential features in classifying hate speech would be the speaker's
emotional state and its influence on the spoken words, therefore limiting our
current research to these modalities. This paper proposes the first multimodal
deep learning framework to combine the auditory features representing emotion
and the semantic features to detect hateful content. Our results demonstrate
that incorporating emotional attributes leads to significant improvement over
text-based models in detecting hateful multimedia content. This paper also
presents a new Hate Speech Detection Video Dataset (HSDVD) collected for the
purpose of multimodal learning as no such dataset exists today.",http://arxiv.org/abs/2202.06218v1
"In the absence of sensitive race and ethnicity data, researchers, regulators,
and firms alike turn to proxies. In this paper, I train a Bidirectional Long
Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data
from all 50 US states and create an ensemble that achieves up to 36.8% higher
out of sample (OOS) F1 scores than the best performing machine learning models
in the literature. Additionally, I construct the most comprehensive database of
first and surname distributions in the US in order to improve the coverage and
accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved
Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality
benchmark dataset in order to fairly compare existing models and aid future
model developers.",http://arxiv.org/abs/2307.08496v2
"The drug overdose crisis in the United States continues to intensify.
Fatalities have increased five-fold since 1999 reaching a record high of
108,000 deaths in 2021. The epidemic has unfolded through distinct waves of
different drug types, uniquely impacting various age, gender, race and ethnic
groups in specific geographical areas. One major challenge in designing
effective interventions is the forecasting of age-specific overdose patterns at
the local level so that prevention and preparedness can be effectively
delivered. We develop a forecasting method that assimilates observational data
obtained from the CDC WONDER database with an age-structured model of addiction
and overdose mortality. We apply our method nationwide and to three select
areas: Los Angeles County, Cook County and the five boroughs of New York City,
providing forecasts of drug-overdose mortality and estimates of relevant
epidemiological quantities, such as mortality and age-specific addiction rates.",http://arxiv.org/abs/2309.14452v1
"We contribute empirical and conceptual insights regarding the roles of
digital labor platforms in online freelancing, focusing attention to social
identities such as gender, race, ethnicity, and occupation. Findings highlight
how digital labor platforms reinforce and exacerbate identity-based
stereotypes, bias and expectations in online freelance work. We focus on online
freelancing as this form of working arrangement is becoming more prevalent.
Online freelancing also relies on the market-making power of digital platforms
to create an online labor market. Many see this as one likely future of work
with less bias. Others worry that labor platforms' market power allows them to
embed known biases into new working arrangements: a platformization of
inequality. Drawing on data from 108 online freelancers, we discuss six
findings: 1) female freelance work is undervalued; 2) gendered occupational
expectations; 3) gendered treatment; 4) shared expectations of differential
values; 5) racial stereotypes and expectations; and 6) race and ethnicity as an
asset. We discuss the role of design in the platformization and visibility of
social identity dimensions and the implications of the reinforced identity
perceptions and marginalization in digital labor platforms.",http://arxiv.org/abs/2309.16887v1
"Tools, models and statistical methods for signal processing and medical image
analysis and training deep learning models to create research prototypes for
eventual clinical applications are of special interest to the biomedical
imaging community. But material and optical properties of biological tissues
are complex and not easily captured by imaging devices. Added complexity can be
introduced by datasets with underrepresentation of medical images from races
and ethnicities for deep learning, and limited knowledge about the regulatory
framework needed for commercialization and safety of emerging Artificial
Intelligence (AI) and Machine Learning (ML) technologies for medical image
analysis. This extended version of the workshop paper presented at the special
session of the 2022 IEEE 19th International Symposium on Biomedical Imaging,
describes strategy and opportunities by University of California professors
engaged in machine learning (section I) and clinical research (section II), the
Office of Science and Engineering Laboratories (OSEL) section III, and
officials at the US FDA in Center for Devices & Radiological Health (CDRH)
section IV. Performance evaluations of AI/ML models of skin (RGB), tissue
biopsy (digital pathology), and lungs and kidneys (Magnetic Resonance, X-ray,
Computed Tomography) medical images for regulatory evaluations and real-world
deployment are discussed.",http://arxiv.org/abs/2312.13333v1
"This paper investigates gaps in access to and the cost of housing credit by
race and ethnicity using the near universe of U.S. mortgage applications. Our
data contain borrower creditworthiness variables that have historically been
absent from industry-wide application data and that are likely to affect
application approval and loan pricing. We find large unconditional disparities
in approval and pricing between racial and ethnic groups. After conditioning on
key elements of observable borrower creditworthiness, these disparities are
smaller but remain economically meaningful. Sensitivity analysis indicates that
omitted factors as predictive of approval/pricing and race/ethnicity as credit
score can explain some of the pricing disparities but cannot explain the
approval disparities. Taken together, our results suggest that credit score,
income, and down payment requirements significantly contribute to disparities
in mortgage access and affordability but that other systemic barriers are also
responsible for a large share of disparate outcomes in the mortgage market.",http://arxiv.org/abs/2405.00895v1
"Large participatory biomedical studies, studies that recruit individuals to
join a dataset, are gaining popularity and investment, especially for analysis
by modern AI methods. Because they purposively recruit participants, these
studies are uniquely able to address a lack of historical representation, an
issue that has affected many biomedical datasets. In this work, we define
representativeness as the similarity to a target population distribution of a
set of attributes and our goal is to mirror the U.S. population across
distributions of age, gender, race, and ethnicity. Many participatory studies
recruit at several institutions, so we introduce a computational approach to
adaptively allocate recruitment resources among sites to improve
representativeness. In simulated recruitment of 10,000-participant cohorts from
medical centers in the STAR Clinical Research Network, we show that our
approach yields a more representative cohort than existing baselines. Thus, we
highlight the value of computational modeling in guiding recruitment efforts.",http://arxiv.org/abs/2408.01375v1
"This study builds on person perception and human AI interaction (HAII)
theories to investigate how content and source cues, specifically race,
ethnicity, and nationality, affect judgments of AI-generated content in a
high-stakes self-presentation context: college applications. Results of a
pre-registered experiment with a nationally representative U.S. sample (N =
644) show that content heuristics, such as linguistic style, played a dominant
role in AI detection. Source heuristics, such as nationality, also emerged as a
significant factor, with international students more likely to be perceived as
using AI, especially when their statements included AI-sounding features.
Interestingly, Asian and Hispanic applicants were more likely to be judged as
AI users when labeled as domestic students, suggesting interactions between
racial stereotypes and AI detection. AI attribution led to lower perceptions of
personal statement quality and authenticity, as well as negative evaluations of
the applicant's competence, sociability, morality, and future success.",http://arxiv.org/abs/2412.18647v1
"Public policy decision making has become more complex and complicated in
recent times. Some authors have attributed this to the fact that public policy
decision makers now have more variables to consider in every decision more than
ever before. Others have argued that the rate of civilization, globalization
and information technology has made the public to be more enlightened and
abreast with the activities of government and so can oppose government
decisions if they are unfavourable. This tends to increase government need for
more and better information in order to satisfy the public. Consequently, this
paper examined the issue of fuel subsidy removal in Nigeria, the impact of the
policy on the public as well as the country and the role marketing principles
would have played if the Nigerian government had taken some time to investigate
what should be done, how it should be done and when it should be done. It also
proposed a roadmap for future policies that have direct implications for the
general public.",http://arxiv.org/abs/2404.17551v1
"Where science, design, business and art meet, COINs13 looks at the emerging
forces behind the phenomena of open-source, creative, entrepreneurial and
social movements. COINs13 combines a wide range of interdisciplinary fields
such as social network analysis, group dynamics, design and visualization,
information systems, collective action and the psychology and sociality of
collaboration. The COINs13 conference theme is Learning from the Swarm. The
papers in this volume explore what is relevant with regard to the innovative
powers of creative and civic swarms, what are the observable qualities of
virtual collaboration and mobilization, and how does the quest for global
cooperation affect local networks.",http://arxiv.org/abs/1308.1028v1
"Social tipping, where minorities trigger larger populations to engage in
collective action, has been suggested as one key aspect in addressing
contemporary global challenges. Here, we refine Granovetter's widely
acknowledged theoretical threshold model of collective behavior as a numerical
modelling tool for understanding social tipping processes and resolve issues
that so far have hindered such applications. Based on real-world observations
and social movement theory, we group the population into certain or potential
actors, such that -- in contrast to its original formulation -- the model
predicts non-trivial final shares of acting individuals. Then, we use a network
cascade model to explain and analytically derive that previously hypothesized
broad threshold distributions emerge if individuals become active via social
interaction. Thus, through intuitive parameters and low dimensionality our
refined model is adaptable to explain the likelihood of engaging in collective
behavior where social tipping like processes emerge as saddle-node bifurcations
and hysteresis.",http://arxiv.org/abs/1911.04126v2
"There is mounting public concern over the influence that AI based systems has
in our society. Coalitions in all sectors are acting worldwide to resist hamful
applications of AI. From indigenous people addressing the lack of reliable
data, to smart city stakeholders, to students protesting the academic
relationships with sex trafficker and MIT donor Jeffery Epstein, the
questionable ethics and values of those heavily investing in and profiting from
AI are under global scrutiny. There are biased, wrongful, and disturbing
assumptions embedded in AI algorithms that could get locked in without
intervention. Our best human judgment is needed to contain AI's harmful impact.
Perhaps one of the greatest contributions of AI will be to make us ultimately
understand how important human wisdom truly is in life on earth.",http://arxiv.org/abs/2107.14052v1
"Fairness-aware learning is a novel framework for classification tasks. Like
regular empirical risk minimization (ERM), it aims to learn a classifier with a
low error rate, and at the same time, for the predictions of the classifier to
be independent of sensitive features, such as gender, religion, race, and
ethnicity. Existing methods can achieve low dependencies on given samples, but
this is not guaranteed on unseen samples. The existing fairness-aware learning
algorithms employ different dependency measures, and each algorithm is
specifically designed for a particular one. Such diversity makes it difficult
to theoretically analyze and compare them. In this paper, we propose a general
framework for fairness-aware learning that uses f-divergences and that covers
most of the dependency measures employed in the existing methods. We introduce
a way to estimate the f-divergences that allows us to give a unified analysis
for the upper bound of the estimation error; this bound is tighter than that of
the existing convergence rate analysis of the divergence estimation. With our
divergence estimate, we propose a fairness-aware learning algorithm, and
perform a theoretical analysis of its generalization error. Our analysis
reveals that, under mild assumptions and even with enforcement of fairness, the
generalization error of our method is $O(\sqrt{1/n})$, which is the same as
that of the regular ERM. In addition, and more importantly, we show that, for
any f-divergence, the upper bound of the estimation error of the divergence is
$O(\sqrt{1/n})$. This indicates that our fairness-aware learning algorithm
guarantees low dependencies on unseen samples for any dependency measure
represented by an f-divergence.",http://arxiv.org/abs/1506.07721v1
"In this article, we draw on previous reports from physics, science education,
and women's studies to propose a more nuanced treatment of gender in physics
education research (PER). A growing body of PER examines gender differences in
participation, performance, and attitudes toward physics. We have three
critiques of this work: (1) it does not question whether the achievements of
men are the most appropriate standard, (2) individual experiences and student
identities are undervalued, and (3) the binary model of gender is not
questioned. Driven by these critiques, we propose a conception of gender that
is more up-to-date with other fields and discuss gender-as-performance as an
extended example. We also discuss work on the intersection of identities [e.g.,
gender with race and ethnicity, socioeconomic status, lesbian, gay, bisexual,
and transgender (LGBT) status], much of which has been conducted outside of
physics. Within PER, some studies examine the intersection of gender and race,
and identify the lack of a single identity as a key challenge of ""belonging"" in
physics. Acknowledging this complexity enables us to further critique what we
term a binary gender deficit model. This framework, which is implicit in much
of the gender-based PER, casts gender as a fixed binary trait and suggests that
women are deficient in characteristics necessary to succeed. Alternative models
of gender allow a greater range and fluidity of gender identities, and
highlight deficiencies in data that exclude women's experiences. We suggest new
investigations that diverge from this expanded gender framework in PER.",http://arxiv.org/abs/1507.05107v1
"In many application areas, predictive models are used to support or make
important decisions. There is increasing awareness that these models may
contain spurious or otherwise undesirable correlations. Such correlations may
arise from a variety of sources, including batch effects, systematic
measurement errors, or sampling bias. Without explicit adjustment, machine
learning algorithms trained using these data can produce poor out-of-sample
predictions which propagate these undesirable correlations. We propose a method
to pre-process the training data, producing an adjusted dataset that is
statistically independent of the nuisance variables with minimum information
loss. We develop a conceptually simple approach for creating an adjusted
dataset in high-dimensional settings based on a constrained form of matrix
decomposition. The resulting dataset can then be used in any predictive
algorithm with the guarantee that predictions will be statistically independent
of the group variable. We develop a scalable algorithm for implementing the
method, along with theory support in the form of independence guarantees and
optimality. The method is illustrated on some simulation examples and applied
to two case studies: removing machine-specific correlations from brain scan
data, and removing race and ethnicity information from a dataset used to
predict recidivism. That the motivation for removing undesirable correlations
is quite different in the two applications illustrates the broad applicability
of our approach.",http://arxiv.org/abs/1810.08255v2
"Physics and physics education in the United States suffer from severe (and,
in some cases, worsening) underrepresentation of Black, Latina/o, and Native
people of all genders and women of all races and ethnicities. This
underrepresentation is a symptom with multiple causes and myriad potential
solutions. In this paper, we describe an approach to addressing the causes of
underrepresentation through physics students' collective and continued
education about racism, sexism, other dimensions of marginalization, as well as
models of allyship and social change. Specifically, we focus on the efforts of
undergraduate students, graduate students, and postdocs who are members of a
student-run diversity-oriented organization in a physics department at a large,
selective, predominantly white university with high research activity. This
group's education was accomplished through quarterly Diversity Workshops. Here
we report on six Diversity Workshops that were co-designed and facilitated by
the authors. We describe the context, motivation, and goals of the workshops,
the theories underlying their design and implementation, and their content. In
addition, we discuss workshop attendance and suggest strategies for maintaining
high attendance in the future. Because the details of our workshops were
tailored to the specific needs and interests of a particular student
organization, our workshop agendas may not be widely applicable beyond our
local context. Therefore, we share our model, design principles, and
facilitation strategies in this paper.",http://arxiv.org/abs/1607.08184v2
"This survey article assesses and compares existing critiques of current
fairness-enhancing technical interventions into machine learning (ML) that draw
from a range of non-computing disciplines, including philosophy, feminist
studies, critical race and ethnic studies, legal studies, anthropology, and
science and technology studies. It bridges epistemic divides in order to offer
an interdisciplinary understanding of the possibilities and limits of hegemonic
computational approaches to ML fairness for producing just outcomes for
society's most marginalized. The article is organized according to nine major
themes of critique wherein these different fields intersect: 1) how ""fairness""
in AI fairness research gets defined; 2) how problems for AI systems to address
get formulated; 3) the impacts of abstraction on how AI tools function and its
propensity to lead to technological solutionism; 4) how racial classification
operates within AI fairness research; 5) the use of AI fairness measures to
avoid regulation and engage in ethics washing; 6) an absence of participatory
design and democratic deliberation in AI fairness considerations; 7) data
collection practices that entrench ""bias,"" are non-consensual, and lack
transparency; 8) the predatory inclusion of marginalized groups into AI
systems; and 9) a lack of engagement with AI's long-term social and ethical
outcomes. Drawing from these critiques, the article concludes by imagining
future ML fairness research directions that actively disrupt entrenched power
dynamics and structural injustices in society.",http://arxiv.org/abs/2205.04460v1
"Prediction of individual's race and ethnicity plays an important role in
social science and public health research. Examples include studies of racial
disparity in health and voting. Recently, Bayesian Improved Surname Geocoding
(BISG), which uses Bayes' rule to combine information from Census surname files
with the geocoding of an individual's residence, has emerged as a leading
methodology for this prediction task. Unfortunately, BISG suffers from two
Census data problems that contribute to unsatisfactory predictive performance
for minorities. First, the decennial Census often contains zero counts for
minority racial groups in the Census blocks where some members of those groups
reside. Second, because the Census surname files only include frequent names,
many surnames -- especially those of minorities -- are missing from the list.
To address the zero counts problem, we introduce a fully Bayesian Improved
Surname Geocoding (fBISG) methodology that accounts for potential measurement
error in Census counts by extending the naive Bayesian inference of the BISG
methodology to full posterior inference. To address the missing surname
problem, we supplement the Census surname data with additional data on last,
first, and middle names taken from the voter files of six Southern states where
self-reported race is available. Our empirical validation shows that the fBISG
methodology and name supplements significantly improve the accuracy of race
imputation across all racial groups, and especially for Asians. The proposed
methodology, together with additional name data, is available via the
open-source software WRU.",http://arxiv.org/abs/2205.06129v3
"Despite the great promise that machine learning has offered in many fields of
medicine, it has also raised concerns about potential biases and poor
generalization across genders, age distributions, races and ethnicities,
hospitals, and data acquisition equipment and protocols. In the current study,
and in the context of three brain diseases, we provide evidence which suggests
that when properly trained, machine learning models can generalize well across
diverse conditions and do not necessarily suffer from bias. Specifically, by
using multi-study magnetic resonance imaging consortia for diagnosing
Alzheimer's disease, schizophrenia, and autism spectrum disorder, we find that
well-trained models have a high area-under-the-curve (AUC) on subjects across
different subgroups pertaining to attributes such as gender, age, racial
groups, and different clinical studies and are unbiased under multiple fairness
metrics such as demographic parity difference, equalized odds difference, equal
opportunity difference etc. We find that models that incorporate multi-source
data from demographic, clinical, genetic factors and cognitive scores are also
unbiased. These models have better predictive AUC across subgroups than those
trained only with imaging features but there are also situations when these
additional features do not help.",http://arxiv.org/abs/2205.13421v2
"Characterizing the cumulative burden of COVID-19 by race/ethnicity is of the
utmost importance for public health researchers and policy makers in order to
design effective mitigation measures. This analysis is hampered, however, by
surveillance case data with substantial missingness in race and ethnicity
covariates. Worse yet, this missingness likely depends on the values of these
missing covariates, i.e. they are not missing at random (NMAR). We propose a
Bayesian parametric model that leverages joint information on spatial variation
in the disease and covariate missingness processes and can accommodate both MAR
and NMAR missingness. We show that the model is locally identifiable when the
spatial distribution of the population covariates is known and observed cases
can be associated with a spatial unit of observation. We also use a simulation
study to investigate the model's finite-sample performance. We compare our
model's performance on NMAR data against complete-case analysis and multiple
imputation (MI), both of which are commonly used by public health researchers
when confronted with missing categorical covariates. Finally, we model spatial
variation in cumulative COVID-19 incidence in Wayne County, Michigan using data
from the Michigan Department and Health and Human Services. The analysis
suggests that population relative risk estimates by race during the early part
of the COVID-19 pandemic in Michigan were understated for non-white residents
compared to white residents when cases missing race were dropped or had these
values imputed using MI.",http://arxiv.org/abs/2206.08161v2
"Critical voices within and beyond the scientific community have pointed to a
grave matter of concern regarding who is included in research and who is not.
Subsequent investigations have revealed an extensive form of sampling bias
across a broad range of disciplines that conduct human subjects research called
""WEIRD"": Western, Educated, Industrial, Rich, and Democratic. Recent work has
indicated that this pattern exists within human-computer interaction (HCI)
research, as well. How then does human-robot interaction (HRI) fare? And could
there be other patterns of sampling bias at play, perhaps those especially
relevant to this field of study? We conducted a systematic review of the
premier ACM/IEEE International Conference on Human-Robot Interaction
(2006-2022) to discover whether and how WEIRD HRI research is. Importantly, we
expanded our purview to other factors of representation highlighted by critical
work on inclusion and intersectionality as potentially underreported,
overlooked, and even marginalized factors of human diversity. Findings from 827
studies across 749 papers confirm that participants in HRI research also tend
to be drawn from WEIRD populations. Moreover, we find evidence of limited,
obscured, and possible misrepresentation in participant sampling and reporting
along key axes of diversity: sex and gender, race and ethnicity, age, sexuality
and family configuration, disability, body type, ideology, and domain
expertise. We discuss methodological and ethical implications for recruitment,
analysis, and reporting, as well as the significance for HRI as a base of
knowledge.",http://arxiv.org/abs/2304.08750v1
"Precision medicine aims to create biomedical solutions tailored to specific
factors that affect disease risk and treatment responses within the population.
The success of the genomics era and recent widespread availability of
electronic health records (EHR) has ushered in a new wave of genomic biobanks
connected to EHR databases (EHR-linked biobanks). This perspective aims to
discuss how race, ethnicity, and genetic ancestry are currently utilized to
study common disease variation through genetic association studies. Although
genetic ancestry plays a significant role in shaping the genetic landscape
underlying disease risk in humans, the overall risk of a disease is caused by a
complex combination of environmental, sociocultural, and genetic factors. When
using EHR-linked biobanks to interrogate underlying disease etiology, it is
also important to be aware of how the biases associated with commonly used
descent-associated concepts such as race and ethnicity can propagate to
downstream analyses. We intend for this resource to support researchers who
perform or analyze genetic association studies in the EHR-linked biobank
setting such as those involved in consortium-wide biobanking efforts. We
provide background on how race, ethnicity, and genetic ancestry play a role in
current association studies, highlight considerations where there is no
consensus about best practices, and provide transparency about the current
shortcomings.",http://arxiv.org/abs/2402.15696v1
"Measuring average differences in an outcome across racial or ethnic groups is
a crucial first step for equity assessments, but researchers often lack access
to data on individuals' races and ethnicities to calculate them. A common
solution is to impute the missing race or ethnicity labels using proxies, then
use those imputations to estimate the disparity. Conventional standard errors
mischaracterize the resulting estimate's uncertainty because they treat the
imputation model as given and fixed, instead of as an unknown object that must
be estimated with uncertainty. We propose a dual-bootstrap approach that
explicitly accounts for measurement uncertainty and thus enables more accurate
statistical inference, which we demonstrate via simulation. In addition, we
adapt our approach to the commonly used Bayesian Improved Surname Geocoding
(BISG) imputation algorithm, where direct bootstrapping is infeasible because
the underlying Census Bureau data are unavailable. In simulations, we find that
measurement uncertainty is generally insignificant for BISG except in
particular circumstances; bias, not variance, is likely the predominant source
of error. We apply our method to quantify the uncertainty of prevalence
estimates of common health conditions by race using data from the American
Family Cohort.",http://arxiv.org/abs/2403.06238v1
"Concerns about representation in computing within the U.S. have driven
numerous activities to broaden participation. Assessment of the impact of these
efforts and, indeed, a clear assessment of the actual ""problem"" being addressed
are limited by the nature of the most common data analysis which looks at the
representation of each population as a percentage of the number of students
graduating with a degree in computing. This use of a single metric cannot
adequately assess the impact of broadening participation efforts. First, this
approach fails to account for changing demographics of the undergraduate
population in terms of overall numbers and relative proportion of the Federally
designated gender, race, and ethnicity groupings. A second issue is that the
majority of literature on broadening participation in computing (BPC) reports
data on gender or on race/ethnicity, omitting data on students' intersectional
identities. This leads to an incorrect understanding of both the data and the
challenges we face as a field. In this paper we present several different
approaches to tracking the impact of BPC efforts. We make three
recommendations: 1) cohort-based analysis should be used to accurately show
student engagement in computing; 2) the field as a whole needs to adopt the
norm of always reporting intersectional data; 3) university demographic context
matters when looking at how well a CS department is doing to broaden
participation in computing, including longitudinal analysis of university
demographic shifts that impact the local demographics of computing.",http://arxiv.org/abs/2403.14708v1
"Image search and retrieval tasks can perpetuate harmful stereotypes, erase
cultural identities, and amplify social disparities. Current approaches to
mitigate these representational harms balance the number of retrieved items
across population groups defined by a small number of (often binary)
attributes. However, most existing methods overlook intersectional groups
determined by combinations of group attributes, such as gender, race, and
ethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel
metric that measures representation across intersectional groups. We develop
practical methods for estimating MPR, provide theoretical guarantees, and
propose optimization algorithms to ensure MPR in retrieval. We demonstrate that
existing methods optimizing for equal and proportional representation metrics
may fail to promote MPR. Crucially, our work shows that optimizing MPR yields
more proportional representation across multiple intersectional groups
specified by a rich function class, often with minimal compromise in retrieval
accuracy.",http://arxiv.org/abs/2407.08571v2
"AI fairness measurements, including tests for equal treatment, often take the
form of disaggregated evaluations of AI systems. Such measurements are an
important part of Responsible AI operations. These measurements compare system
performance across demographic groups or sub-populations and typically require
member-level demographic signals such as gender, race, ethnicity, and location.
However, sensitive member-level demographic attributes like race and ethnicity
can be challenging to obtain and use due to platform choices, legal
constraints, and cultural norms. In this paper, we focus on the task of
enabling AI fairness measurements on race/ethnicity for \emph{U.S. LinkedIn
members} in a privacy-preserving manner. We present the Privacy-Preserving
Probabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.
PPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse
LinkedIn survey sample of self-reported demographics, and privacy-enhancing
technologies like secure two-party computation and differential privacy to
enable meaningful fairness measurements while preserving member privacy. We
provide details of the PPRE method and its privacy guarantees. We then
illustrate sample measurement operations. We conclude with a review of open
research and engineering challenges for expanding our privacy-preserving
fairness measurement capabilities.",http://arxiv.org/abs/2409.04652v2
"There have been growing concerns around high-stake applications that rely on
models trained with biased data, which consequently produce biased predictions,
often harming the most vulnerable. In particular, biased medical data could
cause health-related applications and recommender systems to create outputs
that jeopardize patient care and widen disparities in health outcomes. A recent
framework titled Fairness via AI posits that, instead of attempting to correct
model biases, researchers must focus on their root causes by using AI to debias
data. Inspired by this framework, we tackle bias detection in medical curricula
using NLP models, including LLMs, and evaluate them on a gold standard dataset
containing 4,105 excerpts annotated by medical experts for bias from a large
corpus. We build on previous work by coauthors which augments the set of
negative samples with non-annotated text containing social identifier terms.
However, some of these terms, especially those related to race and ethnicity,
can carry different meanings (e.g., ""white matter of spinal cord""). To address
this issue, we propose the use of Word Sense Disambiguation models to refine
dataset quality by removing irrelevant sentences. We then evaluate fine-tuned
variations of BERT models as well as GPT models with zero- and few-shot
prompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for
bias detection, while fine-tuned BERT models generally perform well across all
evaluated metrics.",http://arxiv.org/abs/2409.07424v1
"Recent progress in generative AI, especially diffusion models, has
demonstrated significant utility in text-to-image synthesis. Particularly in
healthcare, these models offer immense potential in generating synthetic
datasets and training medical students. However, despite these strong
performances, it remains uncertain if the image generation quality is
consistent across different demographic subgroups. To address this critical
concern, we present the first comprehensive study on the fairness of medical
text-to-image diffusion models. Our extensive evaluations of the popular Stable
Diffusion model reveal significant disparities across gender, race, and
ethnicity. To mitigate these biases, we introduce FairDiffusion, an
equity-aware latent diffusion model that enhances fairness in both image
generation quality as well as the semantic correlation of clinical features. In
addition, we also design and curate FairGenMed, the first dataset for studying
the fairness of medical generative models. Complementing this effort, we
further evaluate FairDiffusion on two widely-used external medical datasets:
HAM10000 (dermatoscopic images) and CheXpert (chest X-rays) to demonstrate
FairDiffusion's effectiveness in addressing fairness concerns across diverse
medical imaging modalities. Together, FairDiffusion and FairGenMed
significantly advance research in fair generative learning, promoting equitable
benefits of generative AI in healthcare.",http://arxiv.org/abs/2412.20374v1
"Humanitarian challenges, including natural disasters, food insecurity,
climate change, racial and gender violence, environmental crises, the COVID-19
coronavirus pandemic, human rights violations, and forced displacements,
disproportionately impact vulnerable communities worldwide. According to UN
OCHA, 235 million people will require humanitarian assistance in 2021. Despite
these growing perils, there remains a notable paucity of data science research
to scientifically inform equitable public policy decisions for improving the
livelihood of at-risk populations. Scattered data science efforts exist to
address these challenges, but they remain isolated from practice and prone to
algorithmic harms concerning lack of privacy, fairness, interpretability,
accountability, transparency, and ethics. Biases in data-driven methods carry
the risk of amplifying inequalities in high-stakes policy decisions that impact
the livelihood of millions of people. Consequently, proclaimed benefits of
data-driven innovations remain inaccessible to policymakers, practitioners, and
marginalized communities at the core of humanitarian actions and global
development. To help fill this gap, we propose the Data-driven Humanitarian
Mapping Research Program, which focuses on developing novel data science
methodologies that harness human-machine intelligence for high-stakes public
policy and resilience planning.
  The proceedings of the 2nd Data-driven Humanitarian Mapping workshop at the
27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. August 15th,
2021",http://arxiv.org/abs/2109.00100v4
"Humanitarian challenges, including natural disasters, food insecurity,
climate change, racial and gender violence, environmental crises, the COVID-19
coronavirus pandemic, human rights violations, and forced displacements,
disproportionately impact vulnerable communities worldwide. According to UN
OCHA, 235 million people will require humanitarian assistance in 2021 . Despite
these growing perils, there remains a notable paucity of data science research
to scientifically inform equitable public policy decisions for improving the
livelihood of at-risk populations. Scattered data science efforts exist to
address these challenges, but they remain isolated from practice and prone to
algorithmic harms concerning lack of privacy, fairness, interpretability,
accountability, transparency, and ethics. Biases in data-driven methods carry
the risk of amplifying inequalities in high-stakes policy decisions that impact
the livelihood of millions of people. Consequently, proclaimed benefits of
data-driven innovations remain inaccessible to policymakers, practitioners, and
marginalized communities at the core of humanitarian actions and global
development. To help fill this gap, we propose the Data-driven Humanitarian
Mapping Research Program, which focuses on developing novel data science
methodologies that harness human-machine intelligence for high-stakes public
policy and resilience planning.
  The proceedings of the 1st Data-driven Humanitarian Mapping workshop at the
26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, August 24th,
2020.",http://arxiv.org/abs/2109.00435v3
"Artificial intelligence (AI) can potentially transform global health, but
algorithmic bias can exacerbate social inequities and disparity. Trustworthy AI
entails the intentional design to ensure equity and mitigate potential biases.
To advance trustworthy AI in global health, we convened a workshop on Fairness
in Machine Intelligence for Global Health (FairMI4GH). The event brought
together a global mix of experts from various disciplines, community health
practitioners, policymakers, and more. Topics covered included managing AI bias
in socio-technical systems, AI's potential impacts on global health, and
balancing data privacy with transparency. Panel discussions examined the
cultural, political, and ethical dimensions of AI in global health. FairMI4GH
aimed to stimulate dialogue, facilitate knowledge transfer, and spark
innovative solutions. Drawing from NIST's AI Risk Management Framework, it
provided suggestions for handling AI risks and biases. The need to mitigate
data biases from the research design stage, adopt a human-centered approach,
and advocate for AI transparency was recognized. Challenges such as updating
legal frameworks, managing cross-border data sharing, and motivating developers
to reduce bias were acknowledged. The event emphasized the necessity of diverse
viewpoints and multi-dimensional dialogue for creating a fair and ethical AI
framework for equitable global health.",http://arxiv.org/abs/2309.05088v1
"Social media platforms hold valuable insights, yet extracting essential
information can be challenging. Traditional top-down approaches often struggle
to capture critical signals in rapidly changing events. As global events evolve
swiftly, social media narratives, including instances of disinformation, become
significant sources of insights. To address the need for an inductive strategy,
we explore a niche social media platform GAB and an established messaging
service Telegram, to develop methodologies applicable on a broader scale. This
study investigates narrative evolution on these platforms using quantitative
corpus-based discourse analysis techniques. Our approach is a novel mode to
study multiple social media domains to distil key information which may be
obscured otherwise, allowing for useful and actionable insights. The paper
details the technical and methodological aspects of gathering and preprocessing
GAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying
crucial nouns and verbs for deeper exploration. Empirically, this approach is
applied to a case study of a well defined event that had global impact: the
2023 Wagner mutiny. The main findings are: (1) the time line can be
deconstructed to provide useful data features allowing for improved
interpretation; (2) a methodology is applied which provides a basis for
generalization. The key contribution is an approach, that in some cases,
provides the ability to capture the dynamic narrative shifts over time with
elevated confidence. The approach can augment near-real-time assessment of key
social movements, allowing for informed governance choices. This research is
important because it lays out a useful methodology for time series relevant
info-culling, which can enable proactive modes for positive social engagement.",http://arxiv.org/abs/2403.07090v1
"Starting with the neo-Bayesian revival of the 1950s, many statisticians
argued that it was inappropriate to use Bayesian methods, and in particular
subjective Bayesian methods in governmental and public policy settings because
of their reliance upon prior distributions. But the Bayesian framework often
provides the primary way to respond to questions raised in these settings and
the numbers and diversity of Bayesian applications have grown dramatically in
recent years. Through a series of examples, both historical and recent, we
argue that Bayesian approaches with formal and informal assessments of priors
AND likelihood functions are well accepted and should become the norm in public
settings. Our examples include census-taking and small area estimation, US
election night forecasting, studies reported to the US Food and Drug
Administration, assessing global climate change, and measuring potential
declines in disability among the elderly.",http://arxiv.org/abs/1108.2177v1
"This research analyzes the effects of U.S. science and technology policy on
the technological performance of organizations in a global strategic alliance
network. During the mid-1980s the U.S. semiconductor industry appeared to be
collapsing. Industry leaders and policymakers moved to support and protect U.S.
firms by creating a program called Sematech. While many scholars regard
Sematech as a success, how the program succeeded remains unclear. This study
re-contextualizes Sematech as a network administrative organization which
lowered cooperation costs and enhanced resource combination for innovation at
the cutting edge. This study combines network analysis and longitudinal
regression techniques to test the effects of public policy on organizational
network position and technological performance in an unbalanced panel of
semiconductor firms between 1986 and 2001. This research suggests governments
might achieve policy through inter-organizational innovations aimed at the
development and administration of robust governance networks.",http://arxiv.org/abs/1903.07730v2
"Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.",http://arxiv.org/abs/2204.07603v2
"Social movements are dominated by storytelling, as narratives play a key role
in how communities involved in these movements shape their identities. Thus,
recognizing the accepted narratives of different communities is central to
understanding social movements. In this context, journalists face the challenge
of making sense of these emerging narratives in social media when they seek to
report social protests. Thus, they would benefit from support tools that allow
them to identify and explore such narratives. In this work, we propose a
narrative extraction algorithm from social media that incorporates the concept
of community acceptance. Using our method, we study the 2021 Cuban protests and
characterize five relevant communities. The extracted narratives differ in both
structure and content across communities. Our work has implications in the
study of social movements, intelligence analysis, computational journalism, and
misinformation research.",http://arxiv.org/abs/2208.04465v1
"Many socioeconomic studies have been carried out to explain the phenomenon of
gentrification. Although results of these works shed light on the process
around this phenomenon, a perspective which focuses on the relationship between
city form and gentrification is still missing. With this paper we try to
address this gap by studying and comparing, through classic methods of
mathematical statistics, morphological features of five London gentrified
neighbourhoods. Outcomes confirm that areas which have undergone gentrification
display similar and recognizable morphological patterns in terms of urban type
and geographical location of main and local roads as well as businesses. These
initial results confirm findings from previous research in urban sociology, and
highlight the role of urban form in contributing to shape dynamics of
non-spatial nature in cities.",http://arxiv.org/abs/1411.2984v1
"The spatial distribution of people exhibits clustering across a wide range of
scales, from household ($\sim 10^{-2}$ km) to continental ($\sim 10^4$ km)
scales. Empirical data indicates simple power-law scalings for the size
distribution of cities (known as Zipf's law) and the population density
fluctuations as a function of scale. Using techniques from random field theory
and statistical physics, we show that these power laws are fundamentally a
consequence of the scale-free spatial clustering of human populations and the
fact that humans inhabit a two-dimensional surface. In this sense, the
symmetries of scale invariance in two spatial dimensions are intimately
connected to urban sociology. We test our theory by empirically measuring the
power spectrum of population density fluctuations and show that the logarithmic
slope $\alpha = 2.04 \pm 0.09$, in excellent agreement with our theoretical
prediction $\alpha = 2$. The model enables the analytic computation of many new
predictions by importing the mathematical formalism of random fields.",http://arxiv.org/abs/1501.00738v3
"In this paper, we use analysis on graphs to study quantitative measures of
segregation. We focus on a classical statistic from the geography and urban
sociology literature known as Moran's I, which in our language is a score
associated to a real-valued function on a graph, computed with respect to a
spatial weight matrix such as the adjacency matrix associated to the geographic
units that tile a city. Our results characterizing the extremal behavior of I
illustrate the important role of the underlying graph structure, especially the
degree distribution, in interpreting the score. In addition to the standard
spatial weight matrices encoding unit adjacency, we consider the Laplacian L
and a doubly-stochastic approximation M. These alternatives allow us to connect
I to ideas from Fourier analysis and random walks. We offer illustrations of
our theoretical results with a mix of stylized synthetic examples and real
geographic/demographic data.",http://arxiv.org/abs/2112.10708v2
"Developers of computer vision algorithms outsource some of the labor involved
in annotating training data through business process outsourcing companies and
crowdsourcing platforms. Many data annotators are situated in the Global South
and are considered independent contractors. This paper focuses on the
experiences of Argentinian and Venezuelan annotation workers. Through
qualitative methods, we explore the discourses encoded in the task instructions
that these workers follow to annotate computer vision datasets. Our preliminary
findings indicate that annotation instructions reflect worldviews imposed on
workers and, through their labor, on datasets. Moreover, we observe that
for-profit goals drive task instructions and that managers and algorithms make
sure annotations are done according to requesters' commands. This configuration
presents a form of commodified labor that perpetuates power asymmetries while
reinforcing social inequalities and is compelled to reproduce them into
datasets and, subsequently, in computer vision systems.",http://arxiv.org/abs/2105.10990v1
"In the modern world, our cities and societies face several technological and
societal challenges, such as rapid urbanization, global warming & climate
change, the digital divide, and social inequalities, increasing the need for
more sustainable cities and societies. Addressing these challenges requires a
multifaceted approach involving all the stakeholders, sustainable planning,
efficient resource management, innovative solutions, and modern technologies.
Like other modern technologies, social media informatics also plays its part in
developing more sustainable and resilient cities and societies. Despite its
limitations, social media informatics has proven very effective in various
sustainable cities and society applications. In this paper, we review and
analyze the role of social media informatics in sustainable cities and society
by providing a detailed overview of its applications, associated challenges,
and potential solutions. This work is expected to provide a baseline for future
research in the domain.",http://arxiv.org/abs/2412.03600v2
"Increasingly, there is well-grounded concern that through perpetual
scaling-up of computation power and data, current deep learning techniques will
create highly capable artificial intelligence that could pursue goals in a
manner that is not aligned with human values. In turn, such AI could have the
potential of leading to a scenario in which there is serious global-scale
damage to human wellbeing. Against this backdrop, a number of researchers and
public policy professionals have been developing ideas about how to govern AI
in a manner that reduces the chances that it could lead to a global
catastrophe. The jurisdictional focus of a vast majority of their assessments
so far has been the United States, China, and Europe. That preference seems to
reveal an assumption underlying most of the work in this field: That global
south countries can only have a marginal role in attempts to govern AI
development from a global catastrophic risk -focused perspective. Our paper
sets out to undermine this assumption. We argue that global south countries
like India and Singapore (and specific coalitions) could in fact be fairly
consequential in the global catastrophic risk-focused governance of AI. We
support our position using 4 key claims. 3 are constructed out of the current
ways in which advanced foundational AI models are built and used while one is
constructed on the strategic roles that global south countries and coalitions
have historically played in the design and use of multilateral rules and
institutions. As each claim is elaborated, we also suggest some ways through
which global south countries can play a positive role in designing,
strengthening and operationalizing global catastrophic risk-focused AI
governance.",http://arxiv.org/abs/2312.04616v1
"Public policies that supply public goods, especially those involve
collaboration by limiting individual liberty, always give rise to controversies
over governance legitimacy. Multi-Agent Reinforcement Learning (MARL) methods
are appropriate for supporting the legitimacy of the public policies that
supply public goods at the cost of individual interests. Among these policies,
the inter-regional collaborative pandemic control is a prominent example, which
has become much more important for an increasingly inter-connected world facing
a global pandemic like COVID-19. Different patterns of collaborative strategies
have been observed among different systems of regions, yet it lacks an
analytical process to reason for the legitimacy of those strategies. In this
paper, we use the inter-regional collaboration for pandemic control as an
example to demonstrate the necessity of MARL in reasoning, and thereby
legitimizing policies enforcing such inter-regional collaboration. Experimental
results in an exemplary environment show that our MARL approach is able to
demonstrate the effectiveness and necessity of restrictions on individual
liberty for collaborative supply of public goods. Different optimal policies
are learned by our MARL agents under different collaboration levels, which
change in an interpretable pattern of collaboration that helps to balance the
losses suffered by regions of different types, and consequently promotes the
overall welfare. Meanwhile, policies learned with higher collaboration levels
yield higher global rewards, which illustrates the benefit of, and thus
provides a novel justification for the legitimacy of, promoting inter-regional
collaboration. Therefore, our method shows the capability of MARL in
computationally modeling and supporting the theory of calculus of consent,
developed by Nobel Prize winner J. M. Buchanan.",http://arxiv.org/abs/2111.10627v1
"Objective: Amyotrophic lateral sclerosis (ALS) is a rare disease, but is also
one of the most common motor neuron diseases, and people of all races and
ethnic backgrounds are affected. There is currently no cure. Brain computer
interfaces (BCIs) can establish a communication channel directly between the
brain and an external device by recognizing brain activities that reflect user
intent. Therefore, this technology could help ALS patients in promoting
functional independence through BCI-based speller systems and motor assistive
devices. Methods: In this paper, two kinds of ERP-based speller systems were
tested on 18 ALS patients to: (1) assess performance when they spelled 42
characters online continuously, without a break; and (2) to compare performance
between a matrix-based speller paradigm (MS-P, mean visual angle 6 degree) and
a new speller paradigm that used a larger visual angle called the large visual
angle speller paradigm (LS-P, mean visual angle 8 degree). Results: Although
results showed that there were no significant differences between the two
paradigms in accuracy trend over continuous use (p>0.05), the fatigue during
the LS-P condition was significantly lower than that of MS-P (p<0.05). Results
also showed that continuous use slightly reduced the performance of this
ERP-based BCI. Conclusion: 15 subjects obtained higher than 80% feedback
accuracy (online output accuracy) and 9 subjects obtained higher than 90%
feedback accuracy in one of the two paradigms, thus validating the BCI
approaches in this study. Significance: Most ALS subjects in this study could
spell effectively after continuous use of an ERP-based BCI. The new LS-P
display may be easier for subjects to use, resulting in lower fatigue.",http://arxiv.org/abs/1706.09089v1
"Autoregressive language models, which use deep learning to produce human-like
texts, have become increasingly widespread. Such models are powering popular
virtual assistants in areas like smart health, finance, and autonomous driving.
While the parameters of these large language models are improving, concerns
persist that these models might not work equally for all subgroups in society.
Despite growing discussions of AI fairness across disciplines, there lacks
systemic metrics to assess what equity means in dialogue systems and how to
engage different populations in the assessment loop. Grounded in theories of
deliberative democracy and science and technology studies, this paper proposes
an analytical framework for unpacking the meaning of equity in human-AI
dialogues. Using this framework, we conducted an auditing study to examine how
GPT-3 responded to different sub-populations on crucial science and social
topics: climate change and the Black Lives Matter (BLM) movement. Our corpus
consists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals
who vary in gender, race and ethnicity, education level, English as a first
language, and opinions toward the issues. We found a substantively worse user
experience with GPT-3 among the opinion and the education minority
subpopulations; however, these two groups achieved the largest knowledge gain,
changing attitudes toward supporting BLM and climate change efforts after the
chat. We traced these user experience divides to conversational differences and
found that GPT-3 used more negative expressions when it responded to the
education and opinion minority groups, compared to its responses to the
majority groups. We discuss the implications of our findings for a deliberative
conversational AI system that centralizes diversity, equity, and inclusion.",http://arxiv.org/abs/2209.13627v2
"Transportation is one of the most pervasive sources of community noise. In
this study, we used a spatially-resolved model of transportation-related noise
with established transportation noise exposure-response functions to estimate
the population highly annoyed (HA) due to aviation, road, and railway traffic
sources in the United States. Additionally, we employed the use of the Fair
Share Ratio to assess race/ethnicity disparities in traffic noise exposures.
Our results estimate that in 2020, 7.8 million (2.4%) individuals were highly
annoyed by aviation noise, while 5.2 million (1.6%) and 7.9 million (2.4%)
people were highly annoyed by rail and roadway noise, respectively, across the
US. The Fair Share Ratio revealed that Non-Hispanic Asian, Black, NHPI, and
Other, and Hispanic populations were disproportionally highly annoyed by
transportation noise nationwide. Notably, Hispanic populations experienced the
greatest share of high annoyance from aviation noise (1.69 times their
population share). Non-Hispanic Black populations experienced the greatest
share of high annoyance from railway noise (1.48 times their population share).
Non-Hispanic Asian populations experienced the greatest share of high annoyance
from roadway noise (1.51 times their population share). Analyses at the state
and Urban Area levels further highlighted varying disparities in transportation
noise exposure and annoyance across different race ethnicity groups, but still
suggested that Non-Hispanic White populations were less annoyed by all sources
of transportation noise compared to non-White populations. Our findings
indicate widespread presence of transportation noise annoyance across the US
and emphasize the need for targeted source-specific noise mitigation strategies
and policies to minimize the disproportionate impact of transportation noise in
the US.",http://arxiv.org/abs/2307.16837v2
"Gender-based violence (GBV) is a human-generated crisis, existing in various
forms, including offline, via physical and sexual violence, and now online via
harassment and trolling. While studying social media campaigns for different
domains such as public health, natural crises, etc. has received attention in
the literature, such studies for GBV are still in nascent form. The dynamics of
campaigns responding to curb this crisis could benefit from systematic
investigation. To our knowledge, this is the first study to examine such public
campaigns involving social media by organizations operating at the local,
national and global levels, with an eye to answering the following research
questions: (1) How do members of one campaign community engage with other
campaign communities? (2) How do demographic variables such as gender effect
campaign engagement in light of given regional crime statistics? (3) Is there
any coordination among organizational users for campaigns with similar
underlying social causes?",http://arxiv.org/abs/1608.01648v1
"Educational disparities within the Dominican Republic (DR) have long-standing
origins rooted in economic, political, and social inequity. Addressing these
challenges has necessarily called for capacity building with respect to
educational materials, high-quality instruction, and structural resourcing.
Generative AI tools like ChatGPT have begun to pique the interest of Dominican
educators due to their perceived potential to bridge these educational gaps.
However, a substantial body of AI fairness literature has documented ways AI
disproportionately reinforces power dynamics reflective of jurisdictions
driving AI development and deployment policies, collectively termed the AI
Global North. As such, indiscriminate adoption of this technology for DR
education, even in part, risks perpetuating forms of digital coloniality.
Therefore, this paper centers embracing AI-facilitated educational reform by
critically examining how AI-driven tools like ChatGPT in DR education may
replicate facets of digital colonialism. We provide a concise overview of
20th-century Dominican education reforms following the 1916 US occupation.
Then, we employ identified neocolonial aspects historically shaping Dominican
education to interrogate the perceived advantages of ChatGPT for contemporary
Dominican education, as outlined by a Dominican scholar. This work invites AI
Global North & South developers, stakeholders, and Dominican leaders alike to
exercise a relational contextualization of data-centric epistemologies like
ChatGPT to reap its transformative benefits while remaining vigilant of
safeguarding Dominican digital sovereignty.",http://arxiv.org/abs/2310.17533v2
"From the urbanists' perspective, the everyday experience of young people, as
an underrepresented group in the design of public spaces, includes tactics they
use to challenge the strategies which rule over urban spaces. In this regard,
youth led social movements are a set of collective tactics which groups of
young people use to resist power structures. Social informational streams have
revolutionized the way youth organize and mobilize for social movements
throughout the world, especially in urban areas. However, just like public
spaces, these algorithm based platforms have been developed with a great power
imbalance between the developers and users which results in the creation of non
inclusive social informational streams for young activists. Social activism
grows agency and confidence in youth which is critical to their development.
This paper employs a youth centric lens, which is used in designing public
spaces, for designing algorithmic spaces that can improve bottom up youth led
movements. By reviewing the structure of these spaces and how young people
interact with these structures in the different cultural contexts of Iran and
the US, we propose a humanistic approach to designing social informational
streams which can enhance youth activism.",http://arxiv.org/abs/2303.07541v2
"To test a hypothesized faster-than-global sea-level acceleration along the
mid-Atlantic United States, I construct a Gaussian process model that
decomposes tide gauge data into short-term variability and longer-term trends,
and into globally-coherent, regionally-coherent and local components. While
tide gauge records indicate a faster-than-global increase in the rate of
mid-Atlantic U.S. sea-level rise beginning ~1975, this acceleration could
reflect either the start of a long-term trend or ocean dynamic variability. The
acceleration will need to continue for ~2 decades before the rate of increase
of the sea-level gradient between the mid-Atlantic and southeastern U.S. can be
judged as very likely unprecedented by 20th century standards. However, the
gradient is correlated with the Atlantic Multidecadal Oscillation, North
Atlantic Oscillation, and Gulf Stream North Wall indices, all of which are
currently within the range of past variability.",http://arxiv.org/abs/1304.5407v3
"Social movements use social computing systems to complement offline
mobilizations, but prior literature has focused almost exclusively on movement
actors' use of social media. In this paper, we analyze participation and
attention to topics connected with the Black Lives Matter movement in the
English language version of Wikipedia between 2014 and 2016. Our results point
to the use of Wikipedia to (1) intensively document and connect historical and
contemporary events, (2) collaboratively migrate activity to support coverage
of new events, and (3) dynamically re-appraise pre-existing knowledge in the
aftermath of new events. These findings reveal patterns of behavior that
complement theories of collective memory and collective action and help explain
how social computing systems can encode and retrieve knowledge about social
movements as they unfold.",http://arxiv.org/abs/1611.01257v1
"In recent years, social media changed the way individuals participate in
social movements. While activists demonstrate on the street to fight for a
public goal, members of specific movements can also act collective online.
Thus, different aspects might influence the formation of collective identity
and therefore drive collective action on social media. This study combines the
perspectives of social identity- and identity theory in order to examine how
members of an opinion-based group contribute to the collective group/social
identity formation and therefore, to collective action. To this end, we applied
automated text classification techniques to Instagram communication related to
the social movement Fridays for Future. Analysing 1,137 comments showed that
individuals mainly express Group Cohesion and Emotional Attachment rather than
Solidarity by commenting on Instagram. This study further presents a proposed
model of collective group/social identity of collective action. Succeeding
research aims at enhancing the classification and testing the model.",http://arxiv.org/abs/1912.05123v1
"Social media has emerged as a cornerstone of social movements, wielding
significant influence in driving societal change. Simulating the response of
the public and forecasting the potential impact has become increasingly
important. However, existing methods for simulating such phenomena encounter
challenges concerning their efficacy and efficiency in capturing the behaviors
of social movement participants. In this paper, we introduce a hybrid framework
HiSim for social media user simulation, wherein users are categorized into two
types. Core users are driven by Large Language Models, while numerous ordinary
users are modeled by deductive agent-based models. We further construct a
Twitter-like environment to replicate their response dynamics following trigger
events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for
evaluation and conduct comprehensive experiments across real-world datasets.
Experimental results demonstrate the effectiveness and flexibility of our
method.",http://arxiv.org/abs/2402.16333v2
"Social media enables activists to directly communicate with the public and
provides a space for movement leaders, participants, bystanders, and opponents
to collectively construct and contest narratives. Focusing on Twitter messages
from social movements surrounding three issues in 2018-2019 (guns, immigration,
and LGBTQ rights), we create a codebook, annotated dataset, and computational
models to detect diagnostic (problem identification and attribution),
prognostic (proposed solutions and tactics), and motivational (calls to action)
framing strategies. We conduct an in-depth unsupervised linguistic analysis of
each framing strategy, and uncover cross-movement similarities in associations
between framing and linguistic features such as pronouns and deontic modal
verbs. Finally, we compare framing strategies across issues and other social,
cultural, and interactional contexts. For example, we show that diagnostic
framing is more common in replies than original broadcast posts, and that
social movement organizations focus much more on prognostic and motivational
framing than journalists and ordinary citizens.",http://arxiv.org/abs/2406.13820v1
"Current social and activist movements find the opportunity in social media to
effectively impact on the agenda of governing bodies and create `global'
perceptions -- it is often claimed. Content related to the social and activist
movements is online, to be accessed, supported or disputed and distributed from
virtually anywhere at any time, in the public sphere of the Internet. This
activity allows the enlargement of social movements and would increase the
empowerment in the concerned communities. The aim of this explorative study is
to assess whether the temporal evolution of the Normalised Web Distance (NWD)
--as defined by Cilibrasi & Vit\'anyi (2007)-- between identifying terms
concerning this activism could be used to measure the progress or decline of
social empowerment through the Internet. The NWD relies on the page count
number of single and joint queries, which in our study have been registered
using a freely available web browser (e.g. Google Search) providing a time
search window for temporal query results. To explore this meta-data technique,
we introduce the case of a perceived Wirikuta online movement, which originated
in Mexico with the aim to protect the Huichols' sacred land and water resources
from open mining projects for silver ore. We conducted a small scale Internet
study relating the key terms `Wirikuta', `Huichol', and `Wixarika' and their
co-occurrence with seven positive qualifiers (e.g. `sacred land'), five
negative qualifiers (e.g. `violence') and one neutral qualifier (`table') over
time, annually from 1994 till 2013. We confirm close semantic clustering over
time of traditional indigeneous identity terms of the Huichol, and observe a
slight convergence of key terms to `mines' and less pronounced to `sacred land'
and a divergence with respect to `ancestors' indicating a complex image of a
tendency of empowerment.",http://arxiv.org/abs/1406.5946v1
"Different people and cultures associate different emotional states to
different parts and spaces of cities. These vary according to individuals,
their cultures and also to the time of day, day of week, season, special
occasions and more. Recurring patterns may occur in correspondence of the
places in which people work, study, entertain themselves, consume, relate, wait
or just take a break. What can we learn from these patterns? Trying to find
possible answers to this question passes through the possibility to visualize
and represent the configurations of emotional expressions in urban spaces,
across time, geography, theme, cultures and other dimensions. We have developed
ways in which it is possible to harvest people's geo-located (or geo-locatable)
emotional expressions from major social networks and to visualize them
according to a variety of different modalities. In this paper we will present a
series of these types of visualizations, and the ways in which they can be used
to gain better understandings of these emotional patterns as they arise, from
points of view which derive from anthropology, urbanism, sociology, politics
and also arts and poetics. The paper will focus on the ways in which the data
is harvested from different social networks, then categorized and annotated
with meta-data describing the emotional states, the languages in which people
express themselves, the geographic locations, the themes expressed. A
methodology for representing this information across a variety of domains
(time, space, emotion, theme) will then be presented in detail. A reflection on
possible usage cases for anthropology, urbanism, policy-making, arts and design
will end the contribution, as well as the description of series of open issues
and the indication of possible next-steps for research.",http://arxiv.org/abs/1412.5583v1
"Collective efficacy -- the capacity of communities to exert social control
toward the realization of their shared goals -- is a foundational concept in
the urban sociology and neighborhood effects literature. Traditionally,
empirical studies of collective efficacy use large sample surveys to estimate
collective efficacy of different neighborhoods within an urban setting. Such
studies have demonstrated an association between collective efficacy and local
variation in community violence, educational achievement, and health. Unlike
traditional collective efficacy measurement strategies, the Adolescent Health
and Development in Context (AHDC) Study implemented a new approach, obtaining
spatially-referenced, place-based ratings of collective efficacy from a
representative sample of individuals residing in Columbus, OH. In this paper,
we introduce a novel nonstationary spatial model for interpolation of the AHDC
collective efficacy ratings across the study area which leverages
administrative data on land use. Our constructive model specification strategy
involves dimension expansion of a latent spatial process and the use of a
filter defined by the land-use partition of the study region to connect the
latent multivariate spatial process to the observed ordinal ratings of
collective efficacy. Careful consideration is given to the issues of parameter
identifiability, computational efficiency of an MCMC algorithm for model
fitting, and fine-scale spatial prediction of collective efficacy.",http://arxiv.org/abs/2206.04781v2
"Existing Open Vocabulary Detection (OVD) models exhibit a number of
challenges. They often struggle with semantic consistency across diverse
inputs, and are often sensitive to slight variations in input phrasing, leading
to inconsistent performance. The calibration of their predictive confidence,
especially in complex multi-label scenarios, remains suboptimal, frequently
resulting in overconfident predictions that do not accurately reflect their
context understanding. To understand these limitations, multi-label detection
benchmarks are needed. A particularly challenging domain for such benchmarking
is social activities. Due to the lack of multi-label benchmarks for social
interactions, in this work we present ELSA: Evaluating Localization of Social
Activities. ELSA draws on theoretical frameworks in urban sociology and design
and uses in-the-wild street-level imagery, where the size of groups and the
types of activities vary significantly. ELSA includes more than 900 manually
annotated images with more than 4,300 multi-labeled bounding boxes for
individual and group activities. We introduce a novel confidence score
computation method NLSE and a novel Dynamic Box Aggregation (DBA) algorithm to
assess semantic consistency in overlapping predictions. We report our results
on the widely-used SOTA models Grounding DINO, Detic, OWL, and MDETR. Our
evaluation protocol considers semantic stability and localization accuracy and
further exposes the limitations of existing approaches.",http://arxiv.org/abs/2406.01551v2
"Modern societies have adopted government-issued fiat currencies many of which
exist today mainly in the form of digits in credit and bank accounts. Fiat
currencies are controlled by central banks for economic stimulation and
stabilization. Boom-and-bust cycles are created. The volatility of the cycle
has become increasingly extreme. Social inequality due to the concentration of
wealth is prevalent worldwide. As such, restoring sound money, which provides
stored value over time, has become a pressing issue. Currently,
cryptocurrencies such as Bitcoin are in their infancy and may someday qualify
as sound money. Bitcoin today is considered as a digital asset for storing
value. But Bitcoin has problems. The first issue of the current Bitcoin network
is its high energy consumption consensus mechanism. The second is the
cryptographic primitives which are unsafe against post-quantum (PQ) attacks. We
aim to propose Green Bitcoin which addresses both issues. To save energy in
consensus mechanism, we introduce a post-quantum secure (self-election)
verifiable coin-toss function and novel PQ secure proof-of-computation
primitives. It is expected to reduce the rate of energy consumption more than
90 percent of the current Bitcoin network. The elliptic curve cryptography will
be replaced with PQ-safe versions. The Green Bitcoin protocol will help Bitcoin
evolve into a post-quantum secure network.",http://arxiv.org/abs/2212.13986v1
"It is arguable whether history is made by great men and women or vice versa,
but undoubtably social connections shape history. Analysing Wikipedia, a global
collective memory place, we aim to understand how social links are recorded
across cultures. Starting with the set of biographies in the English Wikipedia
we focus on the networks of links between these biographical articles on the 15
largest language Wikipedias. We detect the most central characters in these
networks and point out culture-related peculiarities. Furthermore, we reveal
remarkable similarities between distinct groups of language Wikipedias and
highlight the shared knowledge about connections between persons across
cultures.",http://arxiv.org/abs/1204.3799v2
"Minority groups have been using social media to organize social movements
that create profound social impacts. Black Lives Matter (BLM) and Stop Asian
Hate (SAH) are two successful social movements that have spread on Twitter that
promote protests and activities against racism and increase the public's
awareness of other social challenges that minority groups face. However,
previous studies have mostly conducted qualitative analyses of tweets or
interviews with users, which may not comprehensively and validly represent all
tweets. Very few studies have explored the Twitter topics within BLM and SAH
dialogs in a rigorous, quantified and data-centered approach. Therefore, in
this research, we adopted a mixed-methods approach to comprehensively analyze
BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation
model to understand the top high-level words and topics and (2) open-coding
analysis to identify specific themes across the tweets. We collected more than
one million tweets with the #blacklivesmatter and #stopasianhate hashtags and
compared their topics. Our findings revealed that the tweets discussed a
variety of influential topics in depth, and social justice, social movements,
and emotional sentiments were common topics in both movements, though with
unique subtopics for each movement. Our study contributes to the topic analysis
of social movements on social media platforms in particular and the literature
on the interplay of AI, ethics, and society in general.",http://arxiv.org/abs/2205.14725v2
"With our knowledge of the universe, we have sent men to the moon. We know
microscopic details of objects around us and within us. And yet we know
relatively little about how our society works and how it reacts to changes
brought upon it. Humankind is now facing serious crises for which we must
develop new ways to tackle the global challenges of humanity in the 21st
century. With connectivity between people rapidly increasing, we are now able
to exploit information and communication technologies to achieve major
breakthroughs that go beyond the step-wise improvements in other areas.
  The need of a socio-economic knowledge collider was first pointed out in the
OECD Global Science Forum on Applications of Complexity Science for Public
Policy in Erice from October 5 to 7, 2008. Since then, many scientists have
called for a large-scale ICT-based research initiative on
techno-socialeconomic- environmental issues, sometimes phrased as a Manhattan-,
Apollo-, or CERN-like project to study the way our living planet works in a
social dimension. Due to the connotations, we use the term knowledge
accelerator, here.",http://arxiv.org/abs/1004.4969v2
"COVID-19 pandemic represents an unprecedented global health crisis in the
last 100 years. Its economic, social and health impact continues to grow and is
likely to end up as one of the worst global disasters since the 1918 pandemic
and the World Wars. Mathematical models have played an important role in the
ongoing crisis; they have been used to inform public policies and have been
instrumental in many of the social distancing measures that were instituted
worldwide.
  In this article we review some of the important mathematical models used to
support the ongoing planning and response efforts. These models differ in their
use, their mathematical form and their scope.",http://arxiv.org/abs/2009.10014v1
"To realize the potential benefits and mitigate potential risks of AI, it is
necessary to develop a framework of governance that conforms to ethics and
fundamental human values. Although several organizations have issued guidelines
and ethical frameworks for trustworthy AI, without a mediating governance
structure, these ethical principles will not translate into practice. In this
paper, we propose a multilevel governance approach that involves three groups
of interdependent stakeholders: governments, corporations, and citizens. We
examine their interrelationships through dimensions of trust, such as
competence, integrity, and benevolence. The levels of governance combined with
the dimensions of trust in AI provide practical insights that can be used to
further enhance user experiences and inform public policy related to AI.",http://arxiv.org/abs/2307.03198v2
"This paper focuses on explaining changes over time in globally-sourced,
annual temporal data, with the specific objective of identifying pivotal
factors that contribute to these temporal shifts. Leveraging such analytical
frameworks can yield transformative impacts, including the informed refinement
of public policy and the identification of key drivers affecting a country's
economic evolution. We employ Local Interpretable Model-agnostic Explanations
(LIME) to shed light on national happiness indices, economic freedom, and
population metrics, spanning variable time frames. Acknowledging the presence
of missing values, we employ three imputation approaches to generate robust
multivariate time-series datasets apt for LIME's input requirements. Our
methodology's efficacy is substantiated through a series of empirical
evaluations involving multiple datasets. These evaluations include comparative
analyses against random feature selection, correlation with real-world events
as elucidated by LIME, and validation through Individual Conditional
Expectation (ICE) plots, a state-of-the-art technique proficient in feature
importance detection.",http://arxiv.org/abs/2404.11874v1
"Terrorism instills fear in the minds of people and takes away the freedom of
individuals to act as they will. Terrorism has turned out to be an
international menace today. Here, we study the terrorist attack incidents which
occurred in the last half-century across the globe from the open source, Global
Terrorism Database, and develop a view on their spatio-temporal dynamics. We
construct a complex network of global terrorism and study its growth dynamics,
along with the statistical properties of the anti-social network, which are
quite intriguing. Normally, each nation pursues its own vision of international
security based upon its mandate and particular notions of politics and its
policies to counter the threat of terrorism that could naturally include the
use of tactical measures and strategic negotiations, or even physical power. We
study the network resilience against targeted attacks and random failures,
which could guide the counterterrorist outfits in designing strategies to fight
terrorism. We then use a disparity filter method to isolate backbone of the
network, and identify the terror hubs and vulnerable motifs of global
terrorism. We also examine evolution of the hubs and motifs in a few exemplary
cases like Afghanistan, Colombia, India, Israel, Pakistan and the United
Kingdom. The dynamics of the terror hubs and the vulnerable motifs that we
discover in the network backbone turn out to be very significant, and may
provide deep insight on their formations and spreading, and thereby help in
contending terrorism or framing public policies that can check their spread.",http://arxiv.org/abs/1802.01147v2
"The past century ended with an unexpected explosion of Information and
Communication Technologies (ICT), both in planning/managing public policies,
and in exchanging knowledge. However, the extent to which ICT-based tools
increase the level of public knowledge, or help decision makers is still
uncertain. Although indirectly, the overload of unfiltered Web-based
information seems able to hamper the knowledge growth of people, particularly
in some developing communities, whereas Decision Support Systems (DSS) and
Geographical Information Systems (GIS) prove to be ineffective if managed by
unskilled planning bodies. Given such warns, this paper outlines how the
different social and cultural awareness of local communities can affect the
outcomes of ICT-based tools. It further explores the impacts of ICT-based tools
on community development and spatial planning, emphasizing the role of proper
literacy and education for effective management.",http://arxiv.org/abs/cs/0312021v1
"The Covid-19 pandemic presented an unprecedented global public health
emergency, and concomitantly an unparalleled opportunity to investigate public
responses to adverse social conditions. The widespread ability to post messages
to social media platforms provided an invaluable outlet for such an outpouring
of public sentiment, including not only expressions of social solidarity, but
also the spread of misinformation and misconceptions around the effect and
potential risks of the pandemic. This archive of message content therefore
represents a key resource in understanding public responses to health crises,
analysis of which could help to inform public policy interventions to better
respond to similar events in future. We present a benchmark database of public
social media postings from the United Kingdom related to the Covid-19 pandemic
for academic research purposes, along with some initial analysis, including a
taxonomy of key themes organised by keyword. This release supports the findings
of a research study funded by the Scottish Government Chief Scientists' Office
that aims to investigate social sentiment in order to understand the response
to public health measures implemented during the pandemic.",http://arxiv.org/abs/2103.16446v2
"Governments have long standing interests in preventing market failures and
enhancing innovation in strategic industries. Public policy regarding domestic
technology is critical to both national security and economic prosperity.
Governments often seek to enhance their global competitiveness by promoting
private sector cooperative activity at the inter-organizational level. Research
on network governance has illuminated the structure of boundary-spanning
collaboration mainly for programs with immediate public or non-profit
objectives. Far less research has examined how governments might accelerate
private sector cooperation to prevent market failures or to enhance innovation.
The theoretical contribution of this research is to suggest that government
programs might catalyze cooperative activity by accelerating the preferential
attachment mechanism inherent in social networks. We analyze the long-term
effects of a government program on the strategic alliance network of 451
organizations in the high-tech semiconductor industry between 1987 and 1999,
using stochastic network analysis methods for longitudinal social networks.",http://arxiv.org/abs/1907.13087v2
"Scholars have long hypothesized that democratic forms of government are more
compatible with scientific advancement. However, empirical analysis testing the
democracy-science compatibility hypothesis remains underdeveloped. This article
explores the effect of democratic governance on scientific performance using
panel data on 124 countries between 2007 and 2017. We find evidence supporting
the democracy-science hypothesis. Further, using both internal and external
measures of complexity, we estimate the effects of complexity as a moderating
factor between the democracy-science connection. The results show differential
main effects of economic complexity, globalization, and international
collaboration on scientific performance, as well as significant interaction
effects that moderate the effect of democracy on scientific performance. The
findings show the significance of democratic governance and complex systems in
national scientific performance.",http://arxiv.org/abs/1909.04468v4
"The COVID-19 pandemic has emerged as a global public health crisis. To make
decisions about mitigation strategies and to understand the disease dynamics,
policy makers and epidemiologists must know how the disease is spreading in
their communities. We analyze confirmed infections and deaths over multiple
geographic scales to show that COVID-19's impact is highly unequal: many
subregions have nearly zero infections, and others are hot spots. We attribute
the effect to a Reed-Hughes-like mechanism in which disease arrives at
different times and grows exponentially. Hot spots, however, appear to grow
faster than neighboring subregions and dominate spatially aggregated
statistics, thereby amplifying growth rates. The staggered spread of COVID-19
can also make aggregated growth rates appear higher even when subregions grow
at the same rate. Public policy, economic analysis and epidemic modeling need
to account for potential distortions introduced by spatial aggregation.",http://arxiv.org/abs/2004.12994v2
"Precipitated by rapid globalization, rising inequality, population growth,
and longevity gains, social protection programs have been on the rise in low-
and middle-income countries (LMICs) in the last three decades. However, the
introduction of public benefits could displace informal mechanisms for
risk-protection, which are especially prevalent in LMICs. If the displacement
of private transfers is considerably large, the expansion of social protection
programs could even lead to social welfare loss. In this paper, we critically
survey the recent empirical literature on crowd-out effects in response to
public policies, specifically in the context of LMICs. We review and synthesize
patterns from the behavioral response to various types of social protection
programs. Furthermore, we specifically examine for heterogeneous treatment
effects by important socioeconomic characteristics. We conclude by drawing on
lessons from our synthesis of studies. If poverty reduction objectives are
considered, along with careful program targeting that accounts for potential
crowd-out effects, there may well be a net social gain.",http://arxiv.org/abs/2006.00737v2
"Based on a conceptual framework that integrates three dimensions of digital
transformation (DT), namely, the nature of the product, client interaction, and
the level of coordination with industry players, this paper aims to explain the
level of influence that contextual crisis factors may have played in
organizational digitalization choices in search for resilience as part of
adaptation strategies. In particular, this investigation would analyze
digitalization choices as survival strategies for COVID-19 crisis in the case
of Mexican enterprises. The selected country is of particular interest as
research target in the Global South, in that public policy has offered little
support to keep business organizations up and running, leaving entrepreneurs
with no other option but to implement bottom-up resilience strategies,
including digitalization. Qualitative Comparative Analysis (QCA) has been
proposed to identify combinations of conditions to explain the role played by
COVID crisis-related contextual factors that may have led to particular forms
of digitalization. Semi-structured interviews with industry associations are
also proposed to gain knowledge about group responses to the crisis.",http://arxiv.org/abs/2108.09802v1
"Accurately estimating the snowpack in key mountainous basins is critical for
water resource managers to make decisions that impact local and global
economies, wildlife, and public policy. Currently, this estimation requires
multiple LiDAR-equipped plane flights or in situ measurements, both of which
are expensive, sparse, and biased towards accessible regions. In this paper, we
demonstrate that fusing spatial and temporal information from multiple,
openly-available satellite and weather data sources enables estimation of
snowpack in key mountainous regions. Our multisource model outperforms
single-source estimation by 5.0 inches RMSE, as well as outperforms sparse in
situ measurements by 1.2 inches RMSE.",http://arxiv.org/abs/2208.04246v1
"There is growing interest in understanding how interactions between
system-wide objectives and local community decision-making will impact the
clean energy transition. The concept of energysheds has gained traction in the
areas of public policy and social science as a way to study these
relationships. However, development of technical definitions of energysheds
that permit system analysis are still largely missing. In this work, we propose
a mathematical definition for energysheds, and introduce an analytical
framework for studying energyshed concepts within the context of future
electric power system operations. This framework is used to develop insights
into the factors that impact a community's ability to achieve energyshed policy
incentives within a larger connected power grid, as well as the tradeoffs
associated with different spatial policy requirements. We also propose an
optimization-based energyshed policy design problem, and show that it can be
solved to global optimality within arbitrary precision by employing concepts
from quasi-convex optimization. Finally, we investigate how interconnected
energysheds can cooperatively achieve their objectives in bulk power system
operations.",http://arxiv.org/abs/2311.16300v2
"Urban forests provide ecosystem services that are public goods with local
(shade) to global (carbon sequestration) benefits and occur on both public and
private lands. Thus, incentives for private tree owners to invest in tree care
may fall short of those of a public forest manager aiming to optimize ecosystem
service benefits for society. The management of a forest pest provides a
salient focus area because pests threaten public goods provision and pest
management generates feedback that mitigates future risks to forests. We use a
game theoretic model to determine optimal pest treatment subsidies for a focal
privately owned tree and use an optimization approach to guide targeted public
treatment of a representative public tree. We find that optimal public
subsidies for private tree treatment depend on assessed tree health and on the
prevalence of the pest in the community, considerations absent from many
existing programs. Next, by applying our pest treatment policies to a
community-scale model of emerald ash borer forest pest dynamics, we predict ash
mortality under a range of treatment scenarios over a 50-year time horizon. Our
results highlight how designing policies that consider the public goods
benefits of private actions can contribute to sustainable land management.",http://arxiv.org/abs/2312.05403v2
"The Land Matrix initiative (https://landmatrix.org) and its global
observatory aim to provide reliable data on large-scale land acquisitions to
inform debates and actions in sectors such as agriculture, extraction, or
energy in low- and middle-income countries. Although these data are recognized
in the academic world, they remain underutilized in public policy, mainly due
to the complexity of access and exploitation, which requires technical
expertise and a good understanding of the database schema.
  The objective of this work is to simplify access to data from different
database systems. The methods proposed in this article are evaluated using data
from the Land Matrix. This work presents various comparisons of Large Language
Models (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,
RAG, Agents) to query different database systems (GraphQL and REST queries).
The experiments are reproducible, and a demonstration is available online:
https://github.com/tetis-nlp/landmatrix-graphql-python.",http://arxiv.org/abs/2412.12961v1
"Background An advantaged socioeconomic position (SEP) and satisfying social
support during pregnancy (SSP) have been found to be protective factors of
maternal postpartum depression (PDD). An advantaged SEP is also associated with
satisfying SSP, making SSP a potential mediator of social inequalities in PPD.
SEP, SSP and PPD are associated with migrant status. The aim of this study was
to quantify the mediating role of SSP in social inequalities in PPD regarding
mother's migrant status. Methods A sub-sample of 15,000 mothers from the French
nationally-representative ELFE cohort study was used for the present analyses.
SEP was constructed as a latent variable measured with educational attainment,
occupational grade, employment, financial difficulties and household income.
SSP was characterized as perceived support from partner (good relation,
satisfying support and paternal leave) and actual support from midwives
(psychosocial risk factors assessment and antenatal education). Mediation
analyses with multiple mediators, stratified by migrant status were conducted.
Results Study population included 76% of non-migrant women, 12% of second and
12% of first generation migrant. SEP was positively associated with support
from partner, regardless of migrant status. Satisfying partner support was
associated with a 8 (non-migrant women) to 11% (first generation migrant women)
reduction in PPD score. Limitations History of depression was not
reported.Conclusions Partner support could reduce social inequalities in PPD.
This work supports the need of interventions, longitudinal and qualitative
studies including fathers and adapted to women at risk of PPD to better
understand the role of SSP in social inequalities in PPD.",http://arxiv.org/abs/2004.11244v1
"Inward Foreign Direct Investment (IFDI) into Europe and Asian developing
countries like Bangladesh is experimentally examined in this study. IFDI in
emerging markets has been boosted by global investment and inflow influenced by
resource availability and public policy. The economic policy uncertainty on
IFDI in 13 countries is explored at a time when the crisis between Russia and
Ukraine war is having a global impact. Microeconomic factors affected Gross
Domestic Product (GDP) growth, inflation, interest rates, and the currency rate
fluctuated with IFDI, which mostly shocked during COVID-19 and the
Russia-Ukraine war. With data from the World Bank and the United Nations
Conference on Trade and Development (UNCTAD) database, we compile a panel
dataset covering 2018-2022. The researchers used a mixture of panel and linear
regression analysis using a random effect model. Our findings show that the
impact of global rates hurts IFDI in 13 selected countries. There is a
correlation between a country's ability to enforce contracts and the amount of
Inward FDI it receives. Using the top 13 hosts of incoming FDI flows COVID-19
and Russia-Ukraine wartime series analysis gives valuable information for
policymakers in the remaining countries chosen to attract IFDI inflows.",http://arxiv.org/abs/2401.03096v1
"Social movements rely in large measure on networked communication
technologies to organize and disseminate information relating to the movements'
objectives. In this work we seek to understand how the goals and needs of a
protest movement are reflected in the geographic patterns of its communication
network, and how these patterns differ from those of stable political
communication. To this end, we examine an online communication network
reconstructed from over 600,000 tweets from a thirty-six week period covering
the birth and maturation of the American anticapitalist movement, Occupy Wall
Street. We find that, compared to a network of stable domestic political
communication, the Occupy Wall Street network exhibits higher levels of
locality and a hub and spoke structure, in which the majority of non-local
attention is allocated to high-profile locations such as New York, California,
and Washington D.C. Moreover, we observe that information flows across state
boundaries are more likely to contain framing language and references to the
media, while communication among individuals in the same state is more likely
to reference protest action and specific places and and times. Tying these
results to social movement theory, we propose that these features reflect the
movement's efforts to mobilize resources at the local level and to develop
narrative frames that reinforce collective purpose at the national level.",http://arxiv.org/abs/1306.5473v1
"Existing end-to-end-encrypted (E2EE) email systems, mainly PGP, have long
been evaluated in controlled lab settings. While these studies have exposed
usability obstacles for the average user and offer design improvements, there
exist users with an immediate need for private communication, who must cope
with existing software and its limitations. We seek to understand whether
individuals motivated by concrete privacy threats, such as those vulnerable to
state surveillance, can overcome usability issues to adopt complex E2EE tools
for long-term use. We surveyed regional activists, as surveillance of social
movements is well-documented. Our study group includes individuals from 9
social movement groups in the US who had elected to participate in a workshop
on using Thunderbird+Enigmail for email encryption. These workshops tool place
prior to mid-2017, via a partnership with a non-profit which supports social
movement groups. Six to 40 months after their PGP email encryption training,
more than half of the study participants were continuing to use PGP email
encryption despite intervening widespread deployment of simple E2EE messaging
apps such as Signal. We study the interplay of usability with social factors
such as motivation and the risks that individuals undertake through their
activism. We find that while usability is an important factor, it is not enough
to explain long term use. For example, we find that riskiness of one's activism
is negatively correlated with long-term PGP use. This study represents the
first long-term study, and the first in-the-wild study, of PGP email encryption
adoption.",http://arxiv.org/abs/2104.04478v1
"Public policy must confront emergencies that evolve in real time and in
uncertain directions, yet little is known about the nature of policy response.
Here we take the coronavirus pandemic as a global and extraordinarily
consequential case, and study the global policy response by analyzing a novel
dataset recording policy documents published by government agencies, think
tanks, and intergovernmental organizations (IGOs) across 114 countries (37,725
policy documents from Jan 2nd through May 26th 2020). Our analyses reveal four
primary findings. (1) Global policy attention to COVID-19 follows a remarkably
similar trajectory as the total confirmed cases of COVID-19, yet with evolving
policy focus from public health to broader social issues. (2) The COVID-19
policy frontier disproportionately draws on the latest, peer-reviewed, and
high-impact scientific insights. Moreover, policy documents that cite science
appear especially impactful within the policy domain. (3) The global policy
frontier is primarily interconnected through IGOs, such as the WHO, which
produce policy documents that are central to the COVID19 policy network and
draw especially strongly on scientific literature. Removing IGOs' contributions
fundamentally alters the global policy landscape, with the policy citation
network among government agencies increasingly fragmented into many isolated
clusters. (4) Countries exhibit highly heterogeneous policy attention to
COVID-19. Most strikingly, a country's early policy attention to COVID-19 shows
a surprising degree of predictability for the country's subsequent deaths.
Overall, these results uncover fundamental patterns of policy interactions and,
given the consequential nature of emergent threats and the paucity of
quantitative approaches to understand them, open up novel dimensions for
assessing and effectively coordinating global and local responses to COVID-19
and beyond.",http://arxiv.org/abs/2006.13853v1
"Non-linear dynamics is probably much more common in the epigenetic dynamics
of living beings than hitherto recognized. Here we report a case of global
bifurcation triggered by gender that affects higher cognitive functions in
humans. We report a cross-cultural study showing deviations in time perception,
as assessed by estimating the duration of brief sounds, according to their
durations and to the gender of the perciver. Results show that the duration of
sounds lasting less than 10 s were on average overestimated, whereas those
lasting longer were underestimated; estimates of sounds shorter than 1 s were
extremely inaccurate. Females consistently gave longer estimates than males.
Accuracy in time estimation was correlated to academic performance in
disciplines requiring mathematical or scientific skills in male, but not in
female students. This difference in correlation however had nothing to do with
overall skills in mathematics. Both sexes scored similarly in scientific and
technical disciplines, but females had higher grades than males in languages
and lower ones in physical education. Our results confirm existing evidence for
gender differences in cognitive processing, hinting to the existence of
different ""mathematical intelligences"" with different non-linear relationships
between natural or biological mathematical intuition and time perception.",http://arxiv.org/abs/1203.3954v2
"Perception of offensiveness is inherently subjective, shaped by the lived
experiences and socio-cultural values of the perceivers. Recent years have seen
substantial efforts to build AI-based tools that can detect offensive language
at scale, as a means to moderate social media platforms, and to ensure safety
of conversational AI technologies such as ChatGPT and Bard. However, existing
approaches treat this task as a technical endeavor, built on top of data
annotated for offensiveness by a global crowd workforce without any attention
to the crowd workers' provenance or the values their perceptions reflect. We
argue that cultural and psychological factors play a vital role in the
cognitive processing of offensiveness, which is critical to consider in this
context. We re-frame the task of determining offensiveness as essentially a
matter of moral judgment -- deciding the boundaries of ethically wrong vs.
right language within an implied set of socio-cultural norms. Through a
large-scale cross-cultural study based on 4309 participants from 21 countries
across 8 cultural regions, we demonstrate substantial cross-cultural
differences in perceptions of offensiveness. More importantly, we find that
individual moral values play a crucial role in shaping these variations: moral
concerns about Care and Purity are significant mediating factors driving
cross-cultural differences. These insights are of crucial importance as we
build AI models for the pluralistic world, where the values they espouse should
aim to respect and account for moral values in diverse geo-cultural contexts.",http://arxiv.org/abs/2312.06861v1
"We show that the divergent acoustic energy release rate in a quasi-statically
compressed nano-porous material can be used as a precursor to failure in such
materials. A quantification of the inequality of the energy release rate using
social inequality measure indices help constructing a warning signal for large
bursts of energy release. We also verify similar behavior for simulations of
viscoelastic fiber bundle models that mimic the strain-hardening dynamics of
the samples. The results demonstrate experimental applicability of the
precursory signal formulation for any diverging response function near a
transition point using social inequality indices.",http://arxiv.org/abs/2406.06200v1
"Public policy also represent a special subdiscipline within political
science, within political science. They are given increasing importance and
importance in the context of scientific research and scientific approaches.
Public policy as a discipline of political science have their own special
subject and method of research. A particularly important aspect of the
scientific approach to public policy is the aspect of applying research methods
as one of the stages and phases of designing scientific research. In this
sense, the goal of this research is to present the application of scientific
research methods in the field of public policy. Those methods are based on
scientific achievements developed within the framework of modern methodology of
social sciences. Scientific research methods represent an important functional
part of the research project as a model of the scientific research system,
predominantly of an empirical character, which is applicable to all types of
research. This is precisely what imposes the need to develop a project as a
prerequisite for applying scientific methods and conducting scientific
research, and therefore for a more complete understanding of public policy. The
conclusions that will be reached point to the fact that scientific research of
public policy can not be carried out without the creation of a scientific
research project as a complex scientific and operational document and the
application of appropriate methods and techniques developed within the
framework of scientific achievements of modern social science methodology.",http://arxiv.org/abs/2309.15973v2
"ENUM effectively bridges the telephone and Internet worlds by placing
telephone numbers from the ITU Rec. E.164 public telecommunication numbering
plan into the Internet Domain Name System (DNS) as domain names. ENUM
potentially presents significant public policy issues at both the domestic and
international levels. Ultimately, it should not matter whether ENUM is
approached as a telecommunications issue or an Internet issue because: (1) they
are becoming the same thing technically, and (2) they engage the same global
public interests. For the same reasons as apply to traditional
telecommunications, and even to the Internet itself, public oversight of ENUM
naming, numbering, and addressing resources is justified both by technical
necessity and the interests of consumer protection (particularly personal
privacy) and competition at higher service layers. A single, coordinated global
DNS domain for at least Tier 0 (the international level) of the ENUM names
hierarchy should be designated by public authorities. Many of the technical
characteristics and policy considerations relevant at the ENUM Tier 0 and 1
zones are also directly applicable to the Internet's IP address space and DNS
root (or Tier 0) zone - key shared elements of the Internet's logical
infrastructure. Despite the fundamentally international nature of the
Internet's logical infrastructure layer, and the purported privatization of
administration of its IP address space and the DNS, Internet governance is not
yet truly international. The ENUM policy debate illustrates the need for
authoritative international public oversight of public communications network
logical infrastructure, including that of traditional telecommunications, the
Internet, and ENUM.",http://arxiv.org/abs/cs/0109091v2
"Quantitative metrics that measure the global economy's equilibrium have
strong and interdependent relationships with the agricultural supply chain and
international trade flows. Sudden shocks in these processes caused by outlier
events such as trade wars, pandemics, or weather can have complex effects on
the global economy. In this paper, we propose a novel framework, namely:
DeepAg, that employs econometrics and measures the effects of outlier events
detection using Deep Learning (DL) to determine relationships between
commonplace financial indices (such as the DowJones), and the production values
of agricultural commodities (such as Cheese and Milk). We employed a DL
technique called Long Short-Term Memory (LSTM) networks successfully to predict
commodity production with high accuracy and also present five popular models
(regression and boosting) as baselines to measure the effects of outlier
events. The results indicate that DeepAg with outliers' considerations (using
Isolation Forests) outperforms baseline models, as well as the same model
without outliers detection. Outlier events make a considerable impact when
predicting commodity production with respect to financial indices. Moreover, we
present the implications of DeepAg on public policy, provide insights for
policymakers and farmers, and for operational decisions in the agricultural
ecosystem. Data are collected, models developed, and the results are recorded
and presented.",http://arxiv.org/abs/2110.12062v2
"The pepenaactivity is part of the informal sector, characterized by
precariousness and social invisibility; however, it is a key element in the
recycling production chain, providing an economic income for many people. In
the city of Chihuahua, this activity is carried out on a significant scale.
Therefore, this study analyzes and dignifies the recycling of urban solid waste
from the perspective of scavengers, also explaining their relationship with
other social actors at the city's final disposal site. The study starts from
the premise that waste picking is, in the current era of global environmental
crisis, an effective way to conserve resources and reduce environmental
impacts. Methodologically, participatory action research was used as a strategy
to raise awareness among scavengers about the importance of their work and to
present their community organization as an example to other urban waste picking
groups in different locations. Finally, we found issues in these groups such as
informality, lack of legal support,specific health risks, and, more broadly,
the absence of public policies to recognize this activity as valuable in
addressing the global environmental crisis",http://arxiv.org/abs/2410.05283v1
"Social inequality is a topic of interest since ages, and has attracted
researchers across disciplines to ponder over it origin, manifestation,
characteristics, consequences, and finally, the question of how to cope with
it. It is manifested across different strata of human existence, and is
quantified in several ways. In this review we discuss the origins of social
inequality, the historical and commonly used non-entropic measures such as
Lorenz curve, Gini index and the recently introduced $k$ index. We also discuss
some analytical tools that aid in understanding and characterizing them.
Finally, we argue how statistical physics modeling helps in reproducing the
results and interpreting them.",http://arxiv.org/abs/1507.02445v1
"Based on some analytic structural properties of the Gini and Kolkata indices
for social inequality, as obtained from a generic form of the Lorenz function,
we make a conjecture that the limiting (effective saturation) value of the
above-mentioned indices is about 0.865. This, together with some more new
observations on the citation statistics of individual authors (including Nobel
laureates), suggests that about $14\%$ of people or papers or social conflicts
tend to earn or attract or cause about $86\%$ of wealth or citations or deaths
respectively in very competitive situations in markets, universities or wars.
This is a modified form of the (more than a) century old $80-20$ law of Pareto
in economy (not visible today because of various welfare and other strategies)
and gives an universal value ($0.86$) of social (inequality) constant or
number.",http://arxiv.org/abs/2102.01527v5
"The present study examines to what extent cultural background determines
sensorimotor synchronization in humans",http://arxiv.org/abs/2107.03971v1
"With the growth of social media usage, social activists try to leverage this
platform to raise the awareness related to a social issue and engage the public
worldwide. The broad use of social media platforms in recent years, made it
easier for the people to stay up-to-date on the news related to regional and
worldwide events. While social media, namely Twitter, assists social movements
to connect with more people and mobilize the movement, traditional media such
as news articles help in spreading the news related to the events in a broader
aspect. In this study, we analyze linguistic features and cues, such as
individualism vs. pluralism, sentiment and emotion to examine the relationship
between the medium and discourse over time. We conduct this work in a specific
application context, the ""Black Lives Matter"" (BLM) movement, and compare
discussions related to this event in social media vs. news articles.",http://arxiv.org/abs/1808.01742v1
"Messaging platforms, especially those with a mobile focus, have become
increasingly ubiquitous in society. These mobile messaging platforms can have
deceivingly large user bases, and in addition to being a way for people to stay
in touch, are often used to organize social movements, as well as a place for
extremists and other ne'er-do-well to congregate. In this paper, we present a
dataset from one such mobile messaging platform: Telegram. Our dataset is made
up of over 27.8K channels and 317M messages from 2.2M unique users. To the best
of our knowledge, our dataset comprises the largest and most complete of its
kind. In addition to the raw data, we also provide the source code used to
collect it, allowing researchers to run their own data collection instance. We
believe the Pushshift Telegram dataset can help researchers from a variety of
disciplines interested in studying online social movements, protests, political
extremism, and disinformation.",http://arxiv.org/abs/2001.08438v1
"The social structure of an animal population can often influence movement and
inform researchers on a species' behavioral tendencies. Animal social networks
can be studied through movement data; however, modern sources of data can have
identification issues that result in multiply-labeled individuals. Since all
available social movement models rely on unique labels, we extend an existing
Bayesian hierarchical movement model in a way that makes use of a latent social
network and accommodates multiply-labeled movement data (MLMD). We apply our
model to drone-measured movement data from Risso's dolphins (Grampus griseus)
and estimate the effects of sonar exposure on the dolphins' social structure.
Our proposed framework can be applied to MLMD for various social movement
applications.",http://arxiv.org/abs/2204.00542v2
"Social media enables the rapid spread of many kinds of information, from
memes to social movements. However, little is known about how information
crosses linguistic boundaries. We apply causal inference techniques on the
European Twitter network to quantify multilingual users' structural role and
communication influence in cross-lingual information exchange. Overall,
multilinguals play an essential role; posting in multiple languages increases
betweenness centrality by 13%, and having a multilingual network neighbor
increases monolinguals' odds of sharing domains and hashtags from another
language 16-fold and 4-fold, respectively. We further show that multilinguals
have a greater impact on diffusing information less accessible to their
monolingual compatriots, such as information from far-away countries and
content about regional politics, nascent social movements, and job
opportunities. By highlighting information exchange across borders, this work
sheds light on a crucial component of how information and ideas spread around
the world.",http://arxiv.org/abs/2304.03797v1
"Encompassing a diverse population of developers, non-technical users,
organizations, and many other stakeholders, open source software (OSS)
development has expanded to broader social movements from the initial product
development aims. Ideology, as a coherent system of ideas, offers value
commitments and normative implications for any social movement, so does OSS
ideology for the open source movement. However, the literature on open source
ideology is often fragile, or lacking in empirical evidence. In this paper, we
sought to develop a comprehensive empirical theory of ideologies in open source
software movement. Following a grounded theory procedure, we collected and
analyzed data from 22 semi-structured interviews and 41 video recordings of
Open Source Initiative (OSI) board members' public speeches. An empirical
theory of OSS ideology emerged in our analysis, with six key categories:
membership, norms/values, goals, activities, resources, and positions/group
relations; each consists of a number of themes and subthemes. We discussed a
subset of carefully selected themes and subthemes in detail based on their
theoretical significance. With this ideological lens, we examined the
implications and insights into open source development, and shed light on the
research into open source as a social-cultural construction in the future.",http://arxiv.org/abs/2306.05548v1
"Large language and vision models have transformed how social movements
scholars identify protest and extract key protest attributes from multi-modal
data such as texts, images, and videos. This article documents how we
fine-tuned two large pretrained transformer models, including longformer and
swin-transformer v2, to infer potential protests in news articles using textual
and imagery data. First, the longformer model was fine-tuned using the Dynamic
of Collective Action (DoCA) Corpus. We matched the New York Times articles with
the DoCA database to obtain a training dataset for downstream tasks. Second,
the swin-transformer v2 models was trained on UCLA-protest imagery data.
UCLA-protest project contains labeled imagery data with information such as
protest, violence, and sign. Both fine-tuned models will be available via
\url{https://github.com/Joshzyj/llvms4protest}. We release this short technical
report for social movement scholars who are interested in using LLVMs to infer
protests in textual and imagery data.",http://arxiv.org/abs/2311.18241v1
"The cyberspace and development of intelligent systems using Artificial
Intelligence (AI) creates new challenges to computer professionals, data
scientists, regulators and policy makers. For example, self-driving cars raise
new technical, ethical, legal and public policy issues. This paper proposes a
course named Computers, Ethics, Law, and Public Policy, and suggests a
curriculum for such a course. This paper presents ethical, legal, and public
policy issues relevant to building and using intelligent systems.",http://arxiv.org/abs/1904.12470v5
"The potential threat that domestic animals pose to the health of human
populations tends to be overlooked. We posit that positive steps forward can be
made in this area, via suitable state-wide public policy. In this paper, we
describe the data collection process that took place in Casilda (a city in
Argentina), in the context of a canine census. We outline preliminary findings
emerging from the data, based on a number of perspectives, along with
implications of these findings in terms of informing public policy.",http://arxiv.org/abs/2012.07475v1
"Promoting and increasing energy efficiency is a promising method of reducing
CO2 emissions and avoiding the potentially devastating effects of climate
change. The question is: How do we induce a cultural or behavioural change
whereby people nationally and globally adopt more energy efficient lifestyles?
  We propose a new family of mathematical models, based on a statistical
mechanics extension of discrete choice theory, that offer a set of formal tools
to systematically analyse and quantify this problem. An application example
could be to predict the percentage of people choosing to buy new energy
efficient light bulbs instead of the traditional incandescent versions. Through
statistical evaluation of survey responses, the models can identify the key
driving factors in the decision-making process; for example, the extent to
which people imitate each other. These models allow us to incorporate the
effect of social interactions could help us identify 'tipping points' at a
societal level. This knowledge could be used to trigger structural changes in
our society. The results may provide tangible and deliverable evidence-based
policy options to decision-makers.
  We believe that these models offer an opportunity for the research
community-in both the social and physical sciences-and decision-makers in the
private and public sectors to work together towards preventing the potentially
devastating social, economic and environmental effects of climate change.",http://arxiv.org/abs/0804.3319v3
"The Quantum Decision Theory, developed recently by the authors, is applied to
clarify the role of risk and uncertainty in decision making and in particular
in relation to the phenomenon of dynamic inconsistency. By formulating this
notion in precise mathematical terms, we distinguish three types of
inconsistency: time inconsistency, planning paradox, and inconsistency
occurring in some discounting effects. While time inconsistency is well
accounted for in classical decision theory, the planning paradox is in
contradiction with classical utility theory. It finds a natural explanation in
the frame of the Quantum Decision Theory. Different types of discounting
effects are analyzed and shown to enjoy a straightforward explanation within
the suggested theory. We also introduce a general methodology based on
self-similar approximation theory for deriving the evolution equations for the
probabilities of future prospects. This provides a novel classification of
possible discount factors, which include the previously known cases
(exponential or hyperbolic discounting), but also predicts a novel class of
discount factors that decay to a strictly positive constant for very large
future time horizons. This class may be useful to deal with very long-term
discounting situations associated with intergenerational public policy choices,
encompassing issues such as global warming and nuclear waste disposal.",http://arxiv.org/abs/0812.2388v2
"The functioning of the cryptocurrency Bitcoin relies on the open availability
of the entire history of its transactions. This makes it a particularly
interesting socio-economic system to analyse from the point of view of network
science. Here we analyse the evolution of the network of Bitcoin transactions
between users. We achieve this by using the complete transaction history from
December 5th 2011 to December 23rd 2013. This period includes three bubbles
experienced by the Bitcoin price. In particular, we focus on the global and
local structural properties of the user network and their variation in relation
to the different period of price surge and decline. By analysing the temporal
variation of the heterogeneity of the connectivity patterns we gain insights on
the different mechanisms that take place during bubbles, and find that hubs
(i.e., the most connected nodes) had a fundamental role in triggering the burst
of the second bubble. Finally, we examine the local topological structures of
interactions between users, we discover that the relative frequency of triadic
interactions experiences a strong change before, during and after a bubble, and
suggest that the importance of the hubs grows during the bubble. These results
provide further evidence that the behaviour of the hubs during bubbles
significantly increases the systemic risk of the Bitcoin network, and discuss
the implications on public policy interventions.",http://arxiv.org/abs/1805.04460v1
"Reducing and redressing the effects of deforestation is a complex public
policy challenge, and evaluating the efficacy of such policy efforts is crucial
for policy learning and adaptation. Deforestation in high-income nations can
contribute substantially to global forest loss, despite the presence of strong
institutions and high policy capacity. In Queensland, Australia, over 5 million
hectares of native forest has been lost since 1988. Successive regulatory
policies have aimed to reduce deforestation in Queensland, though debate exists
over their effect given the influence of other drivers of forest loss. Using a
hierarchical Bayesian statistical framework, we combine satellite imagery of
forest loss with macroeconomic, land tenure, biophysical and climatic variables
to collectively model deforestation for 50 local government areas (LGAs) across
Queensland. We apply the spatially explicit bent-cable regression model to
detect trend change that may signal a regulatory policy effect. We find that
annual % growth in GDP was the only clear driver of LGA-specific deforestation
after adjusting for other covariate effects. Our model shows strong evidence of
spatial contagion in deforestation across Queensland, and this effect is
influenced by the dominant land tenure type within each LGA. We find our model
exhibits a ""bend"" mostly between 2000 and 2007, consistent with expectations,
but the signal is not particularly strong due extreme variation in
deforestation trends between and within LGAs. Our results demonstrate that the
bent-cable model is a promising technique for detecting system changes in
response to policy interventions, but future work should be conducted at a
national scale to provide more data points, and incorporate more LGA-specific
data to improve model goodness-of-fit.",http://arxiv.org/abs/1906.09365v1
"Intervention policies against COVID-19 have caused large-scale disruptions
globally, and led to a series of pattern changes in the power system operation.
Analyzing these pandemic-induced patterns is imperative to identify the
potential risks and impacts of this extreme event. With this purpose, we
developed an open-access data hub (COVID-EMDA+), an open-source toolbox
(CoVEMDA), and a few evaluation methods to explore what the U.S. power systems
are experiencing during COVID-19. These resources could be broadly used for
research, public policy, and educational purposes. Technically, our data hub
harmonizes a variety of raw data such as generation mix, demand profiles,
electricity price, weather observations, mobility, confirmed cases and deaths.
Typical methods are reformulated and standardized in our toolbox, including
baseline estimation, regression analysis, and scientific visualization. Here
the fluctuation index and probabilistic baseline are proposed for the first
time to consider data fluctuation and estimation uncertainty. Based on these,
we conduct three empirical studies on the U.S. power systems, and share new
solutions and unexpected findings to address the issues of public concerns.
This conveys a more complete picture of the pandemic's impacts, and also opens
up several attractive topics for future work. Python, Matlab source codes, and
user manuals are all publicly shared on a Github repository.",http://arxiv.org/abs/2112.05320v2
"Background The COVID-19 pandemic has increased mental distress globally. The
proportion of people reporting anxiety is 26%, and depression is 34% points.
Disentangling associational and causal contributions of behavior, COVID-19
cases, and economic distress on mental distress will dictate different
mitigation strategies to reduce long-term pandemic-related mental distress.
Methods We use the Household Pulse Survey (HPS) April 2020 to February 2021
data to examine mental distress among U.S. citizens attributable to COVID-19.
We combined HPS survey data with publicly available state-level weekly:
COVID-19 case and death data from the Centers for Disease Control, public
policies, and Apple and Google mobility data. Finally, we constructed economic
and mental distress measures to estimate structural models with lag dependent
variables to tease out public health policies' associational and causal path
coefficients on economic and mental distress. Findings From April 2020 to
February 2021, we found that anxiety and depression had steadily climbed in the
U.S. By design, mobility restrictions primarily affected public health policies
where businesses and restaurants absorbed the biggest hit. Period t-1 COVID-19
cases increased job loss by 4.1% and economic distress by 6.3% points in the
same period. Job-loss and housing insecurity in t-1 increased period t mental
distress by 29.1% and 32.7%, respectively. However, t-1 food insecurity
decreased mental distress by 4.9% in time t. The pandemic-related potential
causal path coefficient of period t-1 economic distress on period t depression
is 57.8%, and anxiety is 55.9%. Thus, we show that period t-1 COVID-19 case
information, behavior, and economic distress may be causally associated with
pandemic related period t mental distress.",http://arxiv.org/abs/2112.11564v1
"Objective: The novel coronavirus COVID-19 outbreak rapidly evolved into
pandemic. Global research efforts focus on this topic and with the
collaboration of the scientific journals publication industry produced more
than 16,000 related published articles in PubMed within five months from the
onset of the outbreak. Herein, a comparison of the COVID-19 citations in PubMed
and Web of Science was performed with SARS-CoV, MERS-CoV, Ebola, Zika, avian
and swine influenza epidemics. Methods: The citations were searched and
collected using the disease terms and the date of publication restriction. The
total number of PubMed citations and the HIV associated papers during the same
chronological periods were examined in parallel. The journal category and
country information of the publications were gathered from Web of Science. The
collected data were statistically analyzed and compared. Results: Significant
correlations were found between COVID-19 and MERS (CC=0.988; p=0.003; q=0.006),
Ebola (CC=0.987; p=0.003; q=0.011), and SARS (CC=0.964; p=0.015; q=0.028)
epidemics five-month pick of novel citations in PubMed. However, COVID-19
publications were accumulated earlier and in larger numbers than any other 21st
century major communicable disease outbreak. Conclusion: The acceleration and
the total number of COVID-19 publications represent an unprecedented landmark
event in the medical library history. The immediate adoption of the fast-track
peer-reviewing and publishing as well as the open access publication policies
by the journal publishers are significant contributors to this bibliographic
phenomenon.",http://arxiv.org/abs/2006.05366v1
"Long-term monitoring of the evolution of the artificial night sky brightness
is a key tool for developing science-informed public policies and assessing the
efficacy of light pollution mitigation measures. Detecting the underlying
artificial brightness trend is a challenging task, since the typical night sky
brightness signal shows a large variability with characteristic time scales
ranging from seconds to years. In order to effectively isolate the weak
signature of the effect of interest, determining the potential long term drifts
of the radiance sensing systems is crucial. If these drifts can be adequately
characterized, the raw measurements could be easily corrected for them and
transformed to a consistent scale. In this short note we report on the
progressive darkening of the signal recorded by SQM detectors belonging to
several monitoring networks, permanently installed outdoors for periods ranging
from several months to several years. The sensitivity drifts were estimated by
means of parallel measurements made at the beginning and at the end of the
evaluation periods using reference detectors of the same kind that were little
or no exposed to weathering in the intervening time. Our preliminary results
suggest that SQM detectors installed outdoors steadily increase their readings
at an average rate of +0.034 magSQM/arcsec^2 per MWh/m^2 of exposure to solar
horizontal global irradiation, that for our locations translates into
approximately +0.05 to +0.06 magSQM/arcsec^2 per year.",http://arxiv.org/abs/2011.15044v2
"Tropical forests represent the home of many species on the planet for flora
and fauna, retaining billions of tons of carbon footprint, promoting clouds and
rain formation, implying a crucial role in the global ecosystem, besides
representing the home to countless indigenous peoples. Unfortunately, millions
of hectares of tropical forests are lost every year due to deforestation or
degradation. To mitigate that fact, monitoring and deforestation detection
programs are in use, in addition to public policies for the prevention and
punishment of criminals. These monitoring/detection programs generally use
remote sensing images, image processing techniques, machine learning methods,
and expert photointerpretation to analyze, identify and quantify possible
changes in forest cover. Several projects have proposed different computational
approaches, tools, and models to efficiently identify recent deforestation
areas, improving deforestation monitoring programs in tropical forests. In this
sense, this paper proposes the use of pattern classifiers based on
neuroevolution technique (NEAT) in tropical forest deforestation detection
tasks. Furthermore, a novel framework called e-NEAT has been created and
achieved classification results above $90\%$ for balanced accuracy measure in
the target application using an extremely reduced and limited training set for
learning the classification models. These results represent a relative gain of
$6.2\%$ over the best baseline ensemble method compared in this paper",http://arxiv.org/abs/2208.11058v1
"Emotion arcs capture how an individual (or a population) feels over time.
They are widely used in industry and research; however, there is little work on
evaluating the automatically generated arcs. This is because of the difficulty
of establishing the true (gold) emotion arc. Our work, for the first time,
systematically and quantitatively evaluates automatically generated emotion
arcs. We also compare two common ways of generating emotion arcs:
Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running
experiments on 18 diverse datasets in 9 languages, we show that despite being
markedly poor at instance level emotion classification, LexO methods are highly
accurate at generating emotion arcs when aggregating information from hundreds
of instances. We also show, through experiments on six indigenous African
languages, as well as Arabic, and Spanish, that automatic translations of
English emotion lexicons can be used to generate high-quality emotion arcs in
less-resource languages. This opens up avenues for work on emotions in
languages from around the world; which is crucial for commerce, public policy,
and health research in service of speakers often left behind. Code and
resources: https://github.com/dteodore/EmotionArcs",http://arxiv.org/abs/2306.02213v3
"The abundance of social media data has presented opportunities for accurately
determining public and group-specific stances around policy proposals or
controversial topics. In contrast with sentiment analysis which focuses on
identifying prevailing emotions, stance detection identifies precise positions
(i.e., supportive, opposing, neutral) relative to a well-defined topic, such as
perceptions toward specific global health interventions during the COVID-19
pandemic. Traditional stance detection models, while effective within their
specific domain (e.g., attitudes towards masking protocols during COVID-19),
often lag in performance when applied to new domains and topics due to changes
in data distribution. This limitation is compounded by the scarcity of
domain-specific, labeled datasets, which are expensive and labor-intensive to
create. A solution we present in this paper combines counterfactual data
augmentation with contrastive learning to enhance the robustness of stance
detection across domains and topics of interest. We evaluate the performance of
current state-of-the-art stance detection models, including a prompt-optimized
large language model, relative to our proposed framework succinctly called
STANCE-C3 (domain-adaptive Cross-target STANCE detection via Contrastive
learning and Counterfactual generation). Empirical evaluations demonstrate
STANCE-C3's consistent improvements over the baseline models with respect to
accuracy across domains and varying focal topics. Despite the increasing
prevalence of general-purpose models such as generative AI, specialized models
such as STANCE-C3 provide utility in safety-critical domains wherein precision
is highly valued, especially when a nuanced understanding of the concerns of
different population segments could result in crafting more impactful public
policies.",http://arxiv.org/abs/2309.15176v2
"Causal machine learning tools are beginning to see use in real-world policy
evaluation tasks to flexibly estimate treatment effects. One issue with these
methods is that the machine learning models used are generally black boxes,
i.e., there is no globally interpretable way to understand how a model makes
estimates. This is a clear problem in policy evaluation applications,
particularly in government, because it is difficult to understand whether such
models are functioning in ways that are fair, based on the correct
interpretation of evidence and transparent enough to allow for accountability
if things go wrong. However, there has been little discussion of transparency
problems in the causal machine learning literature and how these might be
overcome. This paper explores why transparency issues are a problem for causal
machine learning in public policy evaluation applications and considers ways
these problems might be addressed through explainable AI tools and by
simplifying models in line with interpretable AI principles. It then applies
these ideas to a case-study using a causal forest model to estimate conditional
average treatment effects for a hypothetical change in the school leaving age
in Australia. It shows that existing tools for understanding black-box
predictive models are poorly suited to causal machine learning and that
simplifying the model to make it interpretable leads to an unacceptable
increase in error (in this application). It concludes that new tools are needed
to properly understand causal machine learning models and the algorithms that
fit them.",http://arxiv.org/abs/2310.13240v2
"Discussion of ""Bayesian Models and Methods in Public Policy and Government
Settings"" by S. E. Fienberg [arXiv:1108.2177]",http://arxiv.org/abs/1108.3912v1
"We study a behavioral SIR model with time-varying costs of distancing. The
two main causes of the variation in the cost of distancing we explore are
distancing fatigue and public policies (lockdowns). We show that for a second
wave of an epidemic to arise, a steep increase in distancing cost is necessary.
Distancing fatigue cannot increase the distancing cost sufficiently fast to
create a second wave. However, public policies that discontinuously affect the
distancing cost can create a second wave. With that in mind, we characterize
the largest change in the distancing cost (due to, for example, lifting a
public policy) that will not cause a second wave. Finally, we provide a
numerical analysis of public policies under distancing fatigue and show that a
strict lockdown at the beginning of an epidemic (as, for example, recently in
China) can lead to unintended adverse consequences. When the policy is lifted
the disease spreads very fast due to the accumulated distancing fatigue of the
individuals causing high prevalence levels.",http://arxiv.org/abs/2206.03847v2
"Research software is increasingly recognized as a vital component of the
scholarly record. Journals offer authors the opportunity to publish research
software papers, but often have different requirements for how these
publications should be structured and how code should be verified. In this
short case study we gather data from 20 Physical Science journals to trace the
frequency, quality control, and publishing criteria for software papers. Our
goal with the case study is to provide a proof-of-concept for doing descriptive
empirical work with software publication policies across numerous domains of
science and engineering. In the narrative we therefore provide descriptive
statistics showing how these journals differ in criteria required for
archiving, linking, verifying, and documenting software as part of a formal
publication. The contribution of this preliminary work is twofold: 1. We
provide case study of Physical Science research software publications over
time; 2. We demonstrate the use of a new survey method for analyzing research
software publication policies. In our conclusion, we describe how comparative
research into software publication policies can provide better criteria and
requirements for an emerging software publication landscape.",http://arxiv.org/abs/2206.05367v1
"While outdoor advertisements are common features within towns and cities,
they may reinforce social inequalities in health. Vulnerable populations in
deprived areas may have greater exposure to fast food, gambling and alcohol
advertisements encouraging their consumption. Understanding who is exposed and
evaluating potential policy restrictions requires a substantial manual data
collection effort. To address this problem we develop a deep learning workflow
to automatically extract and classify unhealthy advertisements from
street-level images. We introduce the Liverpool 360 Street View (LIV360SV)
dataset for evaluating our workflow. The dataset contains 25,349, 360 degree,
street-level images collected via cycling with a GoPro Fusion camera, recorded
Jan 14th - 18th 2020. 10,106 advertisements were identified and classified as
food (1335), alcohol (217), gambling (149) and other (8405) (e.g., cars and
broadband). We find evidence of social inequalities with a larger proportion of
food advertisements located within deprived areas and those frequented by
students. Our project presents a novel implementation for the incidental
classification of street view images for identifying unhealthy advertisements,
providing a means through which to identify areas that can benefit from tougher
advertisement restriction policies for tackling social inequalities.",http://arxiv.org/abs/2007.04611v2
"In the 60s of the last century the few courses of History of physics in
Physics degree were held by scholars who, apart from a few exceptions, did not
have a specific research background in the field. Some activities, books,
social movements in the civil society allowed in the 70's the entry, among
Physics courses, of teachings in History of physics held by scholars
specifically trained for that job since their degree. A second change happened
in the 90s when many difficulties forced physicists to allocate fewer and fewer
resources to the History of their discipline. I'll outline the features of the
two periods and the efforts of Historians to find a proper space in Physics
departments.",http://arxiv.org/abs/2302.02438v2
"The model of Bonabeau explains the emergence of social hierarchies from the
memory of fights in an initially egalitarian society. Introducing a feedback
from the social inequality into the probability to win a fight, we find a sharp
transition between egalitarian society at low population density and
hierarchical society at high population density.",http://arxiv.org/abs/cond-mat/0208388v1
"High resolution crop type maps are an important tool for improving food
security, and remote sensing is increasingly used to create such maps in
regions that possess ground truth labels for model training. However, these
labels are absent in many regions, and models trained in other regions on
typical satellite features, such as those from optical sensors, often exhibit
low performance when transferred. Here we explore the use of NASA's Global
Ecosystem Dynamics Investigation (GEDI) spaceborne lidar instrument, combined
with Sentinel-2 optical data, for crop type mapping. Using data from three
major cropped regions (in China, France, and the United States) we first
demonstrate that GEDI energy profiles are capable of reliably distinguishing
maize, a crop typically above 2m in height, from crops like rice and soybean
that are shorter. We further show that these GEDI profiles provide much more
invariant features across geographies compared to spectral and phenological
features detected by passive optical sensors. GEDI is able to distinguish maize
from other crops within each region with accuracies higher than 84%, and able
to transfer across regions with accuracies higher than 82% compared to 64% for
transfer of optical features. Finally, we show that GEDI profiles can be used
to generate training labels for models based on optical imagery from
Sentinel-2, thereby enabling the creation of 10m wall-to-wall maps of tall
versus short crops in label-scarce regions. As maize is the second most widely
grown crop in the world and often the only tall crop grown within a landscape,
we conclude that GEDI offers great promise for improving global crop type maps.",http://arxiv.org/abs/2109.06972v1
"China's pledge to reach carbon neutrality before 2060 is an ambitious goal
and could provide the world with much-needed leadership on how to limit warming
to +1.5C warming above pre-industrial levels by the end of the century. But the
pathways that would achieve net zero by 2060 are still unclear, including the
role of negative emissions technologies. We use the Global Change Analysis
Model to simulate how negative emissions technologies, in general, and direct
air capture (DAC) in particular, could contribute to China's meeting this
target. Our results show that negative emissions could play a large role,
offsetting on the order of 3 GtCO2 per year from difficult-to-mitigate sectors
such as freight transportation and heavy industry. This includes up to a 1.6
GtCO2 per year contribution from DAC, constituting up to 60% of total projected
negative emissions in China. But DAC, like bioenergy with carbon capture and
storage and afforestation, has not yet been demonstrated at anywhere
approaching the scales required to meaningfully contribute to climate
mitigation. Deploying NETs at these scales will have widespread impacts on
financial systems and natural resources such as water, land, and energy in
China.",http://arxiv.org/abs/2010.06723v2
"The media frequently describes the 2017 Charlottesville 'Unite the Right'
rally as a turning point for the alt-right and white supremacist movements.
Social movement theory suggests that the media attention and public discourse
concerning the rally may have influenced the alt-right, but this has yet to be
empirically tested. The current study investigates whether there are
differences in language use between 7,142 alt-right and progressive YouTube
channels, in addition to measuring possible changes as a result of the rally.
To do so, we create structural topic models and measure bigram proportions in
video transcripts, spanning eight weeks before to eight weeks after the rally.
We observe differences in topics between the two groups, with the 'alternative
influencers' for example discussing topics related to race and free speech to
an increasing and larger extent than progressive channels. We also observe
structural breakpoints in the use of bigrams at the time of the rally,
suggesting there are changes in language use within the two groups as a result
of the rally. While most changes relate to mentions of the rally itself, the
alternative group also shows an increase in promotion of their YouTube
channels. Results are discussed in light of social movement theory, followed by
a discussion of potential implications for understanding the alt-right and
their language use on YouTube.",http://arxiv.org/abs/1908.11599v2
"Since the fatal shooting of 17-year old Black teenager Trayvon Martin in
February 2012 by a White neighborhood watchman, George Zimmerman in Sanford,
Florida, there has been a significant increase in digital activism addressing
police-brutality related and racially-motivated incidents in the United States.
In this work, we administer an innovative study of digital activism by
exploiting social media as an authoritative tool to examine and analyze the
linguistic cues and thematic relationships in these three mediums. We conduct a
multi-level text analysis on 36,984,559 tweets to investigate users' behaviors
to examine the language used and understand the impact of digital activism on
social media within each social movement on a sentence-level, word-level, and
topic-level. Our results show that excessive use of racially-related or
prejudicial hashtags were used by the counter protests which portray potential
discriminatory tendencies. Consequently, our findings highlight that social
activism done by Black Lives Matter activists does not diverge from the social
issues and topics involving police-brutality related and racially-motivated
killings of Black individuals due to the shape of its topical graph that topics
and conversations encircling the largest component directly relate to the topic
of Black Lives Matter. Finally, we see that both Blue Lives Matter and All
Lives Matter movements depict a different directive, as the topics of Blue
Lives Matter or All Lives Matter do not reside in the center. These findings
suggest that topics and conversations within each social movement are skewed,
random or possessed racially-related undertones, and thus, deviating from the
prominent social injustice issues.",http://arxiv.org/abs/2109.12192v1
"To achieve the ambitious aims of the Paris climate agreement, the majority of
fossil-fuel reserves needs to remain underground. As current national
government commitments to mitigate greenhouse gas emissions are insufficient by
far, actors such as institutional and private investors and the social movement
on divestment from fossil fuels could play an important role in putting
pressure on national governments on the road to decarbonization. Using a
stochastic agent-based model of co-evolving financial market and investors'
beliefs about future climate policy on an adaptive social network, here we find
that the dynamics of divestment from fossil fuels shows potential for social
tipping away from a fossil-fuel based economy. Our results further suggest that
socially responsible investors have leverage: a small share of 10--20\,\% of
such moral investors is sufficient to initiate the burst of the carbon bubble,
consistent with the Pareto Principle. These findings demonstrate that
divestment has potential for contributing to decarbonization alongside other
social movements and policy instruments, particularly given the credible
imminence of strong international climate policy. Our analysis also indicates
the possible existence of a carbon bubble with potentially destabilizing
effects to the economy.",http://arxiv.org/abs/1902.07481v1
"As the uni-cultural studies of website usability have matured, the paucity of
cross-cultural studies of usability become increasingly apparent. Moving toward
these cross-cultural studies will require the development of a new tool to
assess website usability in the context of cultural dimensions. This paper
introduces the preliminary results from the first phase of this project and
then presents the proposed method for the research in progress that
specifically is directed to the development and quantitative evaluation of a
measurement scale of a culture sensitive measurement of website usability. The
recognition of the need to develop this scale resulted from the identification
of culture-related shortcomings of previous measurement tools that have been
used widely within the Management of Information Systems (MIS) literature.",http://arxiv.org/abs/1803.04074v1
"In this paper we analyze the effects of commuting and social inequalities for
the epidemic development of the novel coronavirus (COVID-19). With this aim we
consider a SEIRD (susceptible, exposed, infected, recovered and dead by
disease) model without vital dynamics in a population divided into patches that
have different economic resources and in which the individuals can commute from
one patch to another (bilaterally). In the modeling we choose the social and
commuting parameters arbitrarily. We calculate the basic reproductive number
$R_0$ with the next generation approach and analyze the sensitivity of $R_0$
with respect to the parameters. Furthermore, we run numerical simulations
considering a population divided into two patches to bring some conclusions on
the number of total infected individuals and cumulative deaths for our model
considering heterogeneous populations.",http://arxiv.org/abs/2008.06718v1
"We have studied few social inequality measures associated with the
sub-critical dynamical features (measured in terms of the avalanche size
distributions) of four self-organized critical models while the corresponding
systems approach their respective stationary critical states. It has been
observed that these inequality measures (specifically the Gini and Kolkata
indices) exhibit nearly universal values though the models studied here are
widely different, namely the Bak-Tang-Wiesenfeld sandpile, the Manna sandpile
and the quenched Edwards-Wilkinson interface, and the fiber bundle interface.
These observations suggest that the self-organized critical systems have broad
similarity in terms of these inequality measures. A comparison with similar
earlier observations in the data of socio-economic systems with unrestricted
competitions suggest the emergent inequality as a result of the possible
proximity to the self-organized critical states.",http://arxiv.org/abs/2111.11230v3
"The importance of the working document is that it allows the analysis of
information and cases associated with (SARS-CoV-2) COVID-19, based on the daily
information generated by the Government of Mexico through the Secretariat of
Health, responsible for the Epidemiological Surveillance System for Viral
Respiratory Diseases (SVEERV). The information in the SVEERV is disseminated as
open data, and the level of information is displayed at the municipal, state
and national levels. On the other hand, the monitoring of the genomic
surveillance of (SARS-CoV-2) COVID-19, through the identification of variants
and mutations, is registered in the database of the Information System of the
Global Initiative on Sharing All Influenza Data (GISAID) based in Germany.
These two sources of information SVEERV and GISAID provide the information for
the analysis of the impact of (SARS-CoV-2) COVID-19 on the population in
Mexico. The first data source identifies information, at the national level, on
patients according to age, sex, comorbidities and COVID-19 presence
(SARS-CoV-2), among other characteristics. The data analysis is carried out by
means of the design of an algorithm applying data mining techniques and
methodology, to estimate the case fatality rate, positivity index and identify
a typology according to the severity of the infection identified in patients
who present a positive result. for (SARS-CoV-2) COVID-19. From the second data
source, information is obtained worldwide on the new variants and mutations of
COVID-19 (SARS-CoV-2), providing valuable information for timely genomic
surveillance. This study analyzes the impact of (SARS-CoV-2) COVID-19 on the
indigenous language-speaking population, it allows us to provide information,
quickly and in a timely manner, to support the design of public policy on
health.",http://arxiv.org/abs/2112.01276v1
"The COVID-19 pandemic (SARS-CoV-2 virus) is the defying global health crisis
of our time. The absence of mass testing and the relevant presence of
asymptomatic individuals causes the available data of the COVID-19 pandemic in
Brazil to be largely under-reported regarding the number of infected
individuals and deaths. We propose an adapted Susceptible-Infected-Recovered
(SIR) model which explicitly incorporates the under-reporting and the response
of the population to public policies (such as confinement measures, widespread
use of masks, etc) to cast short-term and long-term predictions. Large amounts
of uncertainty could provide misleading models and predictions. In this paper,
we discuss the role of uncertainty in these prediction, which is illustrated
regarding three key aspects. First, assuming that the number of infected
individuals is under-reported, we demonstrate an anticipation regarding the
peak of infection. Furthermore, while a model with a single class of infected
individuals yields forecasts with increased peaks, a model that considers both
symptomatic and asymptomatic infected individuals suggests a decrease of the
peak of symptomatic. Second, considering that the actual amount of deaths is
larger than what is being register, then demonstrate the increase of the
mortality rates. Third, when consider generally under-reported data, we
demonstrate how the transmission and recovery rate model parameters change
qualitatively and quantitatively. We also investigate the effect of the
""COVID-19 under-reporting tripod"", i.e. the under-reporting in terms of
infected individuals, of deaths and the true mortality rate. If two of these
factors are known, the remainder can be inferred, as long as proportions are
kept constant. The proposed approach allows one to determine the margins of
uncertainty by assessments on the observed and true mortality rates.",http://arxiv.org/abs/2006.15268v1
"Quantifying variable importance is essential for answering high-stakes
questions in fields like genetics, public policy, and medicine. Current methods
generally calculate variable importance for a given model trained on a given
dataset. However, for a given dataset, there may be many models that explain
the target outcome equally well; without accounting for all possible
explanations, different researchers may arrive at many conflicting yet equally
valid conclusions given the same data. Additionally, even when accounting for
all possible explanations for a given dataset, these insights may not
generalize because not all good explanations are stable across reasonable data
perturbations. We propose a new variable importance framework that quantifies
the importance of a variable across the set of all good models and is stable
across the data distribution. Our framework is extremely flexible and can be
integrated with most existing model classes and global variable importance
metrics. We demonstrate through experiments that our framework recovers
variable importance rankings for complex simulation setups where other methods
fail. Further, we show that our framework accurately estimates the true
importance of a variable for the underlying data distribution. We provide
theoretical guarantees on the consistency and finite sample error rates for our
estimator. Finally, we demonstrate its utility with a real-world case study
exploring which genes are important for predicting HIV load in persons with
HIV, highlighting an important gene that has not previously been studied in
connection with HIV. Code is available at
https://github.com/jdonnelly36/Rashomon_Importance_Distribution.",http://arxiv.org/abs/2309.13775v4
"Same-race mentorship preference refers to mentors or mentees forming
connections significantly influenced by a shared race. Although racial
diversity in science has been well-studied and linked to favorable outcomes,
the extent and effects of same-race mentorship preferences remain largely
underexplored. Here, we analyze 465,355 mentor-mentee pairs from more than 60
research areas over the last 70 years to investigate the effect of same-race
mentorship preferences on mentees' academic performance and survival. We use
causal inference and statistical matching to measure same-race mentorship
preferences while accounting for racial demographic variations across
institutions, time periods, and research fields. Our findings reveal a
pervasive same-race mentorship propensity across races, fields, and
universities of varying research intensity. We observe an increase in same-race
mentorship propensity over the years, further reinforced inter-generationally
within a mentorship lineage. This propensity is more pronounced for minorities
(Asians, Blacks, and Hispanics). Our results reveal that mentees under the
supervision of mentors with high same-race propensity experience significantly
lower productivity, impact, and collaboration reach during and after training,
ultimately leading to a 27.6% reduced likelihood of remaining in academia. In
contrast, a mentorship approach devoid of racial propensity appears to offer
the best prospects for academic performance and persistence. These findings
underscore the importance of mentorship diversity for academic success and shed
light on factors contributing to minority underrepresentation in science.",http://arxiv.org/abs/2310.09453v2
"The design of coherent and efficient policies to address infectious diseases
and their consequences requires to model not only epidemics dynamics, but also
individual behaviors, as the latter has a strong influence on the former. In
our work, we provide a theoretical model for this problem, taking into account
the social structure of a population. This model is based on a Mean Field Game
version of a SIR compartmental model, in which individuals are grouped by their
age class and interact together in different settings. This social
heterogeneity allows to reproduce realistic situations while remaining usable
in practice. In our game theoretical approach, individuals can choose to limit
their contacts by making a trade-off between the risks incurred by infection
and the cost of being confined. The aggregation of all these individual choices
and optimizations forms a Nash equilibrium through a system of coupled
equations that we derive and solve numerically. The global cost born by the
population within this scenario is then compared to its societal optimum
counterpart (i.e. the optimal cost from the society viewpoint), and we
investigate how the gap between these two costs can be partially bridged within
a constrained Nash equilibrium for which a governmental institution would
impose lockdowns. Finally we consider the consequences of the finiteness of the
population size $N$, or of a time $T$ at which an external event would end the
epidemic, and show that the variation of these parameters could lead to first
order phase transitions in the choice of optimal strategies. In this paper, all
the strategies considered to mitigate epidemics correspond to
non-pharmaceutical interventions (NPI), and we provide here a theoretical
framework within which guidelines for public policies depending on the
characteristics of an epidemic and on the cost of restrictions on the society
could be assessed.",http://arxiv.org/abs/2404.08758v1
"Public Policy involves proposing changes to existing practices, alternatives,
new habits. Citizens and institutions react accordingly, accepting, refuting or
adapting. Agent-based modeling is a tool that can enrich the policy analysis
package explicitly considering dynamics, space and individual-level
interactions. This paper presents a modeling platform called PolicySpace that
models public policies within an empirical, spatial environment using data from
46 metropolitan regions in Brazil. We describe the basics of the model, its
agents and markets, the tax scheme, the parametrization, and how to run the
model. Finally, we validate the model and demonstrate an application of the
fiscal analysis. Besides providing the basics of the platform, our results
indicate the relevance of the rules of taxes transfer for cities' quality of
life.",http://arxiv.org/abs/1801.00259v1
"In this study we performed an initial investigation and evaluation of
altmetrics and their relationship with public policy citation of research
papers. We examined methods for using altmetrics and other data to predict
whether a research paper is cited in public policy and applied receiver
operating characteristic curve on various feature groups in order to evaluate
their potential usefulness. From the methods we tested, classifying based on
tweet count provided the best results, achieving an area under the ROC curve of
0.91.",http://arxiv.org/abs/1708.01658v1
"This paper has been withdrawn by the authors due to the violation of ATLAS
experiment publication policy.",http://arxiv.org/abs/0705.2001v2
"Rejoinder of ""Bayesian Models and Methods in Public Policy and Government
Settings"" by S. E. Fienberg [arXiv:1108.2177]",http://arxiv.org/abs/1108.3914v1
"Explainability is highly-desired in Machine Learning (ML) systems supporting
high-stakes policy decisions in areas such as health, criminal justice,
education, and employment. While the field of explainable ML has expanded in
recent years, much of this work has not taken real-world needs into account. A
majority of proposed methods are designed with \textit{generic} explainability
goals without well-defined use-cases or intended end-users and evaluated on
simplified tasks, benchmark problems/datasets, or with proxy users (e.g., AMT).
We argue that these simplified evaluation settings do not capture the nuances
and complexities of real-world applications. As a result, the applicability and
effectiveness of this large body of theoretical and methodological work in
real-world applications are unclear. In this work, we take steps toward
addressing this gap for the domain of public policy. First, we identify the
primary use-cases of explainable ML within public policy problems. For each use
case, we define the end-users of explanations and the specific goals the
explanations have to fulfill. Finally, we map existing work in explainable ML
to these use-cases, identify gaps in established capabilities, and propose
research directions to fill those gaps to have a practical societal impact
through ML. The contribution is 1) a methodology for explainable ML researchers
to identify use cases and develop methods targeted at them and 2) using that
methodology for the domain of public policy and giving an example for the
researchers on developing explainable ML methods that result in real-world
impact.",http://arxiv.org/abs/2010.14374v3
"Objective: The aim of this research is to demonstrate how the use of
hierarchical cluster analysis on 366 municipalities and other minor entities
(parishes) of Venezuela, could be useful to consider regional differences and
similarities between territorial entities when designing national public
policies of Water, Sanitation and Hygiene (WASH) based on evidence. Methods and
results: Consider data from various sources to characterize the population of
Venezuela through their territorial entities. Select variables at the level of
the territorial entities to cover demographic characteristics, mortality and
nutrition, coverage of reliable water and sanitation services, access to
education, and access to information and communication technologies. Classify
the territorial entities into a limited number of mutually exclusive groups
using hierarchical clustering techniques and based on proximity in the
multi-dimensional space. Adjust of assignments, reallocating some entities into
a different group based on the specialists' opinion about its hierarchy in the
cities regional system and its geographic location. Define an indicator to
verify the consistency of the groups built. Conduct a statistical analysis to
confirm separation of the groups. Demonstrate the utility of the results with
some examples of common analysis when building a sanitary public policy, using
seven distinct groups of recommendations depending on each cluster.
Conclusions: Cluster analysis can be a useful method to analyse relevant
differences between territorial entities when designing national public
policies based on evidence.",http://arxiv.org/abs/2301.12604v1
"This position paper offers a framework to think about how to better involve
human influence in algorithmic decision-making of contentious public policy
issues. Drawing from insights in communication literature, we introduce a
""public(s)-in-the-loop"" approach and enumerates three features that are central
to this approach: publics as plural political entities, collective
decision-making through deliberation, and the construction of publics. It
explores how these features might advance our understanding of stakeholder
participation in AI design in contentious public policy domains such as
recidivism prediction. Finally, it sketches out part of a research agenda for
the HCI community to support this work.",http://arxiv.org/abs/2204.10814v1
"This document has been prepared as a Snowmass contributed paper by the Public
Policy \& Government Engagement topical group (CEF06) within the Community
Engagement Frontier. The charge of CEF06 is to review all aspects of how the
High Energy Physics (HEP) community engages with government at all levels and
how public policy impacts members of the community and the community at large,
and to assess and raise awareness within the community of direct
community-driven engagement of the U.S. federal government (\textit{i.e.}
advocacy). The focus of this paper is the advocacy undertaken by the HEP
community that pertains directly to the funding of the field by the U.S.
federal government.",http://arxiv.org/abs/2207.00122v2
"This document has been prepared as a Snowmass contributed paper by the Public
Policy \& Government Engagement topical group (CEF06) within the Community
Engagement Frontier. The charge of CEF06 is to review all aspects of how the
High Energy Physics (HEP) community engages with government at all levels and
how public policy impacts members of the community and the community at large,
and to assess and raise awareness within the community of direct
community-driven engagement of the US federal government (\textit{i.e.}
advocacy). The focus of this paper is the potential for HEP community advocacy
on topics other than funding for basic research.",http://arxiv.org/abs/2207.00124v2
"This document has been prepared as a Snowmass contributed paper by the Public
Policy & Government Engagement topical group (CEF06) within the Community
Engagement Frontier. The charge of CEF06 is to review all aspects of how the
High Energy Physics (HEP) community engages with government at all levels and
how public policy impacts members of the community and the community at large,
and to assess and raise awareness within the community of direct
community-driven engagement of the US federal government (i.e. advocacy). The
focus of this paper is HEP community engagement of government entities other
than the U.S. federal legislature (i.e. Congress).",http://arxiv.org/abs/2207.00125v2
"Black-box optimization (BBO) has become increasingly relevant for tackling
complex decision-making problems, especially in public policy domains such as
police redistricting. However, its broader application in public policymaking
is hindered by the complexity of defining feasible regions and the
high-dimensionality of decisions. This paper introduces a novel BBO framework,
termed as the Conditional And Generative Black-box Optimization (CageBO). This
approach leverages a conditional variational autoencoder to learn the
distribution of feasible decisions, enabling a two-way mapping between the
original decision space and a simplified, constraint-free latent space. The
CageBO efficiently handles the implicit constraints often found in public
policy applications, allowing for optimization in the latent space while
evaluating objectives in the original space. We validate our method through a
case study on large-scale police redistricting problems in Atlanta, Georgia.
Our results reveal that our CageBO offers notable improvements in performance
and efficiency compared to the baselines.",http://arxiv.org/abs/2310.18449v4
"Journalists must find stories in huge amounts of textual data (e.g. leaks,
bills, press releases) as part of their jobs: determining when and why text
becomes news can help us understand coverage patterns and help us build
assistive tools. Yet, this is challenging because very few labelled links
exist, language use between corpora is very different, and text may be covered
for a variety of reasons. In this work we focus on news coverage of local
public policy in the San Francisco Bay Area by the San Francisco Chronicle.
First, we gather news articles, public policy documents and meeting recordings
and link them using probabilistic relational modeling, which we show is a
low-annotation linking methodology that outperforms other retrieval-based
baselines. Second, we define a new task: newsworthiness prediction, to predict
if a policy item will get covered. We show that different aspects of public
policy discussion yield different newsworthiness signals. Finally we perform
human evaluation with expert journalists and show our systems identify policies
they consider newsworthy with 68% F1 and our coverage recommendations are
helpful with an 84% win-rate.",http://arxiv.org/abs/2311.09734v1
"This study investigates the impact of disinformation on public policies.
Using 28 sets of keywords in eight databases, a systematic review was carried
out following the Prisma 2020 model (Page et al., 2021). After applying filters
and inclusion and exclusion criteria to 4,128 articles and materials found, 46
publications were analyzed, resulting in 23 disinformation impact categories.
These categories were organized into two main axes: State and Society and
Actors and Dynamics, covering impacts on State actors, society actors, State
dynamics and society dynamics. The results indicate that disinformation affects
public decisions, adherence to policies, prestige of institutions, perception
of reality, consumption, public health and other aspects. Furthermore, this
study suggests that disinformation should be treated as a public problem and
incorporated into the public policy research agenda, contributing to the
development of strategies to mitigate its effects on government actions.",http://arxiv.org/abs/2406.00951v1
"The implementation of public policies is crucial in controlling the spread of
COVID-19. However, the effectiveness of different policies can vary across
different aspects of epidemic containment. Identifying the most effective
policies is essential for providing informed recommendations for pandemic
control. This paper examines the relationship between various public policy
responses and their impact on COVID-19 containment. Using the propensity score
matching-difference in differences (PSM-DID) model to address endogeneity, we
analyze the causal significance of each policy on epidemic control. Our
analysis reveals that that policies related to vaccine delivery, debt relief,
and the cancellation of public events are the most effective measures. These
findings provide key insights for policymakers, highlighting the importance of
focusing on specific, high-impact measures in managing public health crises.",http://arxiv.org/abs/2408.14108v1
"This article surveys the use of algorithmic systems to support
decision-making in the public sector. Governments adopt, procure, and use
algorithmic systems to support their functions within several contexts --
including criminal justice, education, and benefits provision -- with important
consequences for accountability, privacy, social inequity, and public
participation in decision-making. We explore the social implications of
municipal algorithmic systems across a variety of stages, including problem
formulation, technology acquisition, deployment, and evaluation. We highlight
several open questions that require further empirical research.",http://arxiv.org/abs/2106.03673v2
"This study analyzes the role of meritocracy, media influence, and scheduled
theory from multiple perspectives as mechanisms that maintain inequality in
social classes. Social inequality exists in complex forms in the educational,
media, and political spheres. The study focuses on how inequality in society is
structured and reproduced and how the theory of scheduled harmony justifies
this.",http://arxiv.org/abs/2311.02445v2
"This paper is a critical reflection on the epistemic culture of contemporary
computational linguistics, framed in the context of its growing obsession with
tables with numbers. We argue against tables with numbers on the basis of their
epistemic irrelevance, their environmental impact, their role in enabling and
exacerbating social inequalities, and their deep ties to commercial
applications and profit-driven research. We substantiate our arguments with
empirical evidence drawn from a meta-analysis of computational linguistics
research over the last decade.",http://arxiv.org/abs/2408.06062v3
"The book argues ICT are part of the set of goods and services that determine
quality of life, social inequality and the chances for economic development.
Therefore understanding the digital divide demands a broader discussion of the
place of ICT within each society and in the international system. The author
argues against the perspectives that either isolates ICT from other basic
social goods (in particular education and employment) as well as those that
argue that new technologies are luxury of a consumer society. Though the author
accepts that new technologies are not a panacea for the problems of inequality,
access to them become a condition of full integration of social life. Using
examples mainly from Latin America, the work presents some general policy
proposals on the fight against the digital divide which take in consideration
other dimensions of social inequality and access to public goods.
  Bernardo Sorj was born in Montevideo, Uruguay. He is a naturalized Brazilian,
living in Brazil since 1976. He studied anthropology and philosophy in Uruguay,
and holds a B.A. and an M.A. in History and Sociology from Haifa University,
Israel. He received his Ph.D. in Sociology from the University of Manchester in
England. Sorj was a professor at the Department of Political Science at the
Federal University of Minas Gerais and at the Institute for International
Relations, PUC/RJ. The author of 20 books and more than 100 articles, was
visiting professor and chair at many European and North American
universities...",http://arxiv.org/abs/0807.2743v1
"Social inequality manifested across different strata of human existence can
be quantified in several ways. Here we compute non-entropic measures of
inequality such as Lorenz curve, Gini index and the recently introduced $k$
index analytically from known distribution functions. We characterize the
distribution functions of different quantities such as votes, journal
citations, city size, etc. with suitable fits, compute their inequality
measures and compare with the analytical results. A single analytic function is
often not sufficient to fit the entire range of the probability distribution of
the empirical data, and fit better to two distinct functions with a single
crossover point. Here we provide general formulas to calculate these inequality
measures for the above cases. We attempt to specify the crossover point by
minimizing the gap between empirical and analytical evaluations of measures.
Regarding the $k$ index as an `extra dimension', both the lower and upper
bounds of the Gini index are obtained as a function of the $k$ index. This type
of inequality relations among inequality indices might help us to check the
validity of empirical and analytical evaluations of those indices.",http://arxiv.org/abs/1406.2874v1
"Controversies around race and machine learning have sparked debate among
computer scientists over how to design machine learning systems that guarantee
fairness. These debates rarely engage with how racial identity is embedded in
our social experience, making for sociological and psychological complexity.
This complexity challenges the paradigm of considering fairness to be a formal
property of supervised learning with respect to protected personal attributes.
Racial identity is not simply a personal subjective quality. For people labeled
""Black"" it is an ascribed political category that has consequences for social
differentiation embedded in systemic patterns of social inequality achieved
through both social and spatial segregation. In the United States, racial
classification can best be understood as a system of inherently unequal status
categories that places whites as the most privileged category while signifying
the Negro/black category as stigmatized. Social stigma is reinforced through
the unequal distribution of societal rewards and goods along racial lines that
is reinforced by state, corporate, and civic institutions and practices. This
creates a dilemma for society and designers: be blind to racial group
disparities and thereby reify racialized social inequality by no longer
measuring systemic inequality, or be conscious of racial categories in a way
that itself reifies race. We propose a third option. By preceding group
fairness interventions with unsupervised learning to dynamically detect
patterns of segregation, machine learning systems can mitigate the root cause
of social disparities, social segregation and stratification, without further
anchoring status categories of disadvantage.",http://arxiv.org/abs/1811.11668v1
"Human deaths caused by individual man-made conflicts (e.g., wars,
armed-conflicts, terrorist-attacks etc.) occur unequally across the events
(conflicts) and such inequality (in deaths) have been studied here using Lorenz
curve and values of the inequality indices Gini ($g$) and Kolkata ($k$) have
been estimated from it. The data are taken from various well-known databases
maintained by some Universities and Peace Research Institutes. The inequality
measures for man-made conflicts are found to have very high values ($g$ = $0.82
~\pm~ $0.02, $k$ = $0.84~ \pm~ $0.02), which is rarely seen in economic (income
or wealth) inequality measures across the world ($g \leq 0.4$, $k \leq 0.6$;
presumably because of various welfare measures). We also investigated the
inequalities in human deaths from natural disasters (like earthquakes, floods,
etc.). Interestingly, we observe that the social inequality measures ($g$ and
$k$ values) from man-made conflicts compare well with those of academic centers
(inequality in citations; found in earlier studies) of different institutions
of the world, while those for natural disasters can be even higher. We discuss
about the `similarity classes' of social inequality (similar higher values of
$g$ and $k$ indices) for man-made competitive societies like academic
institutions and man-made social conflicts, and connect our observations with
that of the growing recent trend of economic inequality across the world (with
rapid disappearance of welfare strategies).",http://arxiv.org/abs/1812.05709v8
"Social inequalities are ubiquitous and here we show that the values of the
Gini ($g$) and Kolkata ($k$) indices, two generic inequality indices, approach
each other (starting from $g = 0$ and $k = 0.5$ for equality) as the
competitions grow in various social institutions like markets, universities,
elections, etc. It is further showed that these two indices become equal and
stabilize at a value (at $g = k \simeq 0.87$) under unrestricted competitions.
We propose to view this coincidence of inequality indices as a generalized
version of the (more than a) century old 80-20 law of Pareto. Furthermore, the
coincidence of the inequality indices noted here is very similar to the ones
seen before for self-organized critical (SOC) systems. The observations here,
therefore, stand as a quantitative support towards viewing interacting
socio-economic systems in the framework of SOC, an idea conjectured for years.",http://arxiv.org/abs/2111.07516v5
"Statistical physicists and social scientists both study extensively some
characteristic features of the unequal distributions of energy, cluster or
avalanche sizes and of income, wealth etc among the particles (or sites) and
population respectively. While physicists concentrate on the self-similar
(fractal) structure (and the characteristic exponents) of the largest
(percolating) cluster or avalanche, social scientists study the inequality
indices like Gini and Kolkata etc given by the non-linearity of the Lorenz
function representing the cumulative fraction of the wealth possessed by
different fraction of the population. We review here, using results from
earlier publications and some new numerical as well as analytical results, how
the above-mentioned social inequality indices, when extracted from the unequal
distributions of energy (in kinetic exchange models), cluster sizes (in
percolation models) or avalanche sizes (in self-organized critical or fiber
bundle models) can help in a major way in providing precursor signals for an
approaching critical point or imminent failure point. Extensive numerical and
some analytical results have been discussed.",http://arxiv.org/abs/2207.04276v3
"Visualization research often focuses on perceptual accuracy or helping
readers interpret key messages. However, we know very little about how chart
designs might influence readers' perceptions of the people behind the data.
Specifically, could designs interact with readers' social cognitive biases in
ways that perpetuate harmful stereotypes? For example, when analyzing social
inequality, bar charts are a popular choice to present outcome disparities
between race, gender, or other groups. But bar charts may encourage deficit
thinking, the perception that outcome disparities are caused by groups'
personal strengths or deficiencies, rather than external factors. These faulty
personal attributions can then reinforce stereotypes about the groups being
visualized. We conducted four experiments examining design choices that
influence attribution biases (and therefore deficit thinking). Crowdworkers
viewed visualizations depicting social outcomes that either mask variability in
data, such as bar charts or dot plots, or emphasize variability in data, such
as jitter plots or prediction intervals. They reported their agreement with
both personal and external explanations for the visualized disparities.
Overall, when participants saw visualizations that hide within-group
variability, they agreed more with personal explanations. When they saw
visualizations that emphasize within-group variability, they agreed less with
personal explanations. These results demonstrate that data visualizations about
social inequity can be misinterpreted in harmful ways and lead to stereotyping.
Design choices can influence these biases: Hiding variability tends to increase
stereotyping while emphasizing variability reduces it.",http://arxiv.org/abs/2208.04440v2
"This two-part paper presents a new approach to predictive analysis for social
processes. Part I identifies a class of social processes, called positive
externality processes, which are both important and difficult to predict, and
introduces a multi-scale, stochastic hybrid system modeling framework for these
systems. In Part II of the paper we develop a systems theory-based,
computationally tractable approach to predictive analysis for these systems.
Among other capabilities, this analytic methodology enables assessment of
process predictability, identification of measurables which have predictive
power, discovery of reliable early indicators for events of interest, and
robust, scalable prediction. The potential of the proposed approach is
illustrated through case studies involving online markets, social movements,
and protest behavior.",http://arxiv.org/abs/0907.2313v1
"The spread of ideas across a social network can be studied using complex
contagion models, in which agents are activated by contact with multiple
activated neighbors. The investigation of complex contagions can provide
crucial insights into social influence and behavior-adoption cascades on
networks. In this paper, we introduce a model of a multi-stage complex
contagion on networks. Agents at different stages --- which could, for example,
represent differing levels of support for a social movement or differing levels
of commitment to a certain product or idea --- exert different amounts of
influence on their neighbors. We demonstrate that the presence of even one
additional stage introduces novel dynamical behavior, including interplay
between multiple cascades, that cannot occur in single-stage contagion models.
We find that cascades --- and hence collective action --- can be driven not
only by high-stage influencers but also by low-stage influencers.",http://arxiv.org/abs/1111.1596v2
"Using social media archives of the 2011 Chilean student unrest and dynamic
social network analysis, we study how leaders and participants use social media
such as Twitter, and the Web to self-organize and communicate with each other,
and thus generate one of the biggest ""smart movements"" in the history of Chile.
In this paper we i) describe the basic network topology of the 2011 student-led
social movement in Chile; ii) explore how the student leaders are connected to,
and how are they seen by (a) political leaders, and (b) University authorities;
iii) hypothesize about key success factors and risk variables for the Student
Network Movement's organization process and sustainability over time. We
contend that this social media enabled massive movement is yet another
manifestation of the network era, which leverages agents' socio-technical
networks, and thus accelerates how agents coordinate, mobilize resources and
enact collective intelligence.",http://arxiv.org/abs/1204.3939v1
"We examine the temporal evolution of digital communication activity relating
to the American anti-capitalist movement Occupy Wall Street. Using a
high-volume sample from the microblogging site Twitter, we investigate changes
in Occupy participant engagement, interests, and social connectivity over a
fifteen month period starting three months prior to the movement's first
protest action. The results of this analysis indicate that, on Twitter, the
Occupy movement tended to elicit participation from a set of highly
interconnected users with pre-existing interests in domestic politics and
foreign social movements. These users, while highly vocal in the months
immediately following the birth of the movement, appear to have lost interest
in Occupy related communication over the remainder of the study period.",http://arxiv.org/abs/1306.5474v1
"How does political discourse spread in digital networks? Can we empirically
test if certain conceptual frames of social movements have a correlate on their
online discussion networks? Through an analysis of the Twitter data from the
Occupy movement, this paper describes the formation of political discourse over
time. Building on a previous set of concepts - derived from theoretical
discussions about the movement and its roots - we analyse the data to observe
when those concepts start to appear within the networks, who are those Twitter
users responsible for them, and what are the patterns through which those
concepts spread. Preliminary evidence shows that, although there are some signs
of opportunistic behaviour among activists, most of them are central nodes from
the onset of the network, and shape the discussions across time. These central
activists do not only start the conversations around given frames, but also
sustain over time and become key members of the network. From here, we aim to
provide a thorough account of the ""travel"" of political discourse, and the
correlate of online conversational networks with theoretical accounts of the
movement.",http://arxiv.org/abs/1308.1176v1
"Searching for influential spreaders in complex networks is an issue of great
significance for applications across various domains, ranging from the epidemic
control, innovation diffusion, viral marketing, social movement to idea
propagation. In this paper, we first display some of the most important
theoretical models that describe spreading processes, and then discuss the
problem of locating both the individual and multiple influential spreaders
respectively. Recent approaches in these two topics are presented. For the
identification of privileged single spreaders, we summarize several widely used
centralities, such as degree, betweenness centrality, PageRank, k-shell, etc.
We investigate the empirical diffusion data in a large scale online social
community -- LiveJournal. With this extensive dataset, we find that various
measures can convey very distinct information of nodes. Of all the users in
LiveJournal social network, only a small fraction of them involve in spreading.
For the spreading processes in LiveJournal, while degree can locate nodes
participating in information diffusion with higher probability, k-shell is more
effective in finding nodes with large influence. Our results should provide
useful information for designing efficient spreading strategies in reality.",http://arxiv.org/abs/1312.6335v1
"Peer production projects like Wikipedia have inspired voluntary associations,
collectives, social movements, and scholars to embrace open online
collaboration as a model of democratic organization. However, many peer
production projects exhibit entrenched leadership and deep inequalities,
suggesting that they may not fulfill democratic ideals. Instead, peer
production projects may conform to Robert Michels' ""iron law of oligarchy,""
which proposes that democratic membership organizations become increasingly
oligarchic as they grow. Using exhaustive data of internal processes from a
sample of 683 wikis, we construct empirical measures of participation and test
for increases in oligarchy associated with growth. In contrast to previous
studies, we find support for Michels' iron law and conclude that peer
production entails oligarchic organizational forms.",http://arxiv.org/abs/1407.0323v1
"The 5th annual international conference on Collaborative Innovation Networks
Conference (COINS) takes place at Keio University from March 12 to 14, 2015.
COINS15 brings together practitioners, researchers and students of the emerging
science of collaboration to share their work, learn from each other, and get
inspired through creative new ideas.
  Where science, design, business and art meet, COINS15 looks at the emerging
forces behind the phenomena of open-source, creative, entrepreneurial and
social movements. Through interactive workshops, professional presentations,
and fascinating keynotes, COINS15 combines a wide range of interdisciplinary
fields such as social network analysis, group dynamics, design and
visualization, information systems, collective action and the psychology and
sociality of collaboration.",http://arxiv.org/abs/1502.01142v1
"Social media has emerged to be a popular platform for people to express their
viewpoints on political protests like the Arab Spring. Millions of people use
social media to communicate and mobilize their viewpoints on protests. Hence,
it is a valuable tool for organizing social movements. However, the mechanisms
by which protest affects the population is not known, making it difficult to
estimate the number of protestors. In this paper, we are inspired by
sociological theories of protest participation and propose a framework to
predict from the user's past status messages and interactions whether the next
post of the user will be a declaration of protest. Drawing concepts from these
theories, we model the interplay between the user's status messages and
messages interacting with him over time and predict whether the next post of
the user will be a declaration of protest. We evaluate the framework using data
from the social media platform Twitter on protests during the recent Nigerian
elections and demonstrate that it can effectively predict whether the next post
of a user is a declaration of protest.",http://arxiv.org/abs/1512.02968v1
"Given the centrality of regions in social movements, politics and public
administration we aim to quantitatively study inter- and intra-regional
communication for the first time. This work uses social media posts to first
identify contiguous geographical regions with a shared social identity and then
investigate patterns of communication within and between them. Our case study
uses over 150 days of located Twitter data from England and Wales. In contrast
to other approaches, (e.g. phone call data records or online friendship
networks) we have the message contents as well as the social connection. This
allows us to investigate not only the volume of communication but also the
sentiment and vocabulary. We find that the South-East and North-West regions
are the most talked about; regions tend to be more positive about themselves
than about others; people talk politics much more between regions than within.
This methodology gives researchers a powerful tool to study identity and
interaction within and between social-geographic regions.",http://arxiv.org/abs/1807.04107v1
"Advancements in information and communication technologies have enhanced our
possibilities to communicate worldwide, eliminating borders and making it
possible to interact with people coming from other cultures like never happened
before. Such powerful tools have brought us to reconsider our concept of
privacy and social involvement in order to make them fit into this wider
environment. It is possible to claim that the ICT revolution is changing our
world and is having a core role as a mediating factor for social movements
(e.g., Arab spring) and political decisions (e.g., Brexit), shaping the world
in a faster and shared brand new way. It is then interesting to explore how the
perception of this brand new environment (in terms of social engagement,
privacy perception and sense of belonging to a community) differs even in
similar cultures separated by recent historical reasons, as for example in
Italian and Turkish cultures.",http://arxiv.org/abs/1607.06721v1
"In this paper we examine the most influential resources shared on Twitter
about the #MeToo movement. We also examine whether a small proportion of domain
names and URLs (e.g. 20%) appear in a large number of tweets (e.g. 80%) that
contain #MeToo (known as the 80/20 rule or Pareto principle). R and Python were
used to analyze the data. Results demonstrated that the most frequently shared
domains were twitter.com (47.20%), nytimes.com (4.42%) and youtube.com (3.69%).
The most frequently shared content was a recent poll which indicated ""men are
afraid to mentor women after the #MeToo movement"". In accordance with the
Pareto principle, 8% of domain names accounted for 80% of the shared content on
Twitter that contained #MeToo. This study provides a base for researchers who
are interested in understanding what online resources people rely on when
sharing information about online social movements (e.g. #MeToo).",http://arxiv.org/abs/1906.12321v2
"In this paper, we present a dataset containing 9,973 tweets related to the
MeToo movement that were manually annotated for five different linguistic
aspects: relevance, stance, hate speech, sarcasm, and dialogue acts. We present
a detailed account of the data collection and annotation processes. The
annotations have a very high inter-annotator agreement (0.79 to 0.93 k-alpha)
due to the domain expertise of the annotators and clear annotation
instructions. We analyze the data in terms of geographical distribution, label
correlations, and keywords. Lastly, we present some potential use cases of this
dataset. We expect this dataset would be of great interest to psycholinguists,
socio-linguists, and computational linguists to study the discursive space of
digitally mobilized social movements on sensitive issues like sexual
harassment.",http://arxiv.org/abs/1912.06927v2
"This paper investigates the contribution of business model innovations in
improvement of food supply chains. Through a systematic literature review, the
notable business model innovations in the food industry are identified,
surveyed, and evaluated. Findings reveal that the innovations in value
proposition, value creation processes, and value delivery processes of business
models are the successful strategies proposed in food industry. It is further
disclosed that rural female entrepreneurs, social movements, and also urban
conditions are the most important driving forces inducing the farmers to
reconsider their business models. In addition, the new technologies and
environmental factors are the secondary contributors in business model
innovation for the food processors. It is concluded that digitalization has
disruptively changed the food distributors models. E-commerce models and
internet of things are reported as the essential factors imposing the retailers
to innovate their business models. Furthermore, the consumption demand and the
product quality are two main factors affecting the business models of all the
firms operating in the food supply chain regardless of their positions in the
chain. The findings of the current study provide an insight into the food
industry to design a sustainable business model to bridge the gap between food
supply and food demand.",http://arxiv.org/abs/2001.03982v1
"Extracting moral sentiment from text is a vital component in understanding
public opinion, social movements, and policy decisions. The Moral Foundation
Theory identifies five moral foundations, each associated with a positive and
negative polarity. However, moral sentiment is often motivated by its targets,
which can correspond to individuals or collective entities. In this paper, we
introduce morality frames, a representation framework for organizing moral
attitudes directed at different entities, and come up with a novel and
high-quality annotated dataset of tweets written by US politicians. Then, we
propose a relational learning model to predict moral attitudes towards entities
and moral foundations jointly. We do qualitative and quantitative evaluations,
showing that moral sentiment towards entities differs highly across political
ideologies.",http://arxiv.org/abs/2109.04535v1
"This paper investigates how hate speech varies in systematic ways according
to the identities it targets. Across multiple hate speech datasets annotated
for targeted identities, we find that classifiers trained on hate speech
targeting specific identity groups struggle to generalize to other targeted
identities. This provides empirical evidence for differences in hate speech by
target identity; we then investigate which patterns structure this variation.
We find that the targeted demographic category (e.g. gender/sexuality or
race/ethnicity) appears to have a greater effect on the language of hate speech
than does the relative social power of the targeted identity group. We also
find that words associated with hate speech targeting specific identities often
relate to stereotypes, histories of oppression, current social movements, and
other social contexts specific to identities. These experiments suggest the
importance of considering targeted identity, as well as the social contexts
associated with these identities, in automated hate speech classification.",http://arxiv.org/abs/2210.10839v2
"In this paper, we use mixed methods to study a controversial Internet site:
The Kotaku in Action (KiA) subreddit. Members of KiA are part of GamerGate, a
distributed social movement. We present an emic account of what takes place on
KiA who are they, what are their goals and beliefs, and what rules do they
follow. Members of GamerGate in general and KiA in particular have often been
accused of harassment. However, KiA site policies explicitly prohibit such
behavior, and members insist that they have been falsely accused. Underlying
the controversy over whether KiA supports harassment is a complex disagreement
about what ""harassment"" is, and where to draw the line between freedom of
expression and censorship. We propose a model that characterizes perceptions of
controversial speech, dividing it into four categories: criticism, insult,
public shaming, and harassment. We also discuss design solutions that address
the challenges of moderating harassment without impinging on free speech, and
communicating across different ideologies.",http://arxiv.org/abs/1712.05851v2
"Users on Twitter are identified with the help of their profile attributes
that consists of username, display name, profile image, to name a few. The
profile attributes that users adopt can reflect their interests, belief, or
thematic inclinations. Literature has proposed the implications and
significance of profile attribute change for a random population of users.
However, the use of profile attribute for endorsements and to start a movement
have been under-explored. In this work, we consider #LokSabhaElections2019 as a
movement and perform a large-scale study of the profile of users who actively
made changes to profile attributes centered around #LokSabhaElections2019. We
collect the profile metadata for 49.4M users for a period of 2 months from
April 5, 2019 to June 5, 2019 amid #LokSabhaElections2019. We investigate how
the profile changes vary for the influential leaders and their followers over
the social movement. We further differentiate the organic and inorganic ways to
show the political inclination from the prism of profile changes. We report how
the addition of election campaign related keywords lead to spread of behavior
contagion and further investigate it with respect to ""Chowkidar Movement"" in
detail.",http://arxiv.org/abs/1909.10012v1
"3D human dance motion is a cooperative and elegant social movement. Unlike
regular simple locomotion, it is challenging to synthesize artistic dance
motions due to the irregularity, kinematic complexity and diversity. It
requires the synthesized dance is realistic, diverse and controllable. In this
paper, we propose a novel generative motion model based on temporal convolution
and LSTM,TC-LSTM, to synthesize realistic and diverse dance motion. We
introduce a unique control signal, dance melody line, to heighten
controllability. Hence, our model, and its switch for control signals, promote
a variety of applications: random dance synthesis, music-to-dance, user
control, and more. Our experiments demonstrate that our model can synthesize
artistic dance motion in various dance types. Compared with existing methods,
our method achieved start-of-the-art results.",http://arxiv.org/abs/2006.05743v1
"Open source development, to a great extent, is a type of social movement in
which shared ideologies play critical roles. For participants of open source
development, ideology determines how they make sense of things, shapes their
thoughts, actions, and interactions, enables rich social dynamics in their
projects and communities, and hereby realizes profound impacts at both
individual and organizational levels. While software engineering researchers
have been increasingly recognizing ideology's importance in open source
development, the notion of ""ideology"" has shown significant ambiguity and
vagueness, and resulted in theoretical and empirical confusion. In this
article, we first examine the historical development of ideology's
conceptualization, and its theories in multiple disciplines. Then, we review
the extant software engineering literature related to ideology. We further
argue the imperatives of developing an empirical theory of ideology in open
source development, and propose a research agenda for developing such a theory.
How such a theory could be applied is also discussed.",http://arxiv.org/abs/2104.12732v1
"Who actually expresses an intent to buy GameStop shares on Reddit? What
convinces people to buy stocks? Are people convinced to support a coordinated
plan to adversely impact Wall Street investors? Existing literature on
understanding intent has mainly relied on surveys and self reporting; however
there are limitations to these methodologies. Hence, in this paper, we develop
an annotated dataset of communications centered on the GameStop phenomenon to
analyze the subscriber intentions behaviors within the r/WallStreetBets
community to buy (or not buy) stocks. Likewise, we curate a dataset to better
understand how intent interacts with a user's general support towards the
coordinated actions of the community for GameStop. Overall, our dataset can
provide insight to social scientists on the persuasive power to buy into social
movements online by adopting common language and narrative. WARNING: This paper
contains offensive language that commonly appears on Reddit's r/WallStreetBets
subreddit.",http://arxiv.org/abs/2203.08694v1
"The COVID-19 pandemic has intensified numerous social issues that warrant
academic investigation. Although information dissemination has been extensively
studied, the silenced voices and censored content also merit attention due to
their role in mobilizing social movements. In this paper, we provide empirical
evidence to explore the relationships among COVID-19 regulations, censorship,
and protest through a series of social incidents occurred in China during 2022.
We analyze the similarities and differences between censored articles and
discussions on r/china\_irl, the most popular Chinese-speaking subreddit, and
scrutinize the temporal dynamics of government censorship activities and their
impact on user engagement within the subreddit. Furthermore, we examine users'
linguistic patterns under the influence of a censorship-driven environment. Our
findings reveal patterns in topic recurrence, the complex interplay between
censorship activities, user subscription, and collective commenting behavior,
as well as potential linguistic adaptation strategies to circumvent censorship.
These insights hold significant implications for researchers interested in
understanding the survival mechanisms of marginalized groups within censored
information ecosystems.",http://arxiv.org/abs/2304.02800v1
"During the 1950s Scandinavian Design caught international attention with its
minimalism, simplicity, functionalism and sophistication. Several factors
rested at its heart: functionality, democracy and affordability. Aesthetic
styles connected to international minimalist, modernist and functionalist
movements, which were also symbolically connected to education, political and
social movements, and the Nordic welfare model. Studies have shown how social
and political values from this period connect with Nordic interaction design
from the past three decades. How these are represented in contemporary
interaction design discourse, and design form and expression is a perspective
under represented. This paper presents the results of a three-tiered content
analysis of the proceedings of NordiCHI years 2000-2014: categorization of
titles according to emphasis; content analysis of Scandinavian value constructs
overall; and thematic connection of the results to conference theme and site.
Results are then discussed with reflection on form and process in the Nordic
interaction design industry.",http://arxiv.org/abs/2304.06820v1
"Climate change is the defining issue of our time, and we are at a defining
moment. Various interest groups, social movement organizations, and individuals
engage in collective action on this issue on social media. In addition, issue
advocacy campaigns on social media often arise in response to ongoing societal
concerns, especially those faced by energy industries. Our goal in this paper
is to analyze how those industries, their advocacy group, and climate advocacy
group use social media to influence the narrative on climate change. In this
work, we propose a minimally supervised model soup [57] approach combined with
messaging themes to identify the stances of climate ads on Facebook. Finally,
we release our stance dataset, model, and set of themes related to climate
campaigns for future work on opinion mining and the automatic detection of
climate change stances.",http://arxiv.org/abs/2305.06174v2
"The formation mechanisms and cyclical conditions of collective action have
become open issues in research involving public choice, social movements, and
more. For this reason, on the basis of rational decision-making and social
assimilation, this paper proposes an action model that combines Bayesian game
and social network dynamics, and incorporates exogenous cycles into it. For
this model, this paper proves the spontaneous action theorem and action cycle
theorem of collective action, and based on numerical simulation and empirical
calibration, further confirms the theoretical mechanism involving elements such
as risk/risk-free incentives and the number of social ties. Based on such
conclusions and evidence, this paper proposes a theory of spontaneous cycles as
an integrative answer to the open question of collective action
formation/cycles.",http://arxiv.org/abs/2308.01791v1
"Social media has become a major driver of social change, by facilitating the
formation of online social movements. Automatically understanding the
perspectives driving the movement and the voices opposing it, is a challenging
task as annotated data is difficult to obtain. We propose a weakly supervised
graph-based approach that explicitly models perspectives in
#BackLivesMatter-related tweets. Our proposed approach utilizes a
social-linguistic representation of the data. We convert the text to a graph by
breaking it into structured elements and connect it with the social network of
authors, then structured prediction is done over the elements for identifying
perspectives. Our approach uses a small seed set of labeled examples. We
experiment with large language models for generating artificial training
examples, compare them to manual annotation, and find that it achieves
comparable performance. We perform quantitative and qualitative analyses using
a human-annotated test set. Our model outperforms multitask baselines by a
large margin, successfully characterizing the perspectives supporting and
opposing #BLM.",http://arxiv.org/abs/2310.07155v2
"Online memes have emerged as powerful digital cultural artifacts in the age
of social media, offering not only humor but also platforms for political
discourse, social critique, and information dissemination. Their extensive
reach and influence in shaping online communities' sentiments make them
invaluable tools for campaigning and promoting ideologies. Despite the
development of several meme-generation tools, there remains a gap in their
systematic evaluation and their ability to effectively communicate ideologies.
Addressing this, we introduce MemeCraft, an innovative meme generator that
leverages large language models (LLMs) and visual language models (VLMs) to
produce memes advocating specific social movements. MemeCraft presents an
end-to-end pipeline, transforming user prompts into compelling multimodal memes
without manual intervention. Conscious of the misuse potential in creating
divisive content, an intrinsic safety mechanism is embedded to curb hateful
meme production.",http://arxiv.org/abs/2403.14652v1
"Understanding the mechanisms behind opinion formation is crucial for gaining
insight into the processes that shape political beliefs, cultural attitudes,
consumer choices, and social movements. This work aims to explore a nuanced
model that captures the intricacies of real-world opinion dynamics by
synthesizing principles from cognitive science and employing social network
analysis. The proposed model is a hybrid continuous-discrete extension of the
well-known Naming Game opinion model. The added latent continuous layer of
opinion strength follows cognitive processes in the human brain, akin to memory
imprints. The discrete layer allows for the conversion of intrinsic continuous
opinion into discrete form, which often occurs when we publicly verbalize our
opinions. We evaluated our model using real data as ground truth and
demonstrated that the proposed mechanism outperforms the classic Naming Game
model in many cases, reflecting that our model is closer to the real process of
opinion formation.",http://arxiv.org/abs/2406.19204v1
"Coordination is a fundamental aspect of life. The advent of social media has
made it integral also to online human interactions, such as those that
characterize thriving online communities and social movements. At the same
time, coordination is also core to effective disinformation, manipulation, and
hate campaigns. This survey collects, categorizes, and critically discusses the
body of work produced as a result of the growing interest on coordinated online
behavior. We reconcile industry and academic definitions, propose a
comprehensive framework to study coordinated online behavior, and review and
critically discuss the existing detection and characterization methods. Our
analysis identifies open challenges and promising directions of research,
serving as a guide for scholars, practitioners, and policymakers in
understanding and addressing the complexities inherent to online coordination.",http://arxiv.org/abs/2408.01257v1
"We are living in an age of protest. Although we have an excellent
understanding of the factors that predict participation in protest, we
understand little about the conditions that foster a sustained (versus
transient) movement. How do interactions between supporters and authorities
combine to influence whether and how people engage (i.e., using conventional or
radical tactics)? This paper introduces a novel, theoretically-founded and
empirically-informed agent-based model (DIMESim) to address these questions. We
model the complex interactions between the psychological attributes of the
protester (agents), the authority to whom the protests are targeted, and the
environment that allows protesters to coordinate with each other -- over time,
and at a population scale. Where an authority is responsive and failure is
contested, a modest sized conventional movement endured. Where authorities
repeatedly and incontrovertibly fail the movement, the population disengaged
from action but evidenced an ongoing commitment to radicalism (latent
radicalism).",http://arxiv.org/abs/2408.12795v1
"The constant shifts in social and political contexts, driven by emerging
social movements and political events, lead to new forms of hate content and
previously unrecognized hate patterns that machine learning models may not have
captured. Some recent literature proposes the data augmentation-based
techniques to enrich existing hate datasets by incorporating samples that
reveal new implicit hate patterns. This approach aims to improve the model's
performance on out-of-domain implicit hate instances. It is observed, that
further addition of more samples for augmentation results in the decrease of
the performance of the model. In this work, we propose a Knowledge
Transfer-driven Concept Refinement method that distills and refines the
concepts related to implicit hate samples through novel prototype alignment and
concept losses, alongside data augmentation based on concept activation
vectors. Experiments with several publicly available datasets show that
incorporating additional implicit samples reflecting new hate patterns through
concept refinement enhances the model's performance, surpassing baseline
results while maintaining cross-dataset generalization
capabilities.\footnote{DISCLAIMER: This paper contains explicit statements that
are potentially offensive.}",http://arxiv.org/abs/2410.15314v1
"The current COVID-19 pandemic has proven that proper control and prevention
of infectious disease require creating and enforcing the appropriate public
policies. One critical policy imposed by the policymakers is encouraging the
population to practice social distancing (i.e. controlling the contact rate
among the population). Here we pose a mean-field game model of individuals each
choosing a dynamic strategy of making contacts, given the trade-off of gaining
utility but also risking infection from additional contacts. We compute and
compare the mean-field equilibrium (MFE) strategy, which assumes each
individual acting selfishly to maximize its own utility, to the socially
optimal strategy, which maximizes the total utility of the population. We prove
that the optimal decision of the infected is always to make more contacts than
the level at which it would be socially optimal, which reinforces the important
role of public policy to reduce contacts of the infected (e.g. quarantining,
sick paid leave). Additionally, we include cost to incentivize people to change
strategies, when computing the socially optimal strategies. We find that with
this cost, policies reducing contacts of the infected should be further
enforced after the peak of the epidemic has passed. Lastly, we compute the
price of anarchy (PoA) of this system, to understand the conditions under which
large discrepancies between the MFE and socially optimal strategies arise,
which is when intervening public policy would be most effective.",http://arxiv.org/abs/2005.06758v2
"Artificial Intelligence (AI) has an increasing impact on all areas of
people's livelihoods. A detailed look at existing interdisciplinary and
transdisciplinary metrics frameworks could bring new insights and enable
practitioners to navigate the challenge of understanding and assessing the
impact of Autonomous and Intelligent Systems (A/IS). There has been emerging
consensus on fundamental ethical and rights-based AI principles proposed by
scholars, governments, civil rights organizations, and technology companies. In
order to move from principles to real-world implementation, we adopt a lens
motivated by regulatory impact assessments and the well-being movement in
public policy. Similar to public policy interventions, outcomes of AI systems
implementation may have far-reaching complex impacts. In public policy,
indicators are only part of a broader toolbox, as metrics inherently lead to
gaming and dissolution of incentives and objectives. Similarly, in the case of
A/IS, there's a need for a larger toolbox that allows for the iterative
assessment of identified impacts, inclusion of new impacts in the analysis, and
identification of emerging trade-offs. In this paper, we propose the practical
application of an enhanced well-being impact assessment framework for A/IS that
could be employed to address ethical and rights-based normative principles in
AI. This process could enable a human-centered algorithmically-supported
approach to the understanding of the impacts of AI systems. Finally, we propose
a new testing infrastructure which would allow for governments, civil rights
organizations, and others, to engage in cooperating with A/IS developers
towards implementation of enhanced well-being impact assessments.",http://arxiv.org/abs/2007.14826v2
"Network data is increasingly being used in quantitative, data-driven public
policy research. These are typically very rich datasets that contain complex
correlations and inter-dependencies. This richness both promises to be quite
useful for policy research, while at the same time posing a challenge for the
useful extraction of information from these datasets - a challenge which calls
for new data analysis methods. In this report, we formulate a research agenda
of key methodological problems whose solutions would enable new advances across
many areas of policy research. We then review recent advances in applying deep
learning to network data, and show how these methods may be used to address
many of the methodological problems we identified. We particularly emphasize
deep generative methods, which can be used to generate realistic synthetic
networks useful for microsimulation and agent-based models capable of informing
key public policy questions. We extend these recent advances by developing a
new generative framework which applies to large social contact networks
commonly used in epidemiological modeling. For context, we also compare and
contrast these recent neural network-based approaches with the more traditional
Exponential Random Graph Models. Lastly, we discuss some open problems where
more progress is needed.",http://arxiv.org/abs/2010.07870v2
"Governments are increasingly turning to algorithmic risk assessments when
making important decisions, such as whether to release criminal defendants
before trial. Policymakers assert that providing public servants with
algorithmic advice will improve human risk predictions and thereby lead to
better (e.g., fairer) decisions. Yet because many policy decisions require
balancing risk-reduction with competing goals, improving the accuracy of
predictions may not necessarily improve the quality of decisions. If risk
assessments make people more attentive to reducing risk at the expense of other
values, these algorithms would diminish the implementation of public policy
even as they lead to more accurate predictions. Through an experiment with
2,140 lay participants simulating two high-stakes government contexts, we
provide the first direct evidence that risk assessments can systematically
alter how people factor risk into their decisions. These shifts counteracted
the potential benefits of improved prediction accuracy. In the pretrial setting
of our experiment, the risk assessment made participants more sensitive to
increases in perceived risk; this shift increased the racial disparity in
pretrial detention by 1.9%. In the government loans setting of our experiment,
the risk assessment made participants more risk-averse; this shift reduced
government aid by 8.3%. These results demonstrate the potential limits and
harms of attempts to improve public policy by incorporating predictive
algorithms into multifaceted policy decisions. If these observed behaviors
occur in practice, presenting risk assessments to public servants would
generate unexpected and unjust shifts in public policy without being subject to
democratic deliberation or oversight.",http://arxiv.org/abs/2012.05370v2
"Policymakers decide on alternative policies facing restricted budgets and
uncertain, ever-changing future. Designing public policies is further difficult
due to the need to decide on priorities and handle effects across policies.
Housing policies, specifically, involve heterogeneous characteristics of
properties themselves and the intricacy of housing markets and the spatial
context of cities. We propose PolicySpace2 (PS2) as an adapted and extended
version of the open source PolicySpace agent-based model. PS2 is a computer
simulation that relies on empirically detailed spatial data to model real
estate, along with labor, credit, and goods and services markets. Interaction
among workers, firms, a bank, households and municipalities follow the
literature benchmarks to integrate economic, spatial and transport scholarship.
PS2 is applied to a comparison among three competing public policies aimed at
reducing inequality and alleviating poverty: (a) house acquisition by the
government and distribution to lower income households, (b) rental vouchers,
and (c) monetary aid. Within the model context, the monetary aid, that is,
smaller amounts of help for a larger number of households, makes the economy
perform better in terms of production, consumption, reduction of inequality,
and maintenance of financial duties. PS2 as such is also a framework that may
be further adapted to a number of related research questions.",http://arxiv.org/abs/2102.11929v4
"The public policy cycle requires increasingly the use of evidence by policy
makers. Evidence Gap Maps (EGMs) are a relatively new methodology that helps
identify, process, and visualize the vast amounts of studies representing a
rich source of evidence for better policy making. This document performs a
methodological review of EGMs and presents the development of a working
integrated system that automates several critical steps of EGM creation by
means of applied computational and statistical methods. Above all, the proposed
system encompasses all major steps of EGM creation in one place, namely
inclusion criteria determination, processing of information, analysis, and
user-friendly communication of synthesized relevant evidence. This tool
represents a critical milestone in the efforts of implementing cutting-edge
computational methods in usable systems. The contribution of the document is
two-fold. First, it presents the critical importance of EGMs in the public
policy cycle; second, it justifies and explains the development of a usable
tool that encompasses the methodological phases of creation of EGMs, while
automating most time-consuming stages of the process. The overarching goal is
the better and faster information communication to relevant actors like policy
makers, thus promoting well-being through better and more efficient
interventions based on more evidence-driven policy making.",http://arxiv.org/abs/2304.10576v1
"Accessing data collected by federal statistical agencies is essential for
public policy research and improving evidence-based decision making, such as
evaluating the effectiveness of social programs, understanding demographic
shifts, or addressing public health challenges. Differentially private
interactive systems, or validation servers, can form a crucial part of the
data-sharing infrastructure. They may allow researchers to query targeted
statistics, providing flexible, efficient access to specific insights, reducing
the need for broad data releases and supporting timely, focused research.
However, they have not yet been practically implemented. While substantial
theoretical work has been conducted on the privacy and accuracy guarantees of
differentially private mechanisms, prior efforts have not considered usability
as an explicit goal of interactive systems. This work outlines and considers
the barriers to developing differentially private interactive systems for
informing public policy and offers an alternative way forward. We propose
balancing three design considerations: privacy assurance, statistical utility,
and system usability, we develop recommendations for making differentially
private interactive systems work in practice, we present an example
architecture based on these recommendations, and we provide an outline of how
to conduct the necessary user-testing. Our work seeks to move the practical
development of differentially private interactive systems forward to better aid
public policy making and spark future research.",http://arxiv.org/abs/2412.11794v1
"This paper has been withdrawn by the authors in order to comply with the
ATLAS publication policy and is now only available via the CERN cdsweb
(http://cdsweb.cern.ch/record/984015)",http://arxiv.org/abs/0704.2307v3
"The paper proposes a new model of spin dynamics which can be treated as a
model of sociological coupling between individuals. Our approach takes into
account two different human features: gregariousness and individuality. We will
show how they affect a psychological distance between individuals and how the
distance changes the opinion formation in a social group. Apart from its
sociological aplications the model displays the variety of other interesting
phenomena like self-organizing ferromagnetic state or a second order phase
transition and can be studied from different points of view, e.g. as a model of
ferromagnetic fluid, complex evolving network or multiplicative random process.",http://arxiv.org/abs/physics/0603248v1
"We explore the promises and challenges of employing sequential
decision-making algorithms -- such as bandits, reinforcement learning, and
active learning -- in law and public policy. While such algorithms have
well-characterized performance in the private sector (e.g., online
advertising), the tendency to naively apply algorithms motivated by one domain,
often online advertisements, can be called the ""advertisement fallacy."" Our
main thesis is that law and public policy pose distinct methodological
challenges that the machine learning community has not yet addressed. Machine
learning will need to address these methodological problems to move ""beyond
ads."" Public law, for instance, can pose multiple objectives, necessitate
batched and delayed feedback, and require systems to learn rational, causal
decision-making policies, each of which presents novel questions at the
research frontier. We discuss a wide range of potential applications of
sequential decision-making algorithms in regulation and governance, including
public health, environmental protection, tax administration, occupational
safety, and benefits adjudication. We use these examples to highlight research
needed to render sequential decision making policy-compliant, adaptable, and
effective in the public sector. We also note the potential risks of such
deployments and describe how sequential decision systems can also facilitate
the discovery of harms. We hope our work inspires more investigation of
sequential decision making in law and public policy, which provide unique
challenges for machine learning researchers with potential for significant
social benefit.",http://arxiv.org/abs/2112.06833v3
"This collection of Big Bayes Stories could be partitioned into two groups,
one relating to the sciences, cosmology in particular, and the other relating
to public policy, that is, health, fisheries management and demographics.",http://arxiv.org/abs/1405.4982v1
"Fienberg convincingly demonstrates that Bayesian models and methods represent
a powerful approach to squeezing illumination from data in public policy
settings. However, no school of inference is without its weaknesses, and, in
the face of the ambiguities, uncertainties, and poorly posed questions of the
real world, perhaps we should not expect to find a formally correct inferential
strategy which can be universally applied, whatever the nature of the question:
we should not expect to be able to identify a ""norm"" approach. An analogy is
made between George Box's ""no models are right, but some are useful,"" and
inferential systems [arXiv:1108.2177].",http://arxiv.org/abs/1108.3657v1
"Social Security and other public policies can be viewed as a series of cash
in and outflows that depend on parameters such as the age distribution of the
population and the retirement age. Given forecasts of these parameters,
policies can be designed to be financially stable, i.e., to terminate with a
zero balance. If reality deviates from the forecasts, policies normally
terminate with a surplus or a deficit. We derive constraints on the cash flows
of robust policies that terminate with zero balance even in the presence of
forecasting errors. Social Security and most similar policies are not robust.
We show that non-trivial robust policies exist and provide a recipe for
constructing robust extensions of non-robust policies. An example illustrates
our results.",http://arxiv.org/abs/1201.6340v1
"Increasing integration and availability of data on large groups of persons
has been accompanied by proliferation of statistical and other algorithmic
prediction tools in banking, insurance, marketiNg, medicine, and other FIelds
(see e.g., Steyerberg (2009a;b)). Controversy may ensue when such tools are
introduced to fields traditionally reliant on individual clinical evaluations.
Such controversy has arisen about ""actuarial"" assessments of violence
recidivism risk, i.e., the probability that someone found to have committed a
violent act will commit another during a specified period. Recently Hart et al.
(2007a) and subsequent papers from these authors in several reputable journals
have claimed to demonstrate that statistical assessments of such risks are
inherently too imprecise to be useful, using arguments that would seem to apply
to statistical risk prediction quite broadly. This commentary examines these
arguments from a technical statistical perspective, and finds them seriously
mistaken in many particulars. They should play no role in reasoned discussions
of violence recidivism risk assessment.",http://arxiv.org/abs/1503.03666v1
"In this article, we consider modeling ranked responses from a heterogeneous
population. Specifically, we analyze data from the Eurobarometer 34.1 survey
regarding public policy preferences towards drugs, alcohol and AIDS. Such
policy preferences are likely to exhibit substantial differences within as well
as across European nations reflecting a wide variety of cultures, political
affiliations, ideological perspectives and common practices. We use a mixed
membership model to account for multiple subgroups with differing preferences
and to allow each individual to possess partial membership in more than one
subgroup. Previous methods for fitting mixed membership models to rank data in
a univariate setting have utilized an MCMC approach and do not estimate the
relative frequency of each subgroup. We propose a variational EM approach for
fitting mixed membership models with multivariate rank data. Our method allows
for fast approximate inference and explicitly estimates the subgroup sizes.
Analyzing the Eurobarometer 34.1 data, we find interpretable subgroups which
generally agree with the ""left vs right"" classification of political
ideologies.",http://arxiv.org/abs/1512.08731v3
"Throughout the Covid-19 pandemic, a significant amount of effort had been put
into developing techniques that predict the number of infections under various
assumptions about the public policy and non-pharmaceutical interventions. While
both the available data and the sophistication of the AI models and available
computing power exceed what was available in previous years, the overall
success of prediction approaches was very limited. In this paper, we start from
prediction algorithms proposed for XPrize Pandemic Response Challenge and
consider several directions that might allow their improvement. Then, we
investigate their performance over medium-term predictions extending over
several months. We find that augmenting the algorithms with additional
information about the culture of the modeled region, incorporating traditional
compartmental models and up-to-date deep learning architectures can improve the
performance for short term predictions, the accuracy of medium-term predictions
is still very low and a significant amount of future research is needed to make
such models a reliable component of a public policy toolbox.",http://arxiv.org/abs/2112.11187v1
"The availability of Open Government Data (OGD) provides means for citizens to
understand and follow governmental policies and decisions, showing evidence of
how the latter have contributed to both the place they live in and their lives.
In such a scenario, one of the proposals is the use of visualizations to
support the process of data analysis and interpretation. Herein, we present the
use of three different visualization tools, a commercial one and two academic
ones, applied to two specific Brazilian cases: the implementation of the Drink
Driving Law and the construction of a new overpass in an important city avenue.
Our focus was on the analysis of how visualization could help in the
identification of the effects of such traffic public policies. As our main
contributions, we present details on the effects of the observed policies, as
well as new cases showing how visualization tools can assist users to interpret
OGD.",http://arxiv.org/abs/2102.07621v1
"How should we think of the preferences of citizens? Whereas self-optimal
policy is relatively straightforward to produce, socially optimal policy often
requires a more detailed examination. In this paper, we identify an issue that
has received far too little attention in welfarist modelling of public policy,
which we name the ""hidden assumptions"" problem. Hidden assumptions can be
deceptive because they are not expressed explicitly and the social planner
(e.g. a policy maker, a regulator, a legislator) may not give them the critical
attention they need. We argue that ethical expertise has a direct role to play
in public discourse because it is hard to adopt a position on major issues like
public health policy or healthcare prioritisation without making contentious
assumptions about population ethics. We then postulate that ethicists are best
situated to critically evaluate these hidden assumptions, and can therefore
play a vital role in public policy debates.",http://arxiv.org/abs/2205.01957v1
"This document is the Snowmass summary report for the Public Policy &
Government Engagement topical group within the Community Engagement Frontier.
This report discusses how the U.S. High Energy Physics (HEP) community
currently engages with government at all levels and provides recommendations
for how the execution of these actives can be improved and for how the scope of
existing activities can be expanded. This includes the current HEP
Congressional government engagement ""DC trip"", materials produced for
communication with government officials, government engagement in areas other
than HEP funding, and interactions with the funding agencies, Executive Office
of the President, and state and local governments.",http://arxiv.org/abs/2209.09067v2
"Mainstream food delivery platforms, like DoorDash and Uber Eats, have been
the locus of fierce policy debates about their unfair business and labor
practices. At the same time, hundreds of independent food delivery services
provide alternative opportunities to many communities across the U.S. We
surveyed operators of independent food delivery platforms to learn about their
perception of the role of public policy. We found conflicting opinions on
whether and how policy should interact with their businesses, ranging from not
wanting policymakers to interfere to articulating specific policies that would
curtail mainstream platforms' business practices. We provide insights for
technologists and policymakers interested in the sociotechnical challenges of
local marketplaces.",http://arxiv.org/abs/2303.15415v2
"While we typically focus on data visualization as a tool for facilitating
cognitive tasks (e.g., learning facts, making decisions), we know relatively
little about their second-order impacts on our opinions, attitudes, and values.
For example, could design or framing choices interact with viewers' social
cognitive biases in ways that promote political polarization? When reporting on
U.S. attitudes toward public policies, it is popular to highlight the gap
between Democrats and Republicans (e.g., with blue vs red connected dot plots).
But these charts may encourage social-normative conformity, influencing
viewers' attitudes to match the divided opinions shown in the visualization. We
conducted three experiments examining visualization framing in the context of
social conformity and polarization. Crowdworkers viewed charts showing
simulated polling results for public policy proposals. We varied framing
(aggregating data as non-partisan ""All US Adults,"" or partisan ""Democrat"" and
""Republican"") and the visualized groups' support levels. Participants then
reported their own support for each policy. We found that participants'
attitudes biased significantly toward the group attitudes shown in the stimuli
and this can increase inter-party attitude divergence. These results
demonstrate that data visualizations can induce social conformity and
accelerate political polarization. Choosing to visualize partisan divisions can
divide us further.",http://arxiv.org/abs/2309.00690v1
"In order to effectively analyze information regarding ongoing events that
impact local communities across language and country borders, researchers often
need to perform multilingual data analysis. This analysis can be particularly
challenging due to the rapidly evolving event-centric data and the language
barrier. In this abstract we present preliminary results of a case study with
the goal to better understand how researchers interact with multilingual
event-centric information in the context of cross-cultural studies and which
methods and features they use.",http://arxiv.org/abs/1801.07011v1
"From the perspective of cultural studies and critical theory, Luhmann's
communication-theoretical approach in sociology can still be read as a
meta-biology: while biologists take the development of life as a given, Luhmann
tends to treat the development of meaning as a cultural given. Meaning is no
longer considered as constructed in communication, but meaning processing
precedes and controls communication as an independent variable. Habermas
appreciates Luhmann's distinction between psychic and social systems, but he
challenges us to bring the critique of metaphysical issues (of providing
meaning to events in a dialectics) back into this metabiological perspective
that processes meaning without intentionality, that is, as a scientistic
objectivation.",http://arxiv.org/abs/0911.2718v1
"The paper discusses the potential of large vision-language models as objects
of interest for empirical cultural studies. Focusing on the comparative
analysis of outputs from two popular text-to-image synthesis models, DALL-E 2
and Stable Diffusion, the paper tries to tackle the pros and cons of striving
towards culturally agnostic vs. culturally specific AI models. The paper
discusses several examples of memorization and bias in generated outputs which
showcase the trade-off between risk mitigation and cultural specificity, as
well as the overall impossibility of developing culturally agnostic models.",http://arxiv.org/abs/2211.15271v2
"According to physicist David Mermin, the science wars was a series of
exchanges between scientists and ""sociologists, historians, and literary
critics"" whom the scientists thought to be ""ludicrously ignorant of science,
making all kinds of nonsensical pronouncements."" The science wars peaked in
1996 with the publication of a hoax article by physicist Alan Sokal in a
cultural studies journal and then seemed mercifully to die away by the end of
the 1990's. I recently noticed, however, that the kerfuffle has persisted. This
motivated me to pen this essay and point out the silliness of the entire
affair.",http://arxiv.org/abs/2303.11980v1
"Inequalities are abundant in a society with a number of agents competing for
a limited amount of resource. Statistics of such social inequalities are
usually represented by the Lorenz function $L(p)$, where $p$ fraction of the
population possesses $L(p)$ fraction of the total wealth, when the population
is arranged in the ascending order of their wealth. Similarly, in
scientometrics, such inequalities can be represented by a plot of the citation
count against the respective number of papers by a scientist, again arranged in
the ascending order of their citation counts. Quantitatively, these
inequalities are captured by the corresponding inequality indices, namely the
Kolkata $k$ and the Hirsch $h$ indices, given by the fixed points of these
nonlinear functions. In statistical physics of criticality, the fixed points of
the Renormalization Group generator functions are studied in their self-similar
limit, where its (fractal) structure converges to a unique form. The
statistical indices in the social science, however, correspond to the fixed
points where the values of the generator function (wealth or citation sizes)
are commensurately abundant in fractions or numbers (of persons or papers). We
introduce and study these indices for the inequalities of (pre-failure)
avalanches, given by their size distributions in the Fiber Bundle Models (FBM)
of non-brittle materials. We show how a prior knowledge of the terminal and
almost universal value of the $k$ index for a wide range of disorder parameter,
can help in predicting an imminent catastrophic breakdown in the model. This
observation has also been complemented by noting a similar (but not identical)
behavior of the Hirsch index ($h$), redefined for such avalanche statistics.",http://arxiv.org/abs/2106.14294v5
"This paper assesses the equity impacts of for-hire autonomous vehicles (AVs)
and investigates regulatory policies that promote spatial and social equity in
future autonomous mobility ecosystems. To this end, we consider a multimodal
transportation network, where a ride-hailing platform operates a fleet of AVs
to offer mobility-on-demand services in competition with a public transit
agency that offers transit services on a transportation network. A
game-theoretic model is developed to characterize the intimate interactions
between the ride-hailing platform, the transit agency, and multiclass
passengers with distinct income levels. An algorithm is proposed to compute the
Nash equilibrium of the game and conduct an ex-post evaluation of the
performance of the obtained solution. Based on the proposed framework, we
evaluate the spatial and social equity in transport accessibility using the
Theil index, and find that although the proliferation of for-hire AVs in the
ride-hailing network improves overall accessibility, the benefits are not
fairly distributed among distinct locations or population groups, implying that
the deployment of AVs will enlarge the existing spatial and social inequity
gaps in the transportation network if no regulatory intervention is in place.
To address this concern, we investigate two regulatory policies that can
improve transport equity: (a) a minimum service-level requirement on
ride-hailing services, which improves the spatial equity in the transport
network; (b) a subsidy on transit services by taxing ride-hailing services,
which promotes the use of public transit and improves the spatial and social
equity of the transport network. We show that the minimum service-level
requirement entails a trade-off: as a higher minimum service level is imposed,
the spatial inequity reduces, but the social inequity will be exacerbated. On
the other hand ...",http://arxiv.org/abs/2301.05798v2
"Social inequalities are ubiquitous and evolve towards a universal limit.
Herein, we extensively review the values of inequality measures, namely the
Gini ($g$) index and the Kolkata ($k$) index, two standard measures of
inequality used in the analysis of various social sectors through data
analysis. The Kolkata index, denoted as $k$, indicates the proportion of the
`wealth' owned by $(1-k)$ fraction of the `people'. Our findings suggest that
both the Gini index and the Kolkata index tend to converge to similar values
(around $g=k \approx 0.87$, starting from the point of perfect equality, where
$g=0$ and $k=0.5$) as competition increases in different social institutions,
such as markets, movies, elections, universities, prize winning, battle fields,
sports (Olympics), etc., under conditions of unrestricted competition (no
social welfare or support mechanism). In this review, we present the concept of
a generalized form of Pareto's 80/20 law ($k=0.80$), where the coincidence of
inequality indices is observed. The observation of this coincidence is
consistent with the precursor values of the $g$ and $k$ indices for the
self-organized critical (SOC) state in self-tuned physical systems such as sand
piles. These results provide quantitative support for the view that interacting
socioeconomic systems can be understood within the framework of SOC, which has
been hypothesized for many years. These findings suggest that the SOC model can
be extended to capture the dynamics of complex socioeconomic systems and help
us better understand their behavior.",http://arxiv.org/abs/2303.03795v3
"Mathematical modelling has served a central role in studying how infectious
disease transmission manifests at the population level. These models have
demonstrated the importance of population-level factors like social network
heterogeneity on structuring epidemic risk and are now routinely used in public
health for decision support. One barrier to broader utility of mathematical
models is that the existing canon does not readily accommodate the social
determinants of health as distinct, formal drivers of transmission dynamics.
Given the decades of empirical support for the organizational effect of social
determinants on health burden more generally and infectious disease risk more
specially, addressing this modelling gap is of critical importance. In this
study, we build on prior efforts to integrate social forces into mathematical
epidemiology by introducing several new metrics, principally structural causal
influence (SCI). Here, SCI leverages causal analysis to provide a measure of
the relative vulnerability of subgroups within a susceptible population, which
are crafted by differences in healthcare, exposure to disease, and other
determinants. We develop our metrics using a general case and apply it to
specific one of public health importance: Hepatitis C virus in a population of
persons who inject drugs. Our use of the SCI reveals that, under specific
parameters in a multi-community model, the ""less vulnerable"" community may
sustain a basic reproduction number below one when isolated, ensuring disease
extinction. However, even minimal transmission between less and more vulnerable
communities can elevate this number, leading to sustained epidemics within both
communities. Summarizing, we reflect on our findings in light of conversations
surrounding the importance of social inequalities and how their consideration
can influence the study and practice of mathematical epidemiology.",http://arxiv.org/abs/2409.09096v1
"Since culture influences expectations, perceptions, and satisfaction, a
cross-culture study is necessary to understand the differences between Japan's
biggest tourist populations, Chinese and Western tourists. However, with
ever-increasing customer populations, this is hard to accomplish without
extensive customer base studies. There is a need for an automated method for
identifying these expectations at a large scale. For this, we used a
data-driven approach to our analysis. Our study analyzed their satisfaction
factors comparing soft attributes, such as service, with hard attributes, such
as location and facilities, and studied different price ranges. We collected
hotel reviews and extracted keywords to classify the sentiment of sentences
with an SVC. We then used dependency parsing and part-of-speech tagging to
extract nouns tied to positive adjectives. We found that Chinese tourists
consider room quality more than hospitality, whereas Westerners are delighted
more by staff behavior. Furthermore, the lack of a Chinese-friendly environment
for Chinese customers and cigarette smell for Western ones can be disappointing
factors of their stay. As one of the first studies in the tourism field to use
the high-standard Japanese hospitality environment for this analysis, our
cross-cultural study contributes to both the theoretical understanding of
satisfaction and suggests practical applications and strategies for hotel
managers.",http://arxiv.org/abs/2107.14681v1
"Social movements, neurons in the brain or even industrial suppliers are best
described by agents evolving on networks with basic interaction rules. In these
real systems, the connectivity between agents corresponds to the a critical
state of the system related to the noise of the system. The new idea is that
connectivity adjusts itself because of two opposite tendencies: on the one hand
informations percolation is better when the network connectivity is small but
all agents have rapidely the same state and the dynamics stops.
  On the other hand, when agents have a large connectivity, the state of a node
(opinion of a person, state of a neuron, ...) tends to freeze: agents find
always a minority among their neighbours to support their state. The model
introduced here captures this essential feature showing a clear transition
between the two tendencies at some critical connectivity. Depending on the
noise, the dynamics of the system can only take place at a precise critical
connectivity since, away from this critical point, the system remains in a
static phase. When the noise is very small, the critical connectivity becomes
very large, and highly connected networks are obtained like the airports
network and the Internet. This model may be used as a starting point for
understanding the evolution of agents living on networks.",http://arxiv.org/abs/physics/0509074v2
"The number of people using online social networks in their everyday life is
continuously growing at a pace never saw before. This new kind of communication
has an enormous impact on opinions, cultural trends, information spreading and
even in the commercial success of new products. More importantly, social online
networks have revealed as a fundamental organizing mechanism in recent
country-wide social movements. In this paper, we provide a quantitative
analysis of the structural and dynamical patterns emerging from the activity of
an online social network around the ongoing May 15th (15M) movement in Spain.
Our network is made up by users that exchanged tweets in a time period of one
month, which includes the birth and stabilization of the 15M movement. We
characterize in depth the growth of such dynamical network and find that it is
scale-free with communities at the mesoscale. We also find that its dynamics
exhibits typical features of critical systems such as robustness and power-law
distributions for several quantities. Remarkably, we report that the patterns
characterizing the spreading dynamics are asymmetric, giving rise to a clear
distinction between information sources and sinks. Our study represent a first
step towards the use of data from online social media to comprehend modern
societal dynamics.",http://arxiv.org/abs/1107.1750v1
"An increasing fraction of today social interactions occur using online social
media as communication channels. Recent worldwide events, such as social
movements in Spain or revolts in the Middle East, highlight their capacity to
boost people coordination. Online networks display in general a rich internal
structure where users can choose among different types and intensity of
interactions. Despite of this, there are still open questions regarding the
social value of online interactions. For example, the existence of users with
millions of online friends sheds doubts on the relevance of these relations. In
this work, we focus on Twitter, one of the most popular online social networks,
and find that the network formed by the basic type of connections is organized
in groups. The activity of the users conforms to the landscape determined by
such groups. Furthermore, Twitter's distinction between different types of
interactions allows us to establish a parallelism between online and offline
social networks: personal interactions are more likely to occur on internal
links to the groups (the weakness of strong ties), events transmitting new
information go preferentially through links connecting different groups (the
strength of weak ties) or even more through links connecting to users belonging
to several groups that act as brokers (the strength of intermediary ties).",http://arxiv.org/abs/1107.4009v2
"This is an introduction to the special issue titled ""Collective behavior and
evolutionary games"" that is in the making at Chaos, Solitons & Fractals. The
term collective behavior covers many different phenomena in nature and society.
From bird flocks and fish swarms to social movements and herding effects, it is
the lack of a central planner that makes the spontaneous emergence of sometimes
beautifully ordered and seemingly meticulously designed behavior all the more
sensational and intriguing. The goal of the special issue is to attract
submissions that identify unifying principles that describe the essential
aspects of collective behavior, and which thus allow for a better
interpretation and foster the understanding of the complexity arising in such
systems. As the title of the special issue suggests, the later may come from
the realm of evolutionary games, but this is certainly not a necessity, neither
for this special issue, and certainly not in general. Interdisciplinary work on
all aspects of collective behavior, regardless of background and motivation,
and including synchronization and human cognition, is very welcome.",http://arxiv.org/abs/1306.2296v2
"Recent grassroots movements have suggested that online social networks might
play a key role in their organization, as adherents have a fast, many-to-many,
communication channel to help coordinate their mobilization. The structure and
dynamics of the networks constructed from the digital traces of protesters have
been analyzed to some extent recently. However, less effort has been devoted to
the analysis of the semantic content of messages exchanged during the protest.
Using the data obtained from a microblogging service during the brewing and
active phases of the 15M movement in Spain, we perform the first large scale
test of theories on collective emotions and social interaction in collective
actions. Our findings show that activity and information cascades in the
movement are larger in the presence of negative collective emotions and when
users express themselves in terms related to social content. At the level of
individual participants, our results show that their social integration in the
movement, as measured through social network metrics, increases with their
level of engagement and of expression of negativity. Our findings show that
non-rational factors play a role in the formation and activity of social
movements through online media, having important consequences for viral
spreading.",http://arxiv.org/abs/1505.03776v1
"We analyse a large sample of the Twitter activity developed around the social
movement 'Occupy Wall Street' to study the complex interactions between the
human communication activity and the semantic content of a discussion. We use a
network approach based on the analysis of the bipartite graph @Users-#Hashtags
and of its projections: the 'semantic network', whose nodes are hashtags, and
the 'users interest network', whose nodes are users In the first instance, we
find out that discussion topics (#hashtags) present a high heterogeneity, with
the distinct role of the communication hubs where most the 'opinion traffic'
passes through. In the second case, the self-organization process of users
activity leads to the emergence of two classes of communicators: the
'professionals' and the 'amateurs'. Moreover the network presents a strong
community structure, based on the differentiation of the semantic topics, and a
high level of structural robustness when a certain set of topics are censored
and/or accounts are removed. Analysing the characteristics the @Users-#Hashtags
network we can distinguish three phases of the discussion about the movement.
Each phase corresponds to specific moment of the movement: from declaration of
intent, organisation and development and the final phase of political
reactions. Each phase is characterised by the presence of specific #hashtags in
the discussion.",http://arxiv.org/abs/1412.4639v1
"Social media has become an important venue for diverse groups to share
information, discuss political issues, and organize social movements. Recent
scholarship has shown that the social media ecosystem can affect political
thinking and expression. Individuals and groups across the political spectrum
have engaged in the use of these platforms extensively, even creating their own
forums with varying approaches to content moderation in pursuit of freer
standards of speech. The Gab social media platform arose in this context. Gab
is a social media platform for the so-called alt right, and much of the popular
press has opined about the thematic content of discourses on Gab and platforms
like it, but little research has examined the content itself. Using a publicly
available dataset of all Gab posts from August 2016 until July 2019, the
current paper explores a five percent random sample of this dataset to explore
thematic content on the platform. We run multiple structural topic models,
using standard procedures to arrive at an optimal k number of topics. The final
model specifies 85 topics for 403,469 documents. We include as prevalence
variables whether the source account has been flagged as a bot and the number
of followers for the source account. Results suggest the most nodal topics in
the dataset pertain to the authenticity of the Holocaust, the meaning of red
pill, and the journalistic merit of mainstream media. We conclude by discussing
the implications of our findings for work in ethical content moderation, online
community development, political polarization, and avenues for future research.",http://arxiv.org/abs/2007.09685v1
"Black Lives Matter (BLM) is a decentralized social movement protesting
violence against Black individuals and communities, with a focus on police
brutality. The movement gained significant attention following the killings of
Ahmaud Arbery, Breonna Taylor, and George Floyd in 2020. The #BlackLivesMatter
social media hashtag has come to represent the grassroots movement, with
similar hashtags counter protesting the BLM movement, such as #AllLivesMatter,
and #BlueLivesMatter. We introduce a data set of 63.9 million tweets from 13.0
million users from over 100 countries which contain one of the following
keywords: BlackLivesMatter, AllLivesMatter, and BlueLivesMatter. This data set
contains all currently available tweets from the beginning of the BLM movement
in 2013 to 2021. We summarize the data set and show temporal trends in use of
both the BlackLivesMatter keyword and keywords associated with counter
movements. Additionally, for each keyword, we create and release a set of
Latent Dirichlet Allocation (LDA) topics (i.e., automatically clustered groups
of semantically co-occuring words) to aid researchers in identifying linguistic
patterns across the three keywords.",http://arxiv.org/abs/2009.00596v3
"Diffusion of information, spread of rumors and infectious diseases are all
instances of stochastic processes that occur over the edges of an underlying
network. Many times networks over which contagions spread are unobserved, and
such networks are often dynamic and change over time. In this paper, we
investigate the problem of inferring dynamic networks based on information
diffusion data. We assume there is an unobserved dynamic network that changes
over time, while we observe the results of a dynamic process spreading over the
edges of the network. The task then is to infer the edges and the dynamics of
the underlying network.
  We develop an on-line algorithm that relies on stochastic convex optimization
to efficiently solve the dynamic network inference problem. We apply our
algorithm to information diffusion among 3.3 million mainstream media and blog
sites and experiment with more than 179 million different pieces of information
spreading over the network in a one year period. We study the evolution of
information pathways in the online media space and find interesting insights.
Information pathways for general recurrent topics are more stable across time
than for on-going news events. Clusters of news media sites and blogs often
emerge and vanish in matter of days for on-going news events. Major social
movements and events involving civil population, such as the Libyan's civil war
or Syria's uprise, lead to an increased amount of information pathways among
blogs as well as in the overall increase in the network centrality of blogs and
social media sites.",http://arxiv.org/abs/1212.1464v1
"Inspired by the recent social movement of #MeToo, we are building a chatbot
to assist survivors of sexual harassment cases (designed for the city of
Maastricht but can easily be extended). The motivation behind this work is
twofold: properly assist survivors of such events by directing them to
appropriate institutions that can offer them help and increase the incident
documentation so as to gather more data about harassment cases which are
currently under reported. We break down the problem into three data
science/machine learning components: harassment type identification (treated as
a classification problem), spatio-temporal information extraction (treated as
Named Entity Recognition problem) and dialogue with the users (treated as a
slot-filling based chatbot). We are able to achieve a success rate of more than
98% for the identification of a harassment-or-not case and around 80% for the
specific type harassment identification. Locations and dates are identified
with more than 90% accuracy and time occurrences prove more challenging with
almost 80%. Finally, initial validation of the chatbot shows great potential
for the further development and deployment of such a beneficial for the whole
society tool.",http://arxiv.org/abs/1909.02809v1
"Social media provides the means by which extremist social movements, such as
white supremacy and anti LGBTQ, thrive online. Yet, we know little about the
roles played by the participants of such movements. In this paper, we
investigate these participants to characterize their roles, their role
dynamics, and their influence in spreading online extremism. Our participants,
online extremist accounts, are 4,876 public Facebook pages or groups that have
shared information from the websites of 289 Southern Poverty Law Center
designated extremist groups. By clustering the quantitative features followed
by qualitative expert validation, we identify five roles surrounding extremist
activism: educators, solicitors, flamers, motivators, sympathizers. For
example, solicitors use links from extremist websites to attract donations and
participation in extremist issues, whereas flamers share inflammatory extremist
content inciting anger. We further investigate role dynamics such as, how
stable these roles are over time and how likely will extremist accounts
transition from one role into another. We find that roles core to the movement,
educators and solicitors, are more stable, while flamers and motivators can
transition to sympathizers with high probability. We further find that
educators and solicitors exert the most influence in triggering extremist link
posts, whereas flamers are influential in triggering the spread of information
from fake news sources. Our results help in situating various roles on the
trajectory of deeper engagement into the extremist movements and understanding
the potential effect of various counter extremism interventions. Our findings
have implications for understanding how online extremist movements flourish
through participatory activism and how they gain a spectrum of allies for
mobilizing extremism online.",http://arxiv.org/abs/2105.08827v1
"Instagram infographics are a digital activism tool that have redefined action
frames for technology-facilitated social movements. From the 1960s through the
1980s, United States ethnic movements practiced collective action:
ideologically unified, resource-intensive traditional activism. Today,
technologically enabled movements have been categorized as practicing
connective action: individualized, low-resource online activism. Yet, we argue
that Instagram infographics are both connective and collective. This paper
juxtaposes the insights of past and present U.S. ethnic movement activists and
analyzes Black Lives Matter Instagram data over the course of 7 years
(2014-2020). We find that Instagram infographic activism bridges connective and
collective action in three ways: (1) Scope for Education: Visually enticing and
digestible infographics reduce the friction of information dissemination,
facilitating collective movement education while preserving customizability.
(2) Reconciliation for Credibility: Activists use connective features to combat
infographic misinformation and resolve internal differences, creating a trusted
collective movement front. (3) High-Resource Efforts for Transformative Change:
Instagram infographic activism has been paired with boots on the ground and
action-oriented content, curating a connective-to-collective pipeline that
expends movement resources. Our work unveils the vitality of evaluating digital
activism action frames at the movement integration level, exemplifies the
powerful coexistence of connective and collective action, and offers meaningful
design implications for activists seeking to leverage this novel tool.",http://arxiv.org/abs/2111.00714v1
"Science and society inevitably interact with each other and evolve together.
Studying the trend of science helps recognize leading topics significant for
research and establish better policies to allocate funds efficiently. Scholarly
societies such as the Korean Physics Society (KPS) also play an important role
in the history of science. Figuring out the role of these scholarly societies
motivate our research related with our society since societies pay attention to
improve our society. Although several studies try to capture the trend of
science leveraging scientific documents such as paper or patents, but these
studies limited their research scope only to the academic world, neglecting the
interaction with society. Here we try to understand the trend of science along
with society using a public magazine named ""Physics and High Technology,""
published by the Korean Physics Society (KPS). We build keyword co-occurrence
networks for each time period and applied community detection to capture the
keyword structure and tracked the structure's evolution. In the networks, a
research-related cluster is consistently dominant over time, and sub-clusters
of the research-related cluster divide into various fields of physics, implying
specialization of the physics discipline. Also, we found that education and
policy clusters appear consistently, revealing the KPS's contribution to
science and society. Furthermore, we applied PageRank algorithm to selected
keywords ('semiconductor', 'woman', 'evading'...) to investigate the temporal
change of the importance of keywords in the network. For example, the
importance of the keyword 'woman' increases as time goes by, indicating that
academia also pays attention to gender issues reflecting the social movement in
recent years.",http://arxiv.org/abs/2205.09969v2
"Online social networks (OSNs) play a crucial role in today's world. On the
one hand, they allow free speech, information sharing, and social-movements
organization, to cite a few. On the other hand, they are the tool of choice to
spread disinformation, hate speech, and to support propaganda. For these
reasons, OSNs data mining and analysis aimed at detecting disinformation
campaigns that may arm the society and, more in general, poison the democratic
posture of states, are essential activities during key events such as
elections, pandemics, and conflicts. In this paper, we studied the 2022
Russo-Ukrainian conflict on Twitter, one of the most used OSNs. We
quantitatively and qualitatively analyze a dataset of more than 5.5+ million
tweets related to the subject, generated by 1.8+ million unique users. By
leveraging statistical analysis techniques and aspect-based sentiment analysis
(ABSA), we discover hidden insights in the collected data and abnormal patterns
in the users' sentiment that in some cases confirm while in other cases
disprove common beliefs on the conflict. In particular, based on our findings
and contrary to what suggested in some mainstream media, there is no evidence
of massive disinformation campaigns. However, we have identified several
anomalies in the behavior of particular accounts and in the sentiment trend for
some subjects that represent a starting point for further analysis in the
field. The adopted techniques, the availability of the data, the replicability
of the experiments, and the preliminary findings, other than being interesting
on their own, also pave the way to further research in the domain.",http://arxiv.org/abs/2208.04903v1
"The #MeToo movement has catalyzed widespread public discourse surrounding
sexual harassment and assault, empowering survivors to share their stories and
holding perpetrators accountable. While the movement has had a substantial and
largely positive influence, this study aims to examine the potential negative
consequences in the form of increased hostility against women and men on the
social media platform Twitter. By analyzing tweets shared between October 2017
and January 2020 by more than 47.1k individuals who had either disclosed their
own sexual abuse experiences on Twitter or engaged in discussions about the
movement, we identify the overall increase in gender-based hostility towards
both women and men since the start of the movement. We also monitor 16 pivotal
real-life events that shaped the #MeToo movement to identify how these events
may have amplified negative discussions targeting the opposite gender on
Twitter. Furthermore, we conduct a thematic content analysis of a subset of
gender-based hostile tweets, which helps us identify recurring themes and
underlying motivations driving the expressions of anger and resentment from
both men and women concerning the #MeToo movement. This study highlights the
need for a nuanced understanding of the impact of social movements on online
discourse and underscores the importance of addressing gender-based hostility
in the digital sphere.",http://arxiv.org/abs/2308.13076v1
"Narratives can be powerful tools for inspiring action on pressing societal
issues such as climate change. While social science theories offer frameworks
for understanding the narratives that arise within collective movements, these
are rarely applied to the vast data available from social media platforms,
which play a significant role in shaping public opinion and mobilizing
collective action. This gap in the empirical evaluation of online narratives
limits our understanding of their relationship with public response. In this
study, we focus on plant-based diets as a form of pro-environmental action and
employ natural language processing to operationalize a theoretical framework of
moral narratives specific to the vegan movement. We apply this framework to
narratives found in YouTube videos promoting environmental initiatives such as
Veganuary, Meatless March, and No Meat May. Our analysis reveals that several
narrative types, as defined by the theory, are empirically present in the data.
To identify narratives with the potential to elicit positive public engagement,
we used text processing to estimate the proportion of comments supporting
collective action across narrative types. Video narratives advocating social
fight, whether through protest or through efforts to convert others to the
cause, are associated with a stronger sense of collective action in the
respective comments. These narrative types also demonstrate increased semantic
coherence and alignment between the message and public response, markers
typically associated with successful collective action. Our work offers new
insights into the complex factors that influence the emergence of collective
action, thereby informing the development of effective communication strategies
within social movements.",http://arxiv.org/abs/2401.09210v2
"As more algorithmic systems have come under scrutiny for their potential to
inflict societal harms, an increasing number of organizations that hold power
over harmful algorithms have chosen (or were required under the law) to abandon
them. While social movements and calls to abandon harmful algorithms have
emerged across application domains, little academic attention has been paid to
studying abandonment as a means to mitigate algorithmic harms. In this paper,
we take a first step towards conceptualizing ""algorithm abandonment"" as an
organization's decision to stop designing, developing, or using an algorithmic
system due to its (potential) harms. We conduct a thematic analysis of
real-world cases of algorithm abandonment to characterize the dynamics leading
to this outcome. Our analysis of 40 cases reveals that campaigns to abandon an
algorithm follow a common process of six iterative phases: discovery,
diagnosis, dissemination, dialogue, decision, and death, which we term the ""6
D's of abandonment"". In addition, we highlight key factors that facilitate (or
prohibit) abandonment, which include characteristics of both the technical and
social systems that the algorithm is embedded within. We discuss implications
for several stakeholders, including proprietors and technologists who have the
power to influence an algorithm's (dis)continued use, FAccT researchers, and
policymakers.",http://arxiv.org/abs/2404.13802v2
"In the context where social media is increasingly becoming a significant
platform for social movements and the formation of public opinion, accurately
simulating and predicting the dynamics of user opinions is of great importance
for understanding social phenomena, policy making, and guiding public opinion.
However, existing simulation methods face challenges in capturing the
complexity and dynamics of user behavior. Addressing this issue, this paper
proposes an innovative simulation method for the dynamics of social media user
opinions, the FDE-LLM algorithm, which incorporates opinion dynamics and
epidemic model. This effectively constrains the actions and opinion evolution
process of large language models (LLM), making them more aligned with the real
cyber world. In particular, the FDE-LLM categorizes users into opinion leaders
and followers. Opinion leaders are based on LLM role-playing and are
constrained by the CA model, while opinion followers are integrated into a
dynamic system that combines the CA model with the SIR model. This innovative
design significantly improves the accuracy and efficiency of the simulation.
Experiments were conducted on four real Weibo datasets and validated using the
open-source model ChatGLM. The results show that, compared to traditional
agent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion
diffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and
interpretability.",http://arxiv.org/abs/2409.08717v3
"Brownian motion have long been studied on a diversity of fields, not only in
physics of statistical mechanics, but also in biological models, finance and
economic process, and social systems. In the past twenty years, there has been
a growing interest in studying the model in self-propelled feature and
interaction force such that the model also fits into study of social phenomenon
of many individuals. This article will continue with this research trend and
especially investigate the model in paradigms for a quantitative description of
social and economic process. We mainly discuss a class of collective decision
process of Brownian agent/particles, where the stochastic process does not
exist in the fluctuation in the traditional Brownian motion, but in selection
among several discrete choices. Their decisions interacts with each other in a
given social topology. To simply our discussion the binary choice problem is
particularly discussed where each agent only takes an alternative of two
choices. Mathematically, we introduce a set of arrays to describe social
relationship of agents in a quantitative manner, and the arrays deduce the
group social force and opinion dynamics, which are useful to study complex
social movement and self-organization phenomena including discrete-choice
activities, social groups and de-individualization effect. Such agent-based
simulation symbolizes a variety of collective activities in human society,
especially in the field of economics and social science.",http://arxiv.org/abs/2411.09840v2
"The potential for machine learning (ML) systems to amplify social inequities
and unfairness is receiving increasing popular and academic attention. A surge
of recent work has focused on the development of algorithmic tools to assess
and mitigate such unfairness. If these tools are to have a positive impact on
industry practice, however, it is crucial that their design be informed by an
understanding of real-world needs. Through 35 semi-structured interviews and an
anonymous survey of 267 ML practitioners, we conduct the first systematic
investigation of commercial product teams' challenges and needs for support in
developing fairer ML systems. We identify areas of alignment and disconnect
between the challenges faced by industry practitioners and solutions proposed
in the fair ML research literature. Based on these findings, we highlight
directions for future ML and HCI research that will better address industry
practitioners' needs.",http://arxiv.org/abs/1812.05239v2
"We have two main aims in this paper. First we use theories of disease
spreading on networks to look at the COVID-19 epidemic on the basis of
individual contacts -- these give rise to predictions which are often rather
different from the homogeneous mixing approaches usually used. Our second aim
is to look at the role of social deprivation, again using networks as our
basis, in the spread of this epidemic. We choose the city of Kolkata as a case
study, but assert that the insights so obtained are applicable to a wide
variety of urban environments which are densely populated and where social
inequalities are rampant. Our predictions of hotspots are found to be in good
agreement with those currently being identifed empirically as containment zones
and provide a useful guide for identifying potential areas of concern.",http://arxiv.org/abs/2005.00491v1
"Data mining revealed a cluster of economic, psychological, social and
cultural indicators that in combination predicted corruption and wealth of
European nations. This prosperity syndrome of self-reliant citizens, efficient
division of labor, a sophisticated scientific community, and respect for the
law, was clearly distinct from that of poor countries that had a diffuse
relationship between high corruption perception, low GDP/capita, high social
inequality, low scientific development, reliance on family and friends, and
languages with many words for guilt. This suggests that there are many ways for
a nation to be poor, but few ones to become rich, supporting the existence of
synergistic interactions between the components in the prosperity syndrome
favoring economic growth. No single feature was responsible for national
prosperity. Focusing on synergies rather than on single features should improve
our understanding of the transition from poverty and corruption to prosperity
in European nations and elsewhere.",http://arxiv.org/abs/1604.00283v1
"Word embeddings trained on large-scale historical corpora can illuminate
human biases and stereotypes that perpetuate social inequalities. These
embeddings are often trained in separate vector space models defined according
to different attributes of interest. In this paper, we develop a unified
dynamic embedding model that learns attribute-specific word embeddings. We
apply our model to investigate i) 20th century gender and ethnic occupation
biases embedded in the Corpus of Historical American English (COHA), and ii)
biases against refugees embedded in a novel corpus of talk radio transcripts
containing 119 million words produced over one month across 83 stations and 64
cities. Our results shed preliminary light on scenarios when dynamic embedding
models may be more suitable for representing linguistic biases than individual
vector space models, and vice-versa.",http://arxiv.org/abs/1904.03352v2
"Accompanied by the development of digital media, the threat of information
cocoon has become a significant issue. However, little is known about the
measure of information cocoon as a cultural space and its relationship with
social class. This study addresses this problem by constructing the cultural
space with word embedding models and random shuffling methods among three
large-scale digital media use datasets. In the light of field theory of
cultural production, we investigate the information cocoon effect on different
social classes among 979 computer users, 100,000 smartphone users, and 159,373
mobile reading application users. Our analysis reveals that information cocoons
widely exist in the daily use of digital media. Moreover, people of lower
social class have a higher probability of getting stuck in the information
cocoon filled with the entertainment content. In contrast, the people of higher
social class have more capability to stride over the constraints of the
information cocoon. The results suggest that the disadvantages for vulnerable
groups in acquiring knowledge may further widen social inequality.",http://arxiv.org/abs/2007.10083v3
"The form of political polarization where citizens develop strongly negative
attitudes towards out-party policies and members has become increasingly
prominent across many democracies. Economic hardship and social inequality, as
well as inter-group and racial conflict, have been identified as important
contributing factors to this phenomenon known as ""affective polarization."" Such
partisan animosities are exacerbated when these interests and identities become
aligned with existing party cleavages. In this paper we use a model of cultural
evolution to study how these forces combine to generate and maintain affective
political polarization. We show that economic events can drive both affective
polarization and sorting of group identities along party lines, which in turn
can magnify the effects of underlying inequality between those groups. But on a
more optimistic note, we show that sufficiently high levels of wealth
redistribution through the provision of public goods can counteract this
feedback and limit the rise of polarization. We test some of our key
theoretical predictions using survey data on inter-group polarization, sorting
of racial groups and affective polarization in the United States over the past
50 years.",http://arxiv.org/abs/2103.14619v1
"Cultural code-switching concerns how we adjust our overall behaviours,
manners of speaking, and appearance in response to a perceived change in our
social environment. We defend the need to investigate cultural code-switching
capacities in artificial intelligence systems. We explore a series of ethical
and epistemic issues that arise when bringing cultural code-switching to bear
on artificial intelligence. Building upon Dotson's (2014) analysis of
testimonial smothering, we discuss how emerging technologies in AI can give
rise to epistemic oppression, and specifically, a form of self-silencing that
we call 'cultural smothering'. By leaving the socio-dynamic features of
cultural code-switching unaddressed, AI systems risk negatively impacting
already-marginalised social groups by widening opportunity gaps and further
entrenching social inequalities.",http://arxiv.org/abs/2112.08256v1
"As different research works report and daily life experiences confirm,
learning models can result in biased outcomes. The biased learned models
usually replicate historical discrimination in society and typically negatively
affect the less represented identities. Robots are equipped with these models
that allow them to operate, performing tasks more complex every day. The
learning process consists of different stages depending on human judgments.
Moreover, the resulting learned models for robot decisions rely on recorded
labeled data or demonstrations. Therefore, the robot learning process is
susceptible to bias linked to human behavior in society. This imposes a
potential danger, especially when robots operate around humans and the learning
process can reflect the social unfairness present today. Different feminist
proposals study social inequality and provide essential perspectives towards
removing bias in various fields. What is more, feminism allowed and still
allows to reconfigure numerous social dynamics and stereotypes advocating for
equality across people through their diversity. Consequently, we provide a
feminist perspective on the robot learning process in this work. We base our
discussion on intersectional feminism, community feminism, decolonial feminism,
and pedagogy perspectives, and we frame our work in a feminist robotics
approach. In this paper, we present an initial discussion to emphasize the
relevance of feminist perspectives to explore, foresee, en eventually correct
the biased robot decisions.",http://arxiv.org/abs/2201.10853v1
"At the heart of what drives the bulk of innovation and activity in Silicon
Valley and elsewhere is scalability. This unwavering commitment to scalability
-- to identify strategies for efficient growth -- is at the heart of what we
refer to as ""scale thinking."" Whether people are aware of it or not, scale
thinking is all-encompassing. It is not just an attribute of one's product,
service, or company, but frames how one thinks about the world (what
constitutes it and how it can be observed and measured), its problems (what is
a problem worth solving versus not), and the possible technological fixes for
those problems. This paper examines different facets of scale thinking and its
implication on how we view technology and collaborative work. We argue that
technological solutions grounded in scale thinking are unlikely to be as
liberatory or effective at deep, systemic change as their purveyors imagine.
Rather, solutions which resist scale thinking are necessary to undo the social
structures which lie at the heart of social inequality. We draw on recent work
on mutual aid networks and propose questions to ask of collaborative work
systems as a means to evaluate technological solutions and guide designers in
identifying sites of resistance to scale thinking.",http://arxiv.org/abs/2010.08850v2
"Sociotechnical systems within cities are now equipped with machine learning
algorithms in hopes to increase efficiency and functionality by modeling and
predicting trends. Machine learning algorithms have been applied in these
domains to address challenges such as balancing the distribution of bikes
throughout a city and identifying demand hotspots for ride sharing drivers.
However, these algorithms applied to challenges in sociotechnical systems have
exacerbated social inequalities due to previous bias in data sets or the lack
of data from marginalized communities. In this paper, I will address how smart
mobility initiatives in cities use machine learning algorithms to address
challenges. I will also address how these algorithms unintentionally
discriminate against features such as socioeconomic status to motivate the
importance of algorithmic fairness. Using the bike sharing program in
Pittsburgh, PA, I will present a position on how discrimination can be
eliminated from the pipeline using Bayesian Optimization.",http://arxiv.org/abs/2011.13988v2
"Reviewing contracts is a time-consuming procedure that incurs large expenses
to companies and social inequality to those who cannot afford it. In this work,
we propose ""document-level natural language inference (NLI) for contracts"", a
novel, real-world application of NLI that addresses such problems. In this
task, a system is given a set of hypotheses (such as ""Some obligations of
Agreement may survive termination."") and a contract, and it is asked to
classify whether each hypothesis is ""entailed by"", ""contradicting to"" or ""not
mentioned by"" (neutral to) the contract as well as identifying ""evidence"" for
the decision as spans in the contract. We annotated and release the largest
corpus to date consisting of 607 annotated contracts. We then show that
existing models fail badly on our task and introduce a strong baseline, which
(1) models evidence identification as multi-label classification over spans
instead of trying to predict start and end tokens, and (2) employs more
sophisticated context segmentation for dealing with long documents. We also
show that linguistic characteristics of contracts, such as negations by
exceptions, are contributing to the difficulty of this task and that there is
much room for improvement.",http://arxiv.org/abs/2110.01799v1
"We introduce and discuss a system of one-dimensional kinetic equations
describing the influence of higher education in the social stratification of a
multi-agent society. The system is obtained by coupling a model for knowledge
formation with a kinetic description of the social climbing in which the
parameters characterizing the elementary interactions leading to the formation
of a social elite are assumed to depend on the degree of knowledge/education of
the agents. In addition, we discuss the case in which the education level of an
individual is function of the position occupied in the social ranking. With
this last assumption we obtain a fully coupled model in which knowledge and
social status influence each other. In the last part, we provide several
numerical experiments highlighting the role of education in reducing social
inequalities and in promoting social mobility.",http://arxiv.org/abs/2110.08153v1
"Social stereotypes negatively impact individuals' judgements about different
groups and may have a critical role in how people understand language directed
toward minority social groups. Here, we assess the role of social stereotypes
in the automated detection of hateful language by examining the relation
between individual annotator biases and erroneous classification of texts by
hate speech classifiers. Specifically, in Study 1 we investigate the impact of
novice annotators' stereotypes on their hate-speech-annotation behavior. In
Study 2 we examine the effect of language-embedded stereotypes on expert
annotators' aggregated judgements in a large annotated corpus. Finally, in
Study 3 we demonstrate how language-embedded stereotypes are associated with
systematic prediction errors in a neural-network hate speech classifier. Our
results demonstrate that hate speech classifiers learn human-like biases which
can further perpetuate social inequalities when propagated at scale. This
framework, combining social psychological and computational linguistic methods,
provides insights into additional sources of bias in hate speech moderation,
informing ongoing debates regarding fairness in machine learning.",http://arxiv.org/abs/2110.14839v1
"The psychological costs of the attention economy are often considered through
the binary of harmful design and healthy use, with digital well-being chiefly
characterised as a matter of personal responsibility. This article adopts an
interdisciplinary approach to highlight the empirical, ideological, and
political limits of embedding this individualised perspective in computational
discourses and designs of digital well-being measurement. We will reveal
well-being to be a culturally specific and environmentally conditioned concept
and will problematize user engagement as a universal proxy for well-being.
Instead, the contributing factors of user well-being will be located in
environing social, cultural, and political conditions far beyond the control of
individual users alone. In doing so, we hope to reinvigorate the issue of
digital well-being measurement as a nexus point of political concern, through
which multiple disciplines can study experiences of digital ill as symptomatic
of wider social inequalities and (capitalist) relations of power.",http://arxiv.org/abs/2203.08199v1
"Recent research has shown that seemingly fair machine learning models, when
used to inform decisions that have an impact on peoples' lives or well-being
(e.g., applications involving education, employment, and lending), can
inadvertently increase social inequality in the long term. This is because
prior fairness-aware algorithms only consider static fairness constraints, such
as equal opportunity or demographic parity. However, enforcing constraints of
this type may result in models that have negative long-term impact on
disadvantaged individuals and communities. We introduce ELF (Enforcing
Long-term Fairness), the first classification algorithm that provides
high-confidence fairness guarantees in terms of long-term, or delayed, impact.
We prove that the probability that ELF returns an unfair solution is less than
a user-specified tolerance and that (under mild assumptions), given sufficient
training data, ELF is able to find and return a fair solution if one exists. We
show experimentally that our algorithm can successfully mitigate long-term
unfairness.",http://arxiv.org/abs/2208.11744v1
"Based on measure transportation ideas and the related concepts of
center-outward quantile functions, we propose multiple-output center-outward
generalizations of the traditional univariate concepts of Lorenz and
concentration functions, and the related Gini and Kakwani coefficients. These
new concepts have a natural interpretation, either in terms of contributions of
central (""middle-class"") regions to the expectation of some variable of
interest, or in terms of the physical notions of work and energy, which sheds
new light on the nature of economic and social inequalities. Importantly, the
proposed concepts pave the way to statistically sound definitions, based on
multiple variables, of quantiles and quantile regions, and the concept of
""middle class,"" of high relevance
  in various socio-economic contexts.",http://arxiv.org/abs/2211.10822v1
"A simple heuristic model, including the multiple exchanges between economic
agents, is used to explain the mechanism of emerging and maintenance of social
inequality in the market economy. The model allows calculating a density
function of the population distribution over income. The function can be
considered as a strongly deformed Gauss distribution function, whereas, at
large incomes, it coincides with the Pareto distribution. The external, in
relation to the model under consideration, force is necessary to maintain the
strong non-equilibrium in a stationary state, and this force is the
non-equivalence of elementary exchanges: the agent who already receives the
higher income has the advantage: it provokes the rich to be getting more rich
and the poor to be getting pauper.",http://arxiv.org/abs/2302.10751v3
"Intersectionality is a critical framework that, through inquiry and praxis,
allows us to examine how social inequalities persist through domains of
structure and discipline. Given AI fairness' raison d'etre of ""fairness"", we
argue that adopting intersectionality as an analytical framework is pivotal to
effectively operationalizing fairness. Through a critical review of how
intersectionality is discussed in 30 papers from the AI fairness literature, we
deductively and inductively: 1) map how intersectionality tenets operate within
the AI fairness paradigm and 2) uncover gaps between the conceptualization and
operationalization of intersectionality. We find that researchers
overwhelmingly reduce intersectionality to optimizing for fairness metrics over
demographic subgroups. They also fail to discuss their social context and when
mentioning power, they mostly situate it only within the AI pipeline. We: 3)
outline and assess the implications of these gaps for critical inquiry and
praxis, and 4) provide actionable recommendations for AI fairness researchers
to engage with intersectionality in their work by grounding it in AI
epistemology.",http://arxiv.org/abs/2303.17555v2
"While machine learning can myopically reinforce social inequalities, it may
also be used to dynamically seek equitable outcomes. In this paper, we
formalize long-term fairness in the context of online reinforcement learning.
This formulation can accommodate dynamical control objectives, such as driving
equity inherent in the state of a population, that cannot be incorporated into
static formulations of fairness. We demonstrate that this framing allows an
algorithm to adapt to unknown dynamics by sacrificing short-term incentives to
drive a classifier-population system towards more desirable equilibria. For the
proposed setting, we develop an algorithm that adapts recent work in online
learning. We prove that this algorithm achieves simultaneous probabilistic
bounds on cumulative loss and cumulative violations of fairness (as statistical
regularities between demographic groups). We compare our proposed algorithm to
the repeated retraining of myopic classifiers, as a baseline, and to a deep
reinforcement learning algorithm that lacks safety guarantees. Our experiments
model human populations according to evolutionary game theory and integrate
real-world datasets.",http://arxiv.org/abs/2304.09362v2
"Factors contributing to social inequalities are also associated with negative
mental health outcomes leading to disparities in mental well-being. We propose
a Bayesian hierarchical model which can evaluate the impact of policies on
population well-being, accounting for spatial/temporal dependencies. Building
on an interrupted time series framework, our approach can evaluate how
different profiles of individuals are affected in different ways, whilst
accounting for their uncertainty. We apply the framework to assess the impact
of the United Kingdoms welfare reform, which took place throughout the 2010s,
on mental well-being using data from the UK Household Longitudinal Study. The
additional depth of knowledge is essential for effective evaluation of current
policy and implementation of future policy.",http://arxiv.org/abs/2306.15525v1
"Individuals socio-demographic and economic characteristics crucially shape
the spread of an epidemic by largely determining the exposure level to the
virus and the severity of the disease for those who got infected. While the
complex interplay between individual characteristics and epidemic dynamics is
widely recognized, traditional mathematical models often overlook these
factors. In this study, we examine two important aspects of human behavior
relevant to epidemics: contact patterns and vaccination uptake. Using data
collected during the Covid-19 pandemic in Hungary, we first identify the
dimensions along which individuals exhibit the greatest variation in their
contact patterns and vaccination attitudes. We find that generally privileged
groups of the population have higher number of contact and a higher vaccination
uptake with respect to disadvantaged groups. Subsequently, we propose a
data-driven epidemiological model that incorporates these behavioral
differences. Finally, we apply our model to analyze the fourth wave of Covid-19
in Hungary, providing valuable insights into real-world scenarios. By bridging
the gap between individual characteristics and epidemic spread, our research
contributes to a more comprehensive understanding of disease dynamics and
informs effective public health strategies.",http://arxiv.org/abs/2307.04865v1
"Accessibility of different places, such as hospitals or areas with jobs, is
important in understanding transportation systems, urban environments, and
potential inequalities in what services and opportunities different people can
reach. Often, research in this area is framed around the question of whether
people living in an area are able to reach certain destinations within a
prespecified time frame. However, the cost of such journeys, and whether they
are affordable, is often omitted or not considered to the same level. Here, we
present a Python package and an associated data set which allows to analyse the
cost of train journeys in Great Britain. We present the original data set we
used to construct this, the Python package we developed to analyse it, and the
output data set which we generated. We envisage our work to allow researchers,
policy makers, and other stakeholders, to investigate questions around the cost
of train journeys, any geographical or social inequalities arising from this,
and how the transport system could be improved.",http://arxiv.org/abs/2310.19754v1
"Intelligent software systems (e.g., conversational agents, profiling systems,
recruitment systems) are often designed in a manner which may perpetuates
anti-Black racism and other forms of socio-cultural discrimination. This may
reinforce social inequities by supporting the automation of consequential and
sometimes unfair decisions that may be made by such systems and which may have
an adverse impact on credit scores, insurance payouts, and even health
evaluations, just to name a few. My lightning talk will therefore emphasize the
need to propose a new type of non-functional requirements called ECI (emotional
and cultural intelligence) requirements that will aim at developing
discrimination-aware intelligent software systems. Such systems will notably be
able to behave empathetically toward everyone, including minoritized groups and
will ensure they are treated fairly. My talk will also emphasize the need to
develop novel system assurance solutions to assure these ECI requirements are
sufficiently supported by intelligent software systems.",http://arxiv.org/abs/2311.08431v2
"In critical machine learning applications, ensuring fairness is essential to
avoid perpetuating social inequities. In this work, we address the challenges
of reducing bias and improving accuracy in data-scarce environments, where the
cost of collecting labeled data prohibits the use of large, labeled datasets.
In such settings, active learning promises to maximize marginal accuracy gains
of small amounts of labeled data. However, existing applications of active
learning for fairness fail to deliver on this, typically requiring large
labeled datasets, or failing to ensure the desired fairness tolerance is met on
the population distribution.
  To address such limitations, we introduce an innovative active learning
framework that combines an exploration procedure inspired by posterior sampling
with a fair classification subroutine. We demonstrate that this framework
performs effectively in very data-scarce regimes, maximizing accuracy while
satisfying fairness constraints with high probability. We evaluate our proposed
approach using well-established real-world benchmark datasets and compare it
against state-of-the-art methods, demonstrating its effectiveness in producing
fair models, and improvement over existing methods.",http://arxiv.org/abs/2312.08559v1
"Recent conversations in the algorithmic fairness literature have raised
several concerns with standard conceptions of fairness. First, constraining
predictive algorithms to satisfy fairness benchmarks may lead to non-optimal
outcomes for disadvantaged groups. Second, technical interventions are often
ineffective by themselves, especially when divorced from an understanding of
structural processes that generate social inequality. Inspired by both these
critiques, we construct a common decision-making model, using mortgage loans as
a running example. We show that under some conditions, any choice of decision
threshold will inevitably perpetuate existing disparities in financial
stability unless one deviates from the Pareto optimal policy. Then, we model
the effects of three different types of interventions. We show how different
interventions are recommended depending upon the difficulty of enacting
structural change upon external parameters and depending upon the policymaker's
preferences for equity or efficiency. Counterintuitively, we demonstrate that
preferences for efficiency over equity may lead to recommendations for
interventions that target the under-resourced group. Finally, we simulate the
effects of interventions on a dataset that combines HMDA and Fannie Mae loan
data. This research highlights the ways that structural inequality can be
perpetuated by seemingly unbiased decision mechanisms, and it shows that in
many situations, technical solutions must be paired with external,
context-aware interventions to enact social change.",http://arxiv.org/abs/2406.01323v1
"There is an ongoing debate on balancing the benefits and risks of artificial
intelligence (AI) as AI is becoming critical to improving healthcare delivery
and patient outcomes. Such improvements are essential in resource-constrained
settings where millions lack access to adequate healthcare services, such as in
Africa. AI in such a context can potentially improve the effectiveness,
efficiency, and accessibility of healthcare services. Nevertheless, the
development and use of AI-driven healthcare systems raise numerous ethical,
legal, and socio-economic issues. Justice is a major concern in AI that has
implications for amplifying social inequities. This paper discusses these
implications and related justice concepts such as solidarity, Common Good,
sustainability, AI bias, and fairness. For Africa to effectively benefit from
AI, these principles should align with the local context while balancing the
risks. Compared to mainstream ethical debates on justice, this perspective
offers context-specific considerations for equitable healthcare AI development
in Africa.",http://arxiv.org/abs/2406.10653v1
"The sharply increasing sizes of artificial intelligence (AI) models come with
significant energy consumption and environmental footprints, which can
disproportionately impact certain (often marginalized) regions and hence create
environmental inequity concerns. Moreover, concerns with social inequity have
also emerged, as AI computing resources may not be equitably distributed across
the globe and users from certain disadvantaged regions with severe resource
constraints can consistently experience inferior model performance.
Importantly, the inequity concerns that encompass both social and environmental
dimensions still remain unexplored and have increasingly hindered responsible
AI. In this paper, we leverage the spatial flexibility of AI inference
workloads and propose equitable geographical load balancing (GLB) to fairly
balance AI's regional social and environmental costs. Concretely, to penalize
the disproportionately high social and environmental costs for equity, we
introduce $L_q$ norms as novel regularization terms into the optimization
objective for GLB decisions. Our empirical results based on real-world AI
inference traces demonstrate that while the existing GLB algorithms result in
disproportionately large social and environmental costs in certain regions, our
proposed equitable GLB can fairly balance AI's negative social and
environmental costs across all the regions.",http://arxiv.org/abs/2407.05176v1
"Data science pipelines inform and influence many daily decisions, from what
we buy to who we work for and even where we live. When designed incorrectly,
these pipelines can easily propagate social inequity and harm. Traditional
solutions are technical in nature; e.g., mitigating biased algorithms. In this
vision paper, we introduce a novel lens for promoting responsible data science
using theories of behavior change that emphasize not only technical solutions
but also the behavioral responsibility of practitioners. By integrating
behavior change theories from cognitive psychology with data science workflow
knowledge and ethics guidelines, we present a new perspective on responsible
data science. We present example data science interventions in machine learning
and visual data analysis, contextualized in behavior change theories that could
be implemented to interrupt and redirect potentially suboptimal or negligent
practices while reinforcing ethically conscious behaviors. We conclude with a
call to action to our community to explore this new research area of behavior
change interventions for responsible data science.",http://arxiv.org/abs/2410.17273v1
"Liking it or not, ready or not, we are likely to enter a new phase of human
history in which Artificial Intelligence (AI) will dominate economic production
and social life -- the AI Revolution. Before the actual arrival of the AI
Revolution, it is time for us to speculate on how AI will impact the social
world. In this article, we focus on the social impact of generative LLM-based
AI (GELLMAI), discussing societal factors that contribute to its technological
development and its potential roles in enhancing both between-country and
within-country social inequality. There are good indications that the US and
China will lead the field and will be the main competitors for domination of AI
in the world. We conjecture the AI Revolution will likely give rise to a
post-knowledge society in which knowledge per se will become less important
than in today's world. Instead, individual relationships and social identity
will become more important. So will soft skills.",http://arxiv.org/abs/2410.21281v1
"Dialectal Arabic (DA) varieties are under-served by language technologies,
particularly large language models (LLMs). This trend threatens to exacerbate
existing social inequalities and limits LLM applications, yet the research
community lacks operationalized performance measurements in DA. We present a
framework that comprehensively assesses LLMs' DA modeling capabilities across
four dimensions: fidelity, understanding, quality, and diglossia. We evaluate
nine LLMs in eight DA varieties and provide practical recommendations. Our
evaluation suggests that LLMs do not produce DA as well as they understand it,
not because their DA fluency is poor, but because they are reluctant to
generate DA. Further analysis suggests that current post-training can
contribute to bias against DA, that few-shot examples can overcome this
deficiency, and that otherwise no measurable features of input text correlate
well with LLM DA performance.",http://arxiv.org/abs/2412.04193v2
"This paper investigates the impact of Social Discount Rate (SDR) choice on
intergenerational equity issues caused by Public-Private Partnerships (PPPs)
projects. Indeed, more PPPs mean more debt being accumulated for future
generations leading to a fiscal deficit crisis. The paper draws on how the SDR
level taken today distributes societies on the Social Welfare Function (SWF).
This is done by answering two sub-questions: (i) What is the risk of PPPs debts
being off-balance sheet? (ii) How do public policies, based on the envisaged
SDR, position society within different ethical perspectives? The answers are
obtained from a discussion of the different SDRs (applied in the UK for
examples) according to the merits of the pertinent ethical theories, namely
libertarian, egalitarian, utilitarian and Rawlsian. We find that public
policymakers can manipulate the SDR to make PPPs looking like a better option
than the traditional financing form. However, this antagonises the Value for
Money principle. We also point out that public policy is not harmonised with
ethical theories. We find that at present (in the UK), the SDR is somewhere
between weighted utilitarian and Rawlsian societies in the trade-off curve.
Alas, our study finds no evidence that the (UK) government is using a
sophisticated system to keep pace with the accumulated off-balance sheet debts.
Thus, the exact prediction of the final state is hardly made because of the
uncertainty factor. We conclude that our study hopefully provides a good
analytical framework for policymakers in order to draw on the merits of ethical
theories before initiating public policies like PPPs.",http://arxiv.org/abs/2201.09064v1
"The Improbability Scale (IS) is proposed as a way of communicating to the
general public the improbability (and by implication, the probability) of
events predicted as the result of scientific research. Through the use of the
Improbability Scale, the public will be able to evaluate more easily the
relative risks of predicted events and draw proper conclusions when asked to
support governmental and public policy decisions arising from that research.",http://arxiv.org/abs/physics/0503229v2
"Besides an indicator of the GDP, the Central Bank of Venezuela generates the
so called Monthly Economic Activity General Indicator. The a priori knowledge
of this indicator, which represents and sometimes even anticipates the
economy's fluctuations, could be helpful in developing public policies and in
investment decision making. The purpose of this study is forecasting the IGAEM
through non parametric methods, an approach that has proven effective in a wide
variety of problems in economics and finance.",http://arxiv.org/abs/0708.3463v1
"Scientific publications and other genres of research output are increasingly
being cited in policy documents. Citations in documents of this nature could be
considered a critical indicator of the significance and societal impact of the
research output. In this study, we built classification models that predict
whether a particular research work is likely to be cited in a public policy
document based on the attention it received online, primarily on social media
platforms. We evaluated the classifiers based on their accuracy, precision, and
recall values. We found that Random Forest and Multinomial Naive Bayes
classifiers performed better overall.",http://arxiv.org/abs/1706.04140v1
"Risk, including economic risk, is increasingly a concern for public policy
and management. The possibility of dealing effectively with risk is hampered,
however, by lack of a sound empirical basis for risk assessment and management.
The paper demonstrates the general point for cost and demand risks in urban
rail projects. The paper presents empirical evidence that allow valid economic
risk assessment and management of urban rail projects, including benchmarking
of individual or groups of projects. Benchmarking of the Copenhagen Metro is
presented as a case in point. The approach developed is proposed as a model for
other types of policies and projects in order to improve economic and financial
risk assessment and management in policy and planning.",http://arxiv.org/abs/1303.7402v1
"Public policy making has direct and indirect impacts on social behaviors.
However, using system dynamics model alone to assess these impacts fails to
consider the interactions among social elements, thus may produce doubtful
conclusions. In this study, we examine the political science theory of greed
and grievance in modeling civil conflicts. An agent-based model is built based
on an existing rebellion model in Netlogo. The modifications and improvements
in our model are elaborated. Several case studies are used to demonstrate the
use of our model for investigating emergent phenomena and implications of
governmental policies.",http://arxiv.org/abs/1908.06883v1