Abstract,Link,Label
a review of recent theoretical work on the interactions of supermassive single and binary black holes with their nuclear environments highlighting ways in which the observed structure of nuclei can be used to constrain the formation history of black holes,http://arxiv.org/abs/astro-ph/0409290v2,0
primordial stars are likely to be very massive geq30msun form in isolation and will likely leave black holes as remnants in the centers of their host dark matter halos in the mass range 1061010ms such early black holes at redshifts zgtsim10 could be the seed black holes for the many supermassive black holes found in galaxies in the local universe if they exist their mergers with nearby supermassive black holes may be a prime signal for long wavelength gravitational wave detectors we simulate formation of black holes in the center of high redshift dark matter halos and explore implications of initial natal kick velocities conjectured by some formation models the central concentration of early black holes in present day galaxies is reduced if they are born even with moderate kicks of tens of kms the modest kicks allow the black holes to leave their parent halo which consequently leads to dynamical friction being less effective on the lower mass black holes as compared to those still embedded in their parent halos therefore merger rates may be reduced by more than an order of magnitude using analytical and illustrative cosmological nbody simulations we quantify the role of natal kicks of black holes formed from massive metal free stars on their merger rates with supermassive black holes in present day galaxies our results also apply to black holes ejected by the gravitational slingshot mechanism,http://arxiv.org/abs/astro-ph/0512123v2,0
in this article we consider the growth of seed black holes immersed in dark matter halos we first investigate the adiabatic growth in various initial distribution functions isothermal power law and nfw and find the resulting density radial velocity and anisotropy profiles in addition we estimate the growth rate for a given black hole mass in the corresponding adiabatically modified dark matter distribution function only in the isothermal case is there a convincing black hole massage relation by calculating the line of sight velocity dispersion for the various cases as a function of the black hole mass we find the predicted adiabatic mbhsigma relation this never approaches the recently observed power law we conclude by abandoning adiabaticity suggesting that the black hole grows proportionally to the dark matter halo itself on a dynamic time scale this allows us to relate the observed mbhsigma relation to the cosmological power spectrum on galactic scales by using dimensional scaling arguments,http://arxiv.org/abs/astro-ph/0201153v1,0
we discuss the hypothesis that the cosmological baryon asymmetry and entropy were produced in the early universe by the primordial black hole pbhs evaporation,http://arxiv.org/abs/astro-ph/0009407v1,0
the first massive astrophysical black holes likely formed at high redshifts z10 at the centers of low mass 106 msun dark matter concentrations these black holes grow by mergers and gas accretion evolve into the population of bright quasars observed at lower redshifts and eventually leave the supermassive black hole remnants that are ubiquitous at the centers of galaxies in the nearby universe the astrophysical processes responsible for the formation of the earliest seed black holes are poorly understood the purpose of this review is threefold 1 to describe theoretical expectations for the formation and growth of the earliest black holes within the general paradigm of hierarchical cold dark matter cosmologies 2 to summarize several relevant recent observations that have implications for the formation of the earliest black holes and 3 to look into the future and assess the power of forthcoming observations to probe the physics of the first active galactic nuclei,http://arxiv.org/abs/astro-ph/0403225v1,0
primordial stars are likely to be very massive 30 msun form in isolation and will likely leave black holes as remnants in the centers of their host dark matter halos we expect primordial stars to form in halos in the mass range 1061010 msun some of these early black holes formed at redshifts z10 could be the seed black hole for a significant fraction of the supermassive black holes found in galaxies in the local universe if the black hole descendants of the primordial stars exist their mergers with nearby supermassive black holes may be a prime candidate for long wavelength gravitational wave detectors we simulate formation and evolution of dark matter halos in lambdacdm universe we seed highredshift dark matter halos with early black holes and explore the merger history of the host halos and the implications of black holes kick velocities arising from their coalescence the central concentration of low mass early black holes in present day galaxies is reduced if they experience even moderate kicks of tens of kms even such modest kicks allow the black holes to leave their parent halo which consequently leads to dynamical friction being less effective on the low mass black holes that were ejected compared to those still embedded in their parent halos therefore merger rates with central supermassive black holes in the largest halos may be reduced by more than an order of magnitude using analytical and illustrative cosmological nbody simulations we quantify the role of kicks on the merger rates of black holes formed from massive metal free stars with supermassive black holes in present day galaxies,http://arxiv.org/abs/astro-ph/0609443v2,0
if the cosmological dark matter has a component made of small primordial black holes they may have a significant impact on the physics of the first stars and on the subsequent formation of massive black holes primordial black holes would be adiabatically contracted into these stars and then would sink to the stellar center by dynamical friction creating a larger black hole which may quickly swallow the whole star if these primordial black holes are heavier than sim 1022 rm g the first stars would likely live only for a very short time and would not contribute much to the reionization of the universe they would instead become 10 103 modot black holes which depending on subsequent accretion could serve as seeds for the supermassive black holes seen at high redshifts as well as those inside galaxies today,http://arxiv.org/abs/0812.0585v2,0
the relativistic accretion rate of dark matter by a black hole is revisited under the assumption that the phase space density indicator qrhoinftysigma3infty remains constant during the inflow the derived accretion rate can be higher up to five orders of magnitude than the classical accretion formula valid for nonrelativistic and noninteracting particles when typical dark halo conditions are considered for these typical conditions the critical point of the flow is located at distances of about 30150 times the horizon radius application of our results to black hole seeds hosted by halos issued from cosmological simulations indicate that dark matter contributes to no more than 10 of the total accreted mass confirming that the bolometric quasar luminosity is related to the baryonic accretion history of the black hole,http://arxiv.org/abs/0802.2041v1,0
if the cosmological dark matter is primarily in the form of an elementary particle which has cross section and mass for selfinteraction having a ratio similar to that of ordinary nuclear matter then seed black holes formed in stellar collapse will grow in a hubble time due to accretion of the dark matter to a mass range 106 109 solar masses furthermore the dependence of the final black hole mass on the galaxy velocity dispersion will be approximately as observed and the growth rate will show a time dependence consistent with observations other astrophysical consequences of collisional dark matter and tests of the idea are noted,http://arxiv.org/abs/astro-ph/9912548v2,0
supermassive black holes smbhs are nowadays believed to reside in most local galaxies and the available data show an empirical correlation between bulge luminosity or stellar velocity dispersion and black hole mass suggesting a single mechanism for assembling black holes and forming spheroids in galaxy halos the evidence is therefore in favour of a coevolution between galaxies black holes and quasars in cold dark matter cosmogonies smallmass subgalactic systems form first to merge later into larger and larger structures in this paradigm galaxy halos experience multiple mergers during their lifetime if every galaxy with a bulge hosts a smbh in its center and a local galaxy has been made up by multiple mergers then a black hole binary is a natural evolutionary stage the evolution of the supermassive black hole population clearly has to be investigated taking into account both the cosmological framework and the dynamical evolution of smbhs and their hosts the seeds of smbhs have to be looked for in the early universe as very luminous quasars are detected up to redshift higher than z6 these black holes evolve then in a hierarchical fashion following the merger hierarchy of their host halos accretion of gas traced by quasar activity plays a fundamental role in determining the two parameters defining a black hole mass and spin a particularly intriguing epoch is the initial phase of smbh growth it is very challenging to meet the observational constraints at z6 if bhs are not fed at very high rates in their infancy,http://arxiv.org/abs/astro-ph/0602630v1,0
understanding how seed black holes grow into intermediate and supermassive black holes imbhs and smbhs respectively has important implications for the dutycycle of active galactic nuclei agn galaxy evolution and gravitational wave astronomy most studies of the cosmological growth and merger history of black holes have used semianalytic models and have concentrated on smbh growth in luminous galaxies using high resolution cosmological nbody simulations we track the assembly of black holes over a large range of final masses from seed black holes to smbhs over widely varying dynamical histories we used the dynamics of dark matter halos to track the evolution of seed black holes in three different gas accretion scenarios we have found that growth of sagittarius a size smbh reaches its maximum mass msmbh106msun at z6 through early gaseous accretion episodes after which it stays at near constant mass at the same redshift the dutycycle of the host agn ends hence redshift z6 marks the transition from an agn to a starburst galaxy which eventually becomes the milky way by tracking black hole growth as a function of time and mass we estimate that the imbh merger rate reaches a maximum of rmax55 yr1 at z11 from imbh merger rates we calculate nulx7 per milky way type galaxy per redshift in redshift range 2z6,http://arxiv.org/abs/astro-ph/0703540v1,0
it is argued that supermassive black holes in the nuclei of galaxies most likely have grown coevally with their host dark matter halos a calculation based on pressschechter within this framework shows that the mean rate of accretion of matter onto such black holes varies from a value about 0003 of the eddington rate at the present epoch to a value around 008 at redshift 3 the bulk of agn evolution may be explained as a reflection of the diminishing rate of accretion of material onto galaxies the result is almost independent of mass of host dark matter halo and is only weakly dependent on the values of cosmological parameters at high redshifts z greater than about 5 black holes in galaxies would have been accreting close to the eddington limit which is likely to lead to galactic outflows being ubiquitous at those epochs,http://arxiv.org/abs/astro-ph/0506591v2,0
phase transitions in the early universe are prime settings for the production of primordial black holes since they can break the relatively quiescent homogeneity and isotropy of friedmannrobertsonwalker frw cosmologies these epochs of symmetry breaking moreover can affect the subsequent development of spacetime by changing the evolution of some frw parameters including the present age and density of the universe we discuss the relative importance of such effects on constraining mechanisms of black hole formation,http://arxiv.org/abs/astro-ph/0003062v1,0
we investigate the coupled formation and evolution of galaxies and their embedded supermassive black holes using stateoftheart hydrodynamic simulations of cosmological structure formation for the first time we selfconsistently follow the dark matter dynamics radiative gas cooling star formation as well as black hole growth and associated feedback processes starting directly from initial conditions appropriate for the lambdacdm cosmology our modeling of the black hole physics is based on an approach we have developed in simulations of isolated galaxy mergers here we examine i the predicted global history of black hole mass assembly ii the evolution of the local black holehost mass correlations and iii the conditions that allow rapid growth of the first quasars and the properties of their hosts and descendants today we find a total black hole mass density in good agreement with observational estimates the black hole accretion rate density peaks at lower redshift and evolves more strongly at high redshift than the star formation rate density but the ratio of black hole to stellar mass densities shows only a moderate evolution at low redshifts we find strong correlations between black hole masses and properties of the stellar systems agreeing well with the measured local mbhsigma and mbh m relationships but also suggesting dependent on the mass range a weak evolution with redshift in the normalization and the slope our simulations also produce massive black holes at high redshift due to extended periods of exponential growth in regions that collapse early and exhibit strong gas inflows these first supermassive bh systems however are not necessarily the most massive ones today since they are often overtaken in growth by quasars that form later abridged,http://arxiv.org/abs/0705.2269v1,0
we study a cosmological model in which the boson dark matter gradually condensates into dark energy negative pressure associated with the condensate yields the accelerated expansion of the universe and the rapid collapse of the smallest scale fluctuations into many black holes which become the seeds of the first galaxies the cycle of gradual sedimentation and rapid collapse of condensate repeats many times and selfregularizes the ratio of dark energy and dark matter to be order one,http://arxiv.org/abs/astro-ph/0403571v1,0
the physical basis of the modern cosmological inflationary models with baryosynthesis and nonbaryonic dark matter and energy implies such predictions of particle theory that in turn apply to cosmology for their test it makes physics of early universe ambiguous and particle model dependent the study of modern cosmology is inevitably linked with the probe for the new physics underlying it the particle model dependent phenomena such as unstable dark matter primordial black holes strong primordial inhomogeneities can play important role in revealing the true physical cosmology such phenomena having serious physical grounds and leading to new nontrivial cosmological scenarious should be taken into account in the data analysis of observational cosmology,http://arxiv.org/abs/astro-ph/0309703v1,0
this poster discusses a possible explanation for the relationship between the mass of the central supermassive black hole and the velocity dispersion in the bulge of the host galaxy we suppose that the black hole and the dark matter halo are forming simultaneously as matter falls in and a selfsimilar system then exists in which the mass and the velocities of the system evolve as powerlaw functions of time this leads naturally to a relationship between the black hole mass and the velocities in the halo which with a reasonable choice of cosmological parameters is in good agreement with the observed relationship we also confirm this relationship with more robust numerical results,http://arxiv.org/abs/astro-ph/0212278v2,0
we describe the model of protogalaxy formation around the cluster of primordial black holes with a minimum extension of standard cosmological model namely it is supposed that a mass fraction of the universe 103 is composed of the compact clusters of primordial relict black holes produced during the phase transitions in the early universe these clusters are the centers of the dark matter dm condensations as a result the protogalaxies with a mass 2x108msun form at the redshift z15 these induced protogalaxies contain the central black holes of mass 105msun and look like the dwarf spheroidal galaxies with a central density spike subsequent merging of the induced protogalaxies and ordinary dm haloes leads to the standard scenario of the large scale structure formation black holes merging gives the nowadays supermassive black holes and reproduces the observed correlations between their masses and velocity dispersions in the bulges,http://arxiv.org/abs/astro-ph/0412479v1,0
this work is related to different questions within cosmology the principal idea herein is to develop cosmological knowledge making use of the analyses of observational data in order to find the values of the matter density omegam and vacuum energy density omegalambda data fitting is carried out using two statistical methods chi2 and maximum likelihood the data analysis exhibits that a low density and flat universe is strongly favoured applying the omegam value found for clusters of galaxies we demonstrate that clusters have very little room for baryonic dark matter an upper limit to the small but nonnegligible sum of baryonic dark matter and galaxy mass can be estimated requiring the use of special statistics a toroidal black hole tbh study in contrast to the spherical black hole sbh shows that the tbh can be used as an important tool in explaining agn phenomena,http://arxiv.org/abs/astro-ph/0210253v1,0
we incorporate a simple scheme for the growth of supermassive black holes into semianalytic models that follow the formation and evolution of galaxies in a cold dark matter dominated universe we assume that supermassive black holes are formed and fuelled during major mergers if two galaxies of comparable mass merge their central black holes coalesce and a few percent of the gas in the merger remnant is accreted by the new black hole over a timescale of a few times 107 years with these simple assumptions our model not only fits many aspects of the observed evolution of galaxies but also reproduces quantitatively the observed relation between bulge luminosity and black hole mass in nearby galaxies the strong evolution of the quasar population with redshift and the relation between the luminosities of nearby quasars and those of their host galaxies the strong decline in the number density of quasars from z2 to z0 is due to the combination of three effects i a decrease in the merging rate ii a decrease in the amount of cold gas available to fuel black holes and iii an increase in the timescale for gas accretion in a lcdm cosmology the predicted decline in the total content of cold gas in galaxies is consistent with that inferred from observations of damped lymanalpha systems our results strongly suggest that the evolution of supermassive black holes quasars and starbursts is inextricably linked to the hierarchical buildup of galaxies,http://arxiv.org/abs/astro-ph/9906493v1,0
if the dark matter in galactic halos is made up of compact macroscopic objects mo such as black holes with mmo mstars gravitational scattering will lead to kinematic heating of the stars observational constraints on the amount of heating in the disk of the milky way put upper limits on mmo ltorder 1063msun we find limits that are three orders of magnitude more stringent by examining the heating limits in low mass stellar systems where higher densities of dark matter and lower relative velocities disperse the stars in less than a billion years specifically the structure and kinematics of the local group member gr8 yield a limit of mmo ltorder 6times 103msun if the properties of the dark matter are universal these results preclude the dominance of dark matter constituents in the cosmologically interesting mass range sim 106msun and limit them to mmo ltorder 1037msun these results also rule out massive compact halo objects as significant contributors to the kinematic heating of the galactic disk,http://arxiv.org/abs/astro-ph/9308022v1,0
dark matter in the universe consisting of macroscopic objects such as primordial black holes may cause gravitational lensing of distant objects the magnification associated with lensing will lead to additional scatter in the received flux from standard candles and too small an observed scatter could rule out compact dark matter entirely in this letter we show how the scatter in fluxes of distant type 1a supernovae could be used to distinguish between models with and without lensing by macroscopic dark matter the proposed snap project with sim 2400 supernovae in the range 01ls zls 17 should be able to identify models at 999 confidence if systematic errors are controlled note that this test is independent of any evolution of the mean supernova luminosity with redshift the variances of the current supernova cosmology project sample do not rule out compact lenses as dark matter formally they favour such a population but the significance is low and removal of a single faint supernova from the sample reverses the conclusion,http://arxiv.org/abs/astro-ph/0104221v2,0
the possibility that the masses of supermassive black holes sbhs correlate with the total gravitational mass of their host galaxy or the mass of the dark matter halo in which they presumably formed is investigated using a sample of 16 spiral and 20 elliptical galaxies the bulge velocity dispersion typically defined within an aperture of size less than 05 kpc is found to correlate tightly with the galaxys circular velocity the latter measured at distances from the galactic center at which the rotation curve is flat 20 to 80kpc by using the well known msigma relation for sbhs and a prescription to relate the circular velocity to the mass of the dark matter halo in a standard cdm cosmology the correlation between velocity dispersion and circular velocity is equivalent to one between sbh and halo masses such a correlation is found to be nonlinear with the ratio between the two masses decreasing from 2x104 for halos of 1014 solar masses to 105 for halos of 1012 solar masses preliminary evidence suggests that halos smaller than 5x1011 solar masses are increasingly less efficient perhaps unable at forming sbhs,http://arxiv.org/abs/astro-ph/0203469v2,0
the astrophysical processes that led to the formation of the first seed black holes and to their growth into the supermassive variety that powers bright quasars at redshift 6 are poorly understood in standard lcdm hierarchical cosmologies the earliest massive holes mbhs likely formed at redshift z15 at the centers of lowmass m5e5 solar masses dark matter minihalos and produced hard radiation by accretion fuvxray photons from such miniquasars may have permeated the universe more uniformly than euv radiation reduced gas clumping and changed the chemistry of primordial gas the role of accreting seed black holes in determining the thermal and ionization state of the intergalactic medium depends on the amount of cold and dense gas that forms and gets retained in protogalaxies after the formation of the first stars the highest resolution nbody simulation to date of galactic substructure shows that subhalos below the atomic cooling mass were very inefficient at forming stars,http://arxiv.org/abs/astro-ph/0701394v1,0
we show that the luminosity function of the actively starforming lyman break galaxies and the bband quasar luminosity function at z 3 can be fit reasonably well with the mass function of collapsed galaxy scale dark matter haloes predicted by viable variants of hierarchical cold dark matter dominated cosmological models for lifetimes tq of the optically bright phase of qsos in the range 106 to 108 yr there is a strong correlation between tq and the required degree of nonlinearity in the relation between black hole and host halo mass such a nonlinear relation is motivated by suggesting that the mass of supermassive black holes may be limited by the backreaction of the emitted energy on the accretion flow in a selfgravitating disc this would imply a relation of black hole to halo mass of the form mrm bh propto vrm halo5 propto mrm halo53 and a typical duration of the optically bright qso phase of the order of the salpeter time sim 107 yr the high integrated local mass density of black holes inferred from recent kinematic determinations of black hole masses in nearby galaxies seem to indicate that the overall efficiency of supermassive black holes for producing blue light is lower than was previously assumed we discuss three possible accretion modes with low optical emission efficiency i accretion well above the eddington rate ii accretion obscured by dust and iii accretion below the critical rate leading to an advection dominated accretion flow lasting for a hubble time we further argue that accretion with low optical efficiency might be closely related to the origin of the hard xray background,http://arxiv.org/abs/astro-ph/9809250v1,0
abridged the observed properties of supermassive black holes suggest a fundamental link between their assembly and the formation of their host spheroids we model the growth and activity of black holes in galaxies using lambdacdm cosmological hydrodynamic simulations by following the evolution of the baryonic mass component in galaxy potential wells we find that the observed steep relation between black hole mass and spheroid velocity dispersion mbh propto sigma4 is reproduced if the gas mass in bulges is linearly proportional to the black hole mass in this model black hole growth saturates because of the competition with starformation and feedback both of which determine the gas fraction available for accretion unless other processes also operate we predict that the mbhsigma relation is not set in primordial structures but is fully established at low redshifts z approxlt 2 and is shallower at earlier times we find that that central black hole masses are related to their dark matter halos simply via mbh mdm43 we assume that galaxies undergo a quasar phase with a typical lifetime tq 2times 107 yr and show that starformation regulated depletion of gas in spheroids can explain the decrease of the quasar population at redshift z3 in the optical blue band however with the simplest assumption of a redshift independent quasar lifetime the model overpredicts optical quasar numbers at high redshifts although it yields the observed evolution of number density of xray quasars over the redshift range 1 z 6 finally we find that the majority of black hole mass is assembled in galaxies by z 3 and that the black hole accretion rate density peaks in rough correspondence to the star formation rate density at z 45,http://arxiv.org/abs/astro-ph/0301586v2,0
recently it has been suggested that the majority of dark matter in the universe resides in the form of jupiter mass black holes distributed cosmologically this population makes itself apparent by microlensing high redshift quasars and introducing pronounced variability into their observed light curves while several arguments dismissing this hypothesis have been presented a conclusive observational test is alas sadly lacking in this paper we investigate the effect of a cosmologically distributed population of microlensing masses on galaxies at low to intermediate redshift the magnification of bright stars in these galaxies leads to small but observable fluctuations in their surface brightness the variability time scale for jupitermass lensing objects is of the order of a few months and this population may be detected through a future spacebased monitoring campaign of a field containing z sim 05 galaxies the monitoring of galactic surface brightness will provide an effective test of the nature of dark matter on cosmological scales,http://arxiv.org/abs/astro-ph/0004037v1,0
progress in observational cosmology over the past five years has established that the universe is dominated dynamically by dark matter and dark energy both these new and apparently independent forms of matterenergy have properties that are inconsistent with anything in the existing standard model of particle physics and it appears that the latter must be extended we review what is known about dark matter and energy from their impact on the light of the night sky most of the candidates that have been proposed so far are not perfectly black but decay into or otherwise interact with photons in characteristic ways that can be accurately modelled and compared with observational data we show how experimental limits on the intensity of cosmic background radiation in the microwave infrared optical ultraviolet xray and gammaray bands put strong limits on decaying vacuum energy light axions neutrinos unstable weaklyinteracting massive particles wimps and objects like black holes our conclusion is that the dark matter is most likely to be wimps if conventional cosmology holds or higherdimensional sources if spacetime needs to be extended,http://arxiv.org/abs/astro-ph/0407207v1,0
ultralight scalar fields provide an interesting alternative to wimps as halo dark matter in this paper we consider the effect of embedding a supermassive black hole within such a halo and estimate the absorption probability and the accretion rate of dark matter onto the black hole we show that the accretion rate would be small over the lifetime of a typical halo and hence that supermassive central black holes can coexist with scalar field halos,http://arxiv.org/abs/astro-ph/0207493v2,0
we use semianalytic modeling on top of the millennium simulation to study the joint formation of galaxies and their embedded supermassive black holes our goal is to test scenarios in which black hole accretion and quasar activity are triggered by galaxy mergers and to constrain different models for the lightcurves associated with individual quasar events in the present work we focus on studying the spatial distribution of simulated quasars at all luminosities we find that the simulated quasar twopoint correlation function is fit well by a single powerlaw in the range 05 r 20 h1 mpc but its normalization is a strong function of redshift when we select only quasars with luminosities within the range typically accessible by todays quasar surveys their clustering strength depends only weakly on luminosity in agreement with observations this holds independently of the assumed lightcurve model since bright quasars are black holes accreting close to the eddington limit and are hosted by dark matter haloes with a narrow mass range of a few 1012 h1 msun therefore the clustering of bright quasars cannot be used to disentangle lightcurve models but such a discrimination would become possible if the observational samples can be pushed to significantly fainter limits overall our clustering results for the simulated quasar population agree rather well with observations lending support to the conjecture that galaxy mergers could be the main physical process responsible for triggering black hole accretion and quasar activity,http://arxiv.org/abs/0812.0003v2,0
aims we investigate the cosmological growth of dark halos and follow the consequences of coeval growth for the accretion history of associated supermassive black holes methods the pressschechter approximation is used to obtain an analytic expression for the mean rate of growth of dark matter halos dark halo accretion rates are compared with numerical work and the consequences for understanding agn evolution are described results the mean accretion rate onto dark matter halos is shown to have a simple analytic form that agrees with previous numerical work and that may easily be calculated for a wide range of halo mass redshift and cosmological parameters the result offers a significant improvement over published fitting formulae deduced from merger trees we then consider the growth of associated supermassive black holes and make a basic test of the simple hypothesis of pure coeval evolution pce in which on average black hole growth tracks dark halo growth we demonstrate that both the absolute value of the integrated agn bolometric luminosity density and its cosmological evolution derived from hard xray surveys are wellreproduced by pce excellent agreement is found at z 05 although the observed luminosity density drops by a factor 2 compared with pce by z0 black hole growth appears to decouple from halo growth at low redshifts and this may be related to the phenomenon of cosmic downsizing overall agn evolution appears either to be caused by or to be closely linked to the slowdown in the growth of cosmic structure we also discuss the mean eddington ratio averaged over all galaxies which is predicted to show strong evolution to higher values with redshift,http://arxiv.org/abs/astro-ph/0608202v2,0
massive black hole binaries with masses in the range 1e31e8 msun are expected to be the most powerful sources of gravitational radiation at mhz frequencies and hence are among the primary targets for the planned laser interferometer space antenna lisa we extend and refine our previous analysis sesana et al 2004 detailing the gravitational wave signal expected from a cosmological population of massive black hole binaries as done in our previous paper we follow the merger history of dark matter halos the dynamics of the massive black holes they host and their growth via gas accretion and binary coalescences in a lcdm cosmology stellar dynamical processes dominates the orbital evolution of black hole binaries at large separations while gravitational wave emission takes over at small radii causing the final coalescence of the pairs we show that the gw signal from this population in a 3 year lisa observation will be resolved into approx 90 discrete events with sn5 among which approx 35 will be observed above threshold until coalescence these merging events involve relatively massive binaries m10e5 msun in the redshift range 2z6 the remaining approx 55 events come from higher redshift less massive binaries m5e3 msun at z6 and although their sn integrated over the duration of the observation can be substantial the final coalescence phase is at too high frequency to be directly observable by spacebased interferometers such as lisa lisa will be able to detect a fraction approx 90 of all the coalescences of massive black hole binaries occurring at z5 the residual confusion noise from unresolved massive black hole binaries is expected to be at least an order of magnitude below the estimated stochastic noise,http://arxiv.org/abs/astro-ph/0409255v2,0
a new scenario for the emission of highenergy gammarays from dark matter annihilation around massive black holes is presented a black hole can leave its parent halo by means of gravitational radiation recoil in a merger event or in the asymmetric collapse of its progenitor star a recoiled black hole which moves on an almostradial orbit outside the virial radius of its central halo in the cold dark matter background reaches its apapsis in a finite time near or at the apapsis passage a highdensity wake extending over a large radius of influence forms around the black hole it is shown that significant gammaray emission can result from the enhancement of neutralino annihilation in these wakes at its apapsis passage a black hole is shown to produce a flash of highenergy gammarays whose duration is determined by the mass of the black hole and the redshift at which it is ejected the ensemble of such black holes in the hubble volume is shown to produce a diffuse highenergy gammaray background whose magnitude is compared to the diffuse emission from dark matter haloes alone,http://arxiv.org/abs/0709.3321v4,0
if massive black holes constitute the dark matter in the halo surrounding the milky way the existence of low mass globular clusters in the halo suggests an upper limit to their mass mbh we use a combination of the impulse approximation and numerical simulations to constrain mbh lsim 103modot otherwise several of the halo globular clusters would be heated to disruption within one half of their lifetime taken at face value this constraint is three orders of magnitude stronger than the previous limit provided by disk heating arguments however since the initial mass function of clusters is unknown we argue that the real constraint is at most an order of magnitude weaker our results rule out cosmological scenarios such as versions of the primordial baryonic isocurvature fluctuation model which invoke the low jeans mass at early epochs to create a large population of black holes of mass sim 106modot,http://arxiv.org/abs/astro-ph/9306004v1,0
i review scenarios for the assembly of supermassive black holes mbhs at the center of galaxies that trace their hierarchical buildup far up in the dark matter halo merger tree monte carlo realizations of the merger hierarchy in a lcdm cosmology coupled to semianalytical recipes are a powerful tool to follow the merger history of halos and the dynamics and growth of the mbhs they host xray photons from miniquasars powered by intermediatemass seed holes may permeate the universe more uniformly than euv radiation make the lowdensity diffuse intergalactic medium warm and weakly ionized prior to the epoch of reionization breakthrough and set an entropy floor the spin distribution of mbhs is determined by gas accretion and is predicted to be heavily skewed towards fastrotating kerr holes to be already in place at early epochs and not to change significantly below redshift 5 decaying mbh binaries may shape the innermost central regions of galaxies and should be detected in significant numbers by lisa,http://arxiv.org/abs/astro-ph/0410526v1,0
we consider the possibility that massive primordial black holes are the dominant form of dark matter black hole formation generates entropy fluctuations that adds a poisson noise to the matter power spectrum we use lymanalpha forest observations to constrain this poisson term in matter power spectrum then we constrain the mass of black holes to be less than few times 104 solar mass we also find that structures with less than 103 primordial black holes evaporate by now,http://arxiv.org/abs/astro-ph/0302035v1,0
since the original baryonic dark matter detection from quasar microlensing was first announced in 1996 substantial strides have been made in confirming the rapid microlensing signature in the q0957 system and in other gravitational lens systems the most rapid event recognized had a 1 amplitude and a 12hour duration interpretation of the rapid fluctuations has centered upon 3 offered explanations microlensing of fine quasar structure by a population of planet mass astronomical bodies in the lens galaxy orbiting bright structures in the accretion disc of the supermassive black hole of the quasar or dark clouds swarming around the luminous quasar source the observations particularly the equal positive and negative fluctuations seem to strongly favor the cosmological population of planetary mass objects in the lens galaxy of the several ideas advanced for the origin of such a population the most appealing seems to be their birth at the time of recombination 300000 years after the big bang,http://arxiv.org/abs/astro-ph/0406491v1,0
we study the impact of astrophysical processes on the gammaray background produced by the annihilation of dark matter particles in cosmological halos with particular attention to the consequences of the formation of supermassive black holes in scenarios where these objects form adiabatically from the accretion of matter on small seeds dark matter is first compressed into very dense spikes then its density progressively decreases due to annihilations and scattering off of stellar cusps with respect to previous analyses based on nonevolving halos the predicted annihilation signal is higher and significantly distorted at low energies reflecting the large contribution to the total flux from unevolved spikes at high redshifts the peculiar spectral feature arising from the specific redshift distribution of the signal would discriminate the proposed scenario from more conventional astrophysical explanations we discuss how this affects the prospects for detection and demonstrate that the gammaray background from dm annihilations might be detectable even in absence of a signal from the galactic center,http://arxiv.org/abs/astro-ph/0703236v2,0
minor mergers of galaxies are expected to be common in a hierarchical cosmology such as lambdacdm and have the potential to significantly affect galactic structure in this paper we dissect the casebycase outcome from a set of numerical simulations of a single satellite elliptical galaxy accreting onto a massive elliptical galaxy we take care to explore cosmologically relevant orbital parameters and to set up realistic initial galaxy models that include all three relevant dynamical components dark matter halos stellar bulges and central massive black holes the effects of several different parameters are considered including orbital energy and angular momentum satellite density and inner density profile satellitetohost mass ratio and presence of a black hole at the center of the host black holes play a crucial role in protecting the shallow stellar cores of the hosts as satellites merging onto a host with a central black hole are more strongly disrupted than those merging onto hosts without black holes orbital parameters play an important role in determining the degree of disruption satellites on less bound or more eccentric orbits are more easily destroyed than those on more bound or more circular orbits as a result of an increased number of pericentric passages and greater cumulative effects of gravitational shocking and tidal stripping in addition satellites with densities typical of faint elliptical galaxies are disrupted relatively easily while denser satellites can survive much better in the tidal field of the host over the range of parameters explored we find that the accretion of a single satellite elliptical galaxy can result in a broad variety of changes in both signs in the surface brightness profile and color of the central part of an elliptical galaxy,http://arxiv.org/abs/astro-ph/0608122v2,0
the evolution of the dark matter distribution at the galactic center is analyzed which is caused by the combination of gravitational scattering on galactic bulge stars and absorption by a supermassive black hole at the center of the bulge attention is focused on the boundary condition on the black hole it is shown that its form depends on the energy of dark matter particles the modified flux of dark matter particles onto the black hole is calculated estimates of the amount of dark matter absorbed show that the fraction of dark matter in the total mass of the black hole may be significant the density of dark matter at the central part of the bulge is calculated it is shown that recently observed gamma radiation from the galactic center can be attributed to the annihilation of dark matter with this density,http://arxiv.org/abs/astro-ph/0503712v1,0
we show that seeds for quasar black holes could have originated from the initial cosmological collapse of overdense regions with unusually small rotation the gas in these rare regions collapses into a compact disk that shrinks on a short viscous time scale using an analytical model we calculate the lowspin tail of the probability distribution of angular momenta for objects that collapse out of a gaussian random field of initial density perturbations the population of lowspin systems is significant for any viable power spectrum of primordial density perturbations most objects form just above the cosmological jeans mass sim 105 msun at high redshifts z10 in the standard cold dark matter cosmology the comoving density of 1067 msun objects with viscous evolution times shorter than 1067 years is about 103 h053 mpc3 comparable to the local density of bright galaxies the seed black holes tend to reside within larger mass systems that collapse later and supply the gas needed for the bright quasar activity,http://arxiv.org/abs/astro-ph/9401016v2,0
quasars are the earliest objects known to exist we examine their origin in the context of popular models for structure formation in the universe we show that seeds for quasar black holes could have originated from the initial cosmological collapse of overdense regions with unusually small rotation most of these seeds have a mass of order 106 solar masses just above the cosmological jeans mass for cold dark matter cosmologies we find of order one seed black hole per bright galaxy after the galaxy forms the seeds inside its bulge sink to the center by dynamical friction we also describe a few empirical methods to study the properties of quasars and their environments these include probing quasar hosts through c ii emission finding quasar lifetimes from the proximity effect along two lines of sight and measuring the amplitude of clustering at high redshifts through the detection of lymanalpha clouds beyond the quasar redshift,http://arxiv.org/abs/astro-ph/9501009v1,0
for two decades the hot bigbang model as been referred to as the standard cosmology and for good reason for just as long cosmologists have known that there are fundamental questions that are not answered by the standard cosmology and point to a grander theory the best candidate for that grander theory is inflation cold dark matter it can extend our understanding of the universe back to 1032 sec there is now prima facie evidence for the two basic tenets of this new paradigm flat universe and scaleinvariant spectrum of gaussian density perturbations and an avalanche of telling cosmological observations is coming if inflation cold dark matter is correct then there are new fundamental questions to be answered most notably the nature of the dark energy that seems to account for 60 of the critical density and how inflation fits into a unified theory of the forces and particles these are exciting times in cosmology,http://arxiv.org/abs/astro-ph/9811447v1,0
abridged we use high resolution cosmological nbody simulations to study the growth of intermediate to supermassive black holes from redshift 49 to zero we track the growth of black holes from the seeds of population iii stars to black holes in the range of 103 m 107 msun not quasars but rather imbh to lowmass smbhs these lower mass black holes are the primary observable for the laser interferometer space antenna lisa the largescale dynamics of the black holes are followed accurately within the simulation down to scales of 1 kpc thereafter we follow the merger analytically from the last dynamical friction phase to black hole coalescence we find that the merger rate of these black holes is r25 per year between 8 z 11 and r 10 per year at z3 before the merger occurs the incoming imbh may be observed with a next generation of xray telescopes as a ulx source with a rate of about 3 7 per year for 1 z 5 we develop an analytic prescription that captures the most important black hole growth mechanisms galaxy mergerdriven gas accretion and black hole coalescence using this we find that smbh at the center of milky way type galaxy was in place with most of its mass by z 47 and most of the growth was driven by gas accretion excited by major mergers hundreds of black holes have failed to coalesce with the smbh by z0 some with masses of 10000 msun orbiting within the dark matter halo with luminosities up to 30000 lsun these xray sources can easily be observed with chandra at 100 kpc,http://arxiv.org/abs/0805.3154v1,0
cosmological nucleosynthesis calculations imply that many of the baryons in the universe must be dark we discuss the likelihood that some of these dark baryons may reside in the discs or halos of galaxies if they were in the form of compact objects they would then be natural macho candidates in which case they are likely to be the remnants of a first generation of pregalactic or protogalactic population iii stars various candidates have been proposed for such remnants brown dwarfs red dwarfs white dwarfs neutron stars or black holes and we review the many types of observations including microlensing searches which can be used to constrain or exclude them,http://arxiv.org/abs/astro-ph/0008028v1,0
this paper presents a study of quasisteady spherical accretion in the early universe before the formation of the first stars and galaxies the main motivation is to derive the basic formulas that will be used in a companion paper to calculate the accretion luminosity of primordial black holes and their effect on the cosmic ionization history the following cosmological effects are investigated the coupling of the gas to the cmb photon fluid ie compton drag hubble expansion and the growth of the dark matter halo seeded by the gravitational potential of the central point mass the gas equations of motion are solved assuming either a polytropic or an isothermal equation of state we consider the cases in which the accreting object is a point mass or a spherical dark matter halo with powerlaw density profile as predicted by the theory of secondary infall analytical solutions for the sonic radius and fitting formulas for the accretion rate are provided different accretion regimes exist depending on the mass of the accreting object if the black hole mass is smaller than 50100 msun gas accretion is unaffected by compton drag a point mass and an extended dark halo of equal mass accrete at the same rate if m5000 msun while smaller mass dark halos accrete less efficiently than the equivalent point mass for masses m3 x 104 msun the viscous term due to the hubble expansion becomes important and the assumption of quasisteady flow fails hence the steady bondi solutions transition to the timedependent selfsimilar solutions for cold cosmological infall,http://arxiv.org/abs/0706.0864v1,0
the evolution of nonlinear density fluctuations around the jeans mass shortly after cosmological recombination is analyzed using a 3d hydrodynamicsdarkmatter code the cosmic background radiation cbr exerts compton friction on free electrons due to peculiar velocities the dynamics therefore depends strongly on the gas ionization history under a variety of ionization conditions and in systems with or without nonbaryonic components the baryons lose angular momentum efficiently and collapse to form a compact opticallythick object which would probably quickly evolve into a massive black hole attention is concentrated on elucidating some of the novel physical effects in early cosmological collapses but ways in which more realistic calculations might be made and in which the scenario could be incorporated into a more complete cosmogonic model are discussed,http://arxiv.org/abs/astro-ph/9303004v1,0
we do not know 96 of the total matter in the universe at present in this paper a cosmological model is proposed in which dark energy de is identified as boseeinstein condensation bec of some boson field global cosmic acceleration caused by this bec and multiple rapid collapses of bec into black holes etc dark matter dm are examined based on the relativistic version of the grosspitaevskii equation we propose a a novel mechanism of inflation free from the slowrolling condition b a natural solution for the cosmic coincidence why now problem through the transition from de into dm c very early formation of highly nonlinear objects such as black holes which might trigger the first light as a form of quasars and d logz periodicity in the subsequent bec collapsing time all of these are based on the steady slow bec process,http://arxiv.org/abs/astro-ph/0509789v2,0
we reconsider the possibility that the observed baryon asymmetry was generated by the evaporation of primordial black holes that dominated the early universe we present a simple derivation showing that the baryon asymmetry is insensitive to the initial black hole density and the cosmological model but is sensitive to the temperaturedependence of the cp and baryonviolating or leptonviolating interactions we also consider the possibility that black holes stop evaporating and form planckmass remnants that act as dark matter we show that primordial black holes cannot simultaneously account for both the observed baryon asymmetry and the remnant dark matter density unless the magnitude of cp violation is much greater than expected from most particle physics models finally we apply these results to ekpyroticcyclic models in which primordial black holes may form when branes collide we find that obtaining the observed baryon asymmetry is compatible with the other known constraints on parameters,http://arxiv.org/abs/hep-th/0703250v1,0
we investigate a hierarchical structure formation scenario in which galaxy stellar cores are created from the binding energy liberated by shrinking supermassive black hole smbh binaries the binary orbital decay heats the surrounding stars eroding a preexisting 1r2 stellar cusp we follow the merger history of dark matter halos and associated smbhs via cosmological monte carlo realizations of the merger hierarchy from early times to the present in a lcdm cosmology massive black holes get incorporated through a series of mergers into larger and larger halos sink to the center owing to dynamical friction accrete a fraction of the gas in the merger remnant to become supermassive and form a binary system stellar dynamical processes drive the binary to harden and eventually coalesce a simple scheme is applied in which the loss cone is constantly refilled and a constant density core forms due to the ejection of stellar mass we find that a model in which the effect of the hierarchy of smbh interactions is cumulative and cores are preserved during galaxy mergers produces at the present epoch a correlation between the mass deficit the mass needed to bring a flat inner density profile to a 1r2 cusp and the mass of the nuclear smbh with a normalization and slope comparable to the observed relation models in which the mass displaced by the smbh binary is replenished after every major galaxy merger appear instead to underestimate the mass deficit observed in core galaxies,http://arxiv.org/abs/astro-ph/0304389v2,0
it is suggested that dark matter in the universe is made of stars and black holes of wimp matter,http://arxiv.org/abs/astro-ph/0204375v1,0
we investigate the effects of a topheavy stellar initial mass function on the reionisation history of the intergalactic medium igm we use cosmological simulations that include selfconsistently the feedback from ionising radiation h2 dissociating radiation and supernova sn explosions we find that it is difficult to reionise the igm at zrei10 with stellar sources even after making extreme assumptions if star formation in 109 modot galaxies is not suppressed by sn explosions the optical depth to thomson scattering is taue 013 if we allow for the normal energy input from sne or if pairinstability sne are dominant we find taue009 assuming normal yields for the first stars popiii the mean metallicity of the igm is already zzodot2x103 103zzodot1 in overdense regions when the igm mean ionisation fraction is less than 10 for these reasons popiii stars cannot contribute significantly to reionisation unless the mechanical energy input from sne is greatly reduced and either the metal yield or the mixing efficiency is reduced by a factor of 1000 both problems have a solution if popiii stars collapse to black holes this can happen if having masses m 130 modot they are characterised by heavy element fallback or if having masses m260 modot they collapse directly onto black holes without exploding as sneif metalpoor stars are initially important and collapse to black holes is the typical outcome then the secondary emission of ionising radiation from accretion on sn induced seed black holes might be more important than the primary emission surprisingly including feedback effects we estimate that a warm dark matter scenario with particle mass of 125 kev reduces taue by only approximately 10abridged,http://arxiv.org/abs/astro-ph/0310331v3,0
this paper reviews the field of gammaray astronomy and describes future experiments and prospects for advances in fundamental physics and highenergy astrophysics through gammaray measurements we concentrate on recent progress in the understanding of active galaxies and the use of these sources as probes of intergalactic space we also describe prospects for future experiments in a number of areas of fundamental physics including searches for an annihilation line from neutralino dark matter understanding the energetics of supermassive black holes using agns as cosmological probes of the primordial radiation fields constraints on quantum gravity detection of a new spectral component from grbs and the prospects for detecting primordial black holes,http://arxiv.org/abs/astro-ph/0201160v1,0
calculations of the rate of local primordial black hole explosions often assume that the pbhs can be highly concentrated into galaxies thereby weakening the pagehawking limit on the cosmological density of pbhs but if the pbhs are concentrated by a factor exceeding chcirc rcirc approx 4 times 105 where rcirc 85 kpc is the scale of the milky way then the steady emission from the pbhs in the halo will produce an anisotropic high latitude diffuse gamma ray intensity larger than the observed anisotropy this provides a limit on the ratedensity of evaporating pbhs of lesssim 04pc3yr1 which is more than 6 orders of magnitude lower than recent experimental limits however the weak observed anisotropic high latitude diffuse gamma ray intensity is consistent with the idea that the dark matter that closes the universe is planck mass remnants of evaporated black holes,http://arxiv.org/abs/astro-ph/9509074v1,0
recent observations have shown that only a billion years after the big bang the universe was already lit up by bright quasars fuelled by the infall of gas onto supermassive black holes the masses of these early black holes are inferred from their luminosities to be 109 solar masses which is a difficult theoretical challenge to explain like nearby quasars the early objects could have formed in the central cores of massive host galaxies the formation of these hosts could be explained if like local large galaxies they were assembled gravitationally inside massive 1012 solar mass halos of dark matter there has hitherto been no observational evidence for the presence of these massive hosts or their surrounding halos here we show that the cosmic gas surrounding each halo must respond to its strong gravitational pull where absorption by the infalling hydrogen produces a distinct spectral signature that signature can be seen in recent data,http://arxiv.org/abs/astro-ph/0209515v4,0
in cosmological models favored by current observations the first astrophysical objects formed in dark matter halos at redshifts starting at z20 and their properties were determined by primordial h2 molecular chemistry these protogalaxies were very abundant but substantially less massive than typical galaxies in the local universe extreme metalpoor stars and massive black holes in their nuclei reionized the bulk of the hydrogen in the intergalactic medium reionization may have taken place over an extended redshift interval ending around z7 observational probes of the process of reionization are afforded by studying the polarization of the cosmic microwave background anisotropies as well as by studying the spectra and abundance of distant lyman alphaemitting galaxies here we review theoretical expectations on how and when the first galaxies formed and summarize future observational prospects of probing hydrogen reionization,http://arxiv.org/abs/astro-ph/0304131v1,0
the rate of accretion of matter from a solartype star onto a primordial black hole pbh that passes through it is calculated the probability that a pbh is captured into an orbit around a star in a galaxy is found the mean lifetime of the pbh in such an orbit and the rate of orbital captures of pbhs in the galaxy are calculated it is shown that this rate does not depend on the mass of the pbh this mechanism cannot make an appreciable contribution to the rate of observed gammaray bursts the density of pbhs in the galaxy can reach a critical value the density of the mass of dark matter in the galaxy,http://arxiv.org/abs/0710.5275v1,0
we show that a spherical electroweak domain wall is formed around a small black hole and this is a general property of the hawking radiation in the vacuum of the standard model the wall appears not only for the first order phase transition in the electroweak theory but also for the second order one because the black hole heats up its neighborhood locally by the hawking radiation in any case we propose a model for unifying the origin of the baryon number and the cold dark matter in our universe by using properties of the primordial black hole with a mass of several hundred kilograms the interaction between our wall and the hawkingradiatedparticles can create a baryon number which is proportional to the mass of the black hole as well as the cp broken phase in the extension of the standard model our model can explain both the baryonentropy ratio bs sim 1010 and the energy density of the dark matter provided that the following three conditions are satisfied i the primordial black holes dominate in the early universe ii the cp broken phase in the wall is in the order of one and iii any black hole leaves a stable remnant with a planck mass after its evaporation our model also predicts a cosmological graviton background with a peakenergy 120 sim 280 ev in the present universe,http://arxiv.org/abs/hep-ph/0104160v1,0
we present some simple models to determine whether or not the accretion of cold dark matter by supermassive black holes is astrophysically important contrary to some claims in the literature we show that supermassive black holes cannot significantly alter a power law density cusp via accretion whether during mergers or in the steady state,http://arxiv.org/abs/astro-ph/0210658v1,0
we review the consequences of the growth and evolution of black holes on the distribution of stars and dark matter dm around them we focus in particular on supermassive and intermediate mass black holes and discuss under what circumstances they can lead to significant overdensities in the surrounding distribution of dm thus effectively acting as dm annihilation boosters,http://arxiv.org/abs/0711.3148v1,0
we assess models for the assembly of supermassive black holes smbhs at the center of galaxies that trace their hierarchical buildup far up in the dark halo merger tree we assume that the first seed black holes bhs formed in minihalos collapsing at z20 from highsigma density fluctuations as these pregalactic holes become incorporated through a series of mergers into larger and larger halos they sink to the center owing to dynamical friction accrete a fraction of the gas in the merger remnant to become supermassive form a binary system and eventually coalesce the merger history of dark matter halos and associated bhs is followed by cosmological monte carlo realizations of the merger hierarchy a simple model where quasar activity is driven by major mergers and smbhs accrete at the eddington rate a mass that scales with the fifth power of the velocity dispersion is shown to reproduce the optical lf of quasars in the redshift range 1z4 binary and triple bh interactions are followed in our merger tree the assumptions underlying our scenario lead to the prediction of a population of massive bhs wandering in galaxy halos and the intergalactic medium at the present epoch and contributing 10 to the total bh mass density at all epochs the fraction of binary smbhs in galaxy nuclei is of order 10 while the fraction of binary quasars is less than 03,http://arxiv.org/abs/astro-ph/0207276v3,0
results from string theory conclude that a spatial dimension r is equivalent to r if r is considered as a fourspace dimension several interesting results emerge in this paradigm r exists in a timereversed anti parallel universe cosmological distances in our realtime universe are equivalent to the compact dimensions at the instant of a big bang in a time inverted universe this model agrees with standard black hole theory in that particles become frozen in time when they reach the singularity of dimension alpha a particle which enters a black hole in real time exits from the big bang or white hole in the timeinverted universe leaving behind a trace of annihilation radiation a diagram of this phenomenon is constructed consistent with existing knowledge of the early universe the primordeal black holes predicted by hawking are required a cyclic universe is described by m2pn t2n 1 the missing mass proscribed by standard closed universe theories is not required this result is useful if experimental evidence in the search for dark matter continues to leave a significant missing mass and particularly in light of more recent observations indicating that in fact inflation continues at a much reduced rate,http://arxiv.org/abs/astro-ph/0007100v1,0
anisotropic emission of gravitational waves during the merger or formation of black holes can lead to the ejection of these black holes from their host galaxies a recoiled black hole which moves on an almost radial bound orbit outside the virial radius of its central galaxy in the cold dark matter background reaches its apapsis in a finite time the low value of dark matter velocity dispersion at high redshifts and also the black hole velocity near the apapsis passage yield a highdensity wake around these black holes gammaray emission can result from the enhancement of dark matter annihilation in these wakes the diffuse highenergy gammaray background from the ensemble of such black holes in the hubble volume is also evaluated,http://arxiv.org/abs/0802.2534v1,0
how and when did the first generation of stars form at the end of the cosmic dark ages quite generically within variants of the cold dark matter model of cosmological structure formation the first sources of light are expected to form in 106 msun dark matter potential wells at redshifts z 20 i discuss the physical processes that govern the formation of the first stars these socalled populationiii stars are predicted to be predominantly very massive and to have contributed significantly to the early reionization of the intergalactic medium such an early reionization epoch is inferred from the recent measurement of the thomson optical depth by the it wmap satellite i address the importance of heavy elements in bringing about the transition from an early star formation mode dominated by massive stars to the familiar mode dominated by low mass stars at later times and present possible observational probes this transition could have been gradual giving rise to an intermediatemass population of still virtually metalfree stars population ii5 these stars could have given rise to the peculiar class of blackhole forming supernovae inferred from the abundance pattern of extremely ironpoor stars,http://arxiv.org/abs/astro-ph/0509354v1,0
in figure 6 we inadvertently labeled the proxy circular velocity as the virial velocity of the dark matter halo instead of what is actually plotted the maximum circular velocity of the dark matter halo the maximum halo circular velocity is a much better estimate of the disk vc than is vvir this confusion influenced the discussion of the tullyfisher relation in our paper in fact figure 6 demonstrates that it is possible to simultaneously reproduce both the local tullyfisher relation and luminosity function using semianalytic techniques applied to the standard lcdm cosmology thus contradicting previous studies of this issue and our own discussion in section 36,http://arxiv.org/abs/astro-ph/0602065v1,0
galaxies are complex systems the evolution of which apparently results from the interplay of dynamics star formation chemical enrichment and feedback from supernova explosions and supermassive black holes the hierarchical theory of galaxy formation holds that galaxies are assembled from smaller pieces through numerous mergers of cold dark matter the properties of an individual galaxy should be controlled by six independent parameters including mass angularmomentum baryonfraction age and size as well as by the accidents of its recent haphazard merger history here we report that a sample of galaxies that were first detected through their neutral hydrogen radiofrequency emission and are thus free of optical selection effects shows five independent correlations among six independent observables despite having a wide range of properties this implies that the structure of these galaxies must be controlled by a single parameter although we cannot identify this parameter from our dataset such a degree of organisation appears to be at odds with hierarchical galaxy formation a central tenet of the cold dark matter paradigm in cosmology,http://arxiv.org/abs/0811.1554v1,0
there have been proposals that primordial black hole remnants bhrs are the dark matter but the idea is somewhat vague recently we argued that the generalized uncertainty principle gup may prevent black holes from evaporating completely in a similar way that the standard uncertainty principle prevents the hydrogen atom from collapsing we further noted that the hybrid inflation model provides a plausible mechanism for production of large numbers of small black holes combining these we suggested that the dark matter might be composed of plancksize bhrs in this paper we briefly review these arguments and discuss the reheating temperature as a result of black hole evaporation,http://arxiv.org/abs/astro-ph/0305025v1,0
the connection between dark matter and giant black holes in the galactic nuclei is investigated the joint evolution of dark and luminous matter in averaged selfconsistent gravitational potential is considered it is shown that the distribution of dark matter remains spherically symmetric even in presence of essential asymmetry of luminous matter in the galaxy the kinetic equation describing evolution of dark matter particles distribution function which takes into account their dynamics in gravitational potential and scattering on stars is derived it is shown that significant flux of dark matter on a seed black hole in the center of galaxy should appear the growth law of seed black hole due to absorption of dark matter is derived it appears that during the lifetime of galaxy the seed black hole should grow significantly up to observed value brief analysis of observational data shows that the presented theory is in reasonable agreement with observations,http://arxiv.org/abs/astro-ph/0306490v2,0
in this paper we consider dark matter particle annihilation in the gravitational field of black holes we obtain exact distribution function of the infalling dark matter particles and compute the resulting flux and spectra of gamma rays coming from the objects it is shown that the dark matter density significantly increases near a black hole particle collision energy becomes very high affecting relative crosssections of various annihilation channels we also discuss possible experimental consequences of these effects,http://arxiv.org/abs/0805.0124v2,0
a review of the present observational and theoretical status of elliptical galaxies is presented with the aim to clarify whether the monolithic or the hierarchical is a more viable scenario for the origin of these structures we describe the dynamical structure of elliptical galaxies using photometric and spectroscopic data in particular 3d observations from integral field spectrographs with emphasis on properties such as brightness distribution velocity profiles central structures like cuspy profiles and cores as well as central supermassive black holes we also report on the main relations between these quantities like the fundamental plane and colourluminosity diagram we present observational evidences for the presence of dark matter in the elliptical galaxies and examine the theories of galaxy formation within the framework of a cold dark matter cosmological model we discuss the formation of largescale structure pressschechter theory and universal density profile of dark matter halos subsequently gas dynamics star formation feedback angular momentum morphology and the epoch of galaxy formation are studied and comparison between disks and spheroids are made valuable insights into the formation epoch of elliptical galaxies are provided by deriving the mean metallicities and ages at different redshifts through the study of different population synthesis models singleburst and evolutionary models in particular the magnesium to iron ratio,http://arxiv.org/abs/astro-ph/0301248v1,0
we have used the hydrodynamical amr code enzo to investigate the dynamical evolution of the gas at the centre of dark matter haloes with virial velocities of 20 30 kms and virial temperatures of 1300030000 k at z 15 in a cosmological context the virial temperature of the dark matter haloes is above the threshold where atomic cooling by hydrogen allows the gas to cool and collapse we neglect cooling by molecular hydrogen and metals as may be plausible if h2 cooling is suppressed by a metagalactic lymanwerner background or an internal source of lymanwerner photons and metal enrichment has not progressed very far the gas in the haloes becomes gravitationally unstable and develops turbulent velocities comparable to the virial velocities of the dark matter haloes within a few dynamical times it settles into a nearly isothermal density profile over many decades in radius losing most of its angular momentum in the process about 01 1 of the baryons at the centre of the dark matter haloes collapse into a selfgravitating fat ellipsoidal centrifugally supported exponential disc with scalelength of 0075027 pc and rotation velocities of 2560 kms we are able to follow the settling of the gas into centrifugal support and the dynamical evolution of the compact disc in each dark matter halo for a few dynamical times the dynamical evolution of the gas at the centre of the haloes is complex in one of the haloes the gas at the centre fragments into a triple system leading to strong tidal perturbations and eventually to the infall of a secondary smaller clump into the most massive primary clump the formation of centrifugally supported selfgravitating massive discs is likely to be an important intermediary stage en route to the formation of a massive black hole seed,http://arxiv.org/abs/0810.0024v1,0
abridged this is the second of two companion papers on the interior structure of selfsimilar accreting charged black holes in the first paper the black hole was allowed to accrete only a single fluid of charged baryons in this second paper the black hole is allowed to accrete in addition a neutral fluid of almost noninteracting dark matter relativistic streaming between outgoing baryons and ingoing dark matter leads to mass inflation near the inner horizon when enough dark matter has been accreted that the center of mass frame near the inner horizon is ingoing then mass inflation ceases and the fluid collapses to a central singularity a null singularity does not form on the cauchy horizon although the simultaneous presence of ingoing and outgoing fluids near the inner horizon is essential to mass inflation reducing one or other of the ingoing dark matter or outgoing baryonic streams to a trace relative to the other stream makes mass inflation more extreme not the other way round as one might naively have expected consequently if the dark matter has a finite crosssection for being absorbed into the baryonic fluid then the reduction of the amount of ingoing dark matter merely makes inflation more extreme the interior mass exponentiating more rapidly and to a larger value before mass inflation ceases however if the dark matter absorption crosssection is effectively infinite at high collision energy so that the ingoing dark matter stream disappears completely then the outgoing baryonic fluid can drop through the cauchy horizon in all cases the solutions do not depend on what happens in the infinite past or future we discuss in some detail the physical mechanism that drives mass inflation,http://arxiv.org/abs/gr-qc/0411062v2,0
we study the gas metallicity of quasar hosts using cosmological hydrodynamic simulations of the lambdacold dark matter model galaxy formation in the simulations is coupled with a prescription for black hole activity enabling us to study the evolution of the metal enrichment in quasar hosts and hence explore the relationship between starspheroid formation and black hole growthactivity we find a steep radial metallicity gradient in quasar host galaxies with gas metallicities close to solar values in the outer parts but becoming supersolar in the center the hosts of the rare bright quasars at z56 have star formation rates of several hundred solar masses per year and halo masses of order 1012 solar masses already at these redshifts they have supersolar z 23 solar central metallicities with a mild dependence of metallicity on luminosity consistent with observed trends the mean value of metallicity is sensitive to the assumed quasar lifetime providing a useful new probe of this parameter we find that lifetimes from 1074x107yr are favored by comparison to observational data in both the models and observations the rate of evolution of the mean quasar metallicity as a function of redshift is generally flat out to z 45 beyond the observed redshift range and out to redshift z 68 we predict a slow decline of the mean central metallicity towards solar and slightly subsolar values as we approach the epoch of the first significant star formation activity,http://arxiv.org/abs/astro-ph/0309533v2,0
a central supermassive black hole smbh with a mass 106109 modot appears to be a common feature in nearby galaxies and the likely power source in quasars and active galactic nuclei we demonstrate that the formation of a central black hole is a natural and inevitable consequence of the gravothermal catastrophe in a selfinteracting dark matter sidm halo through gravothermal evolution driven by collisional relaxation an sidm halo will form a massive inner core whose density and velocity dispersion will increase secularly in time eventually the inner core arrives at a relativistic radial instability and undergoes dynamical collapse to a black hole the initial mass of the black hole will be 108106 of the total mass of the halo we show that if at formation the overdensity in the sidm halo is not too large smbhs in the observed mass range can form directly in very massive halos following core collapse alternatively with large overdensities moderate mass halos undergo core collapse to form central seed black holes of intermediate mass and these holes can then merge andor accrete to reach the smbh range forming smbhs by core collapse in sidm halos requires no baryons no prior epoch of star formation and no other mechanism of forming black holes seeds,http://arxiv.org/abs/astro-ph/0111176v3,0
we simulate the ionization environment of z 20 luminous objects formed within the framework of the current cdm cosmology and compute their uv escape fraction these objects are likely single very massive stars that are copious uv emitters we present analytical estimates as well as onedimensional radiation hydrodynamical calculations of the evolution of these first hii regions in the universe the initially dtype ionization front evolves to become rtype within lesssim 105 yrs at a distance sim1 pc this ionization front then completely overruns the halo accelerating an expanding shell of gas outward to velocities in excess of 30 km s1 about ten times the escape velocity of the confining dark matter halo we find that the evolution of the hii region depends only weakly on the assumed stellar ionizing luminosities consequently most of the gas surrounding the first stars will leave the dark halo whether or not the stars produce supernovae if they form the first massive seed black holes these are unlikely to accrete within a hubble time after they formed until they are incorporated into larger dark matter halos that contain more gas because these ifronts exit the halo on timescales much shorter than the stars main sequence lifetimes their host halos have uv escape fractions of gtrsim 095 fixing an important parameter for theoretical studies of cosmological hydrogen reionization,http://arxiv.org/abs/astro-ph/0310283v2,0
model of supermassive black holes formation inside the clusters of primordial black holes is developed namely it is supposed that some mass fraction of the universe 103 is composed of the compact clusters of primordial relic black holes produced during phase transitions in the early universe these clusters are the centers of dark matter condensation we model the formation of protogalaxies with masses about 2108msun at the redshift z15 these induced protogalaxies contain central black holes with mass 105msun and look like dwarf spheroidal galaxies with central density spike the subsequent merging of induced protogalaxies and ordinary dark matter haloes corresponds to the standard hierarchical clustering scenario of largescale structure formation the coalescence of primordial black holes results in formation of supermassive black holes in the galactic centers as a result the observed correlation between the masses of central black holes and velocity dispersion in the galactic bulges is reproduced,http://arxiv.org/abs/0801.0885v1,0
there have been proposals that primordial black hole remnants bhrs are the dark matter but the idea is somewhat vague we argue here first that the generalized uncertainty principle gup may prevent black holes from evaporating completely in a similar way that the standard uncertainty principle prevents the hydrogen atom from collapsing secondly we note that the hybrid inflation model provides a plausible mechanism for production of large numbers of small black holes combining these we suggest that the dark matter might be composed of plancksize bhrs and discuss the possible constraints and signatures associated with this notion,http://arxiv.org/abs/astro-ph/0303349v2,0
in cuspy atmospheres jets driven by supermassive black holes bhs offset radiative cooling the jets fire episodically but often enough that the cuspy atmosphere does not move very far towards a cooling catastrophe in the intervals of jet inactivity the ability of energy released on the subparsec scale of the bh to balance cooling on scales of several tens of kiloparsecs arises through a combination of the temperature sensitivity of the accretion rate and the way in which the radius of jet disruption varies with ambient density accretion of hot gas does not significantly increase bh masses which are determined by periods of rapid bh growth and star formation when cold gas is briefly abundant at the galactic centre hot gas does not accumulate in shallow potential wells as the universe ages deeper wells form and eventually hot gas accumulates this gas soon prevents the formation of further stars since jets powered by the bh prevent it from cooling and it mops up most cold infalling gas before many stars can form thus bhs set the upper limit to the masses of galaxies the formation of lowmass galaxies is inhibited by a combination of photoheating and supernovadriven galactic winds working in tandem these mechanisms can probably explain the profound difference between the galaxy luminosity function and the mass function of dark halos expected in the cold dark matter cosmology,http://arxiv.org/abs/astro-ph/0407238v1,0
the tight correlation between galaxy bulges and their central black hole masses likely emerges in a phase of rapid collapse and starburst at high redshift due to the balance of gravity on gas with the feedback force from starbursts and the wind from the black hole the average gravity on per unit mass of gas is 2 x 1010 msec2 during the star burst phase this level of gravity could come from the real r1 cusps of cold dark matter cdm halos but the predicted gravity would have a large scatter due to dependence on cosmological parameters and formation histories better agreement is found with the gravity from the scalar field in some covariant versions of mond which can create the mirage of a newtonian effective dark halo of density pi r1 near the center where the characteristic surface density pi130alpha1 msun pc2 and alpha is a fundamental constant of order unity fixed by the lagrangian of the covariant theory if neglecting environmental effects we show with a toy analytical model and a hydrodynamical simulation that a constant background gravity due to mondteves scalar field implies a critical pressure synchronizing starbursts and the formation of galaxy bulges and ellipticals a universal threshold for the formation of the brightest regions of galaxies in a mondian universe suggests that the central bhs bulges and ellipticals would respect tight correlations like the mbulgembhsigma relations in general mond tends to produce tight correlations in galaxy properties because its effective halo has less freedom and scatter than cdm halos,http://arxiv.org/abs/0802.1073v3,0
smallquiescent black holes can be considered as candidates for the missing dark matter of the universeand as the core energy source of ball lightningby means of gravitational tunnelingdirected radiation is emitted from black holes in a process much attenuated from that of hawking radiationp sh which has proven elusive to detectgravitational tunneling emission is similar to electric field emission of electronsfrom a metal in that a second body is involved which lowers the barrier and gives the barrier a finite rather than infinite widthhawking deals with a single isolated black hole,http://arxiv.org/abs/astro-ph/0212251v1,0
we study the crosscorrelation between quasars and galaxies by embedding models for the formation and evolution of the two populations in cosmological nbody simulations we adopt the quasar evolution model of kauffmann haehnelt 2000 in which supermassive black holes are formed and fuelled during major mergers we define the bias parameter bqg as the ratio of the crosscorrelation function xiqg to the galaxy autocorrelation function xigg on scales larger than 1 h1 mpc the values of bqg predicted by our models at low redshift depend very little on galaxy selection they measure the characteristic mass of the dark matter halos that host quasars and can be used to estimate the typical quasar lifetime in current redshift surveys such measurements will constrain the lifetimes of low z quasars more accurately than measurements of the quasar autocorrelation function because galaxies have much higher space densities than quasars on scales smaller than 1 h1 mpc the main contribution to xiqg comes from quasargalaxy pairs in the same dark matter halo the amplitude of xiqg depends both on the location of the host galaxy and on the density profile of other galaxies within the halo as a result measurements on these scales yield information about the processes responsible for fuelling super massive black holes at high redshifts our models predict that quasars of fixed luminosity are located in less massive halos than at low redshift they are therefore less biased relative to galaxies of given luminosity or stellar mass we have used the simulations to calculate the evolution of the quasar auto correlation function we find that models with quasar lifetimes in the range 106107 years provide a good match to the results of the 2df qso survey,http://arxiv.org/abs/astro-ph/0108275v1,0
using very long baseline interferometry we have searched a sample of 300 compact radio sources for examples of multiple imaging produced by gravitational lensing no multiple images were found with separations in the angular range 1550 milliarcsec this null result allows us to place a limit on the cosmological abundance of intergalactic supermassive compact objects in the mass range sim 106 to sim 108modot such objects cannot make up more than sim 1 of the closure density 95 confidence a uniformly distributed population of supermassive black holes forming soon after the big bang do not therefore contribute significantly to the dark matter content of the universe,http://arxiv.org/abs/astro-ph/0101328v1,0
in popular cold dark matter cosmological scenarios stars may have first appeared in significant numbers around a redshift of 10 or so as the gas within protogalactic halos with virial temperatures in excess of 20000 k corresponding to masses comparable to those of presentday dwarf ellipticals cooled rapidly due to atomic processes and fragmented it is this second generation of subgalactic stellar systems aided perhaps by an early population of accreting black holes in their nuclei which may have generated the ultraviolet radiation and mechanical energy that ended the cosmic dark ages and reheated and reionized most of the hydrogen in the cosmos by a redshift of 6 the detailed history of the universe during and soon after these crucial formative stages depends on the powerspectrum of density fluctuations on small scales and on a complex network of poorly understood feedback mechanisms and is one of the missing link in galaxy formation and evolution studies the astrophysics of the epoch of first light is recorded in the thermal state ionization degree and chemical composition of the intergalactic medium the main repository of baryons at high redshifts,http://arxiv.org/abs/astro-ph/0208045v1,0
the first luminous objects in the concordance cosmology form by molecular hydrogen cooling in dark matter dominated halos of masses 106 msun we use eulerian adaptive mesh refinement simulations to demonstrate that in the presence of a large soft ultraviolet radiation background molecular hydrogen is the dominant coolant even for very large radiation backgrounds the halo masses that cool and collapse are up to two orders of magnitude smaller than the halos that cool via atomic hydrogen line cooling the abundance of cooling halos and the cosmic mass fraction contained within them depends exponentially on this critical mass scale consequently the majority of current models of cosmological reionization chemical evolution supermassive black hole formation and galaxy formation underestimate the number of star forming progenitors of a given system by orders of magnitude at the highest redshifts this disagreement is largest we also show that even in the absence of residual electrons collisional ionization in central shocks create a sufficient amount of electrons to form molecular hydrogen and cool the gas in halos of virial temperatures far below the atomic cooling limit,http://arxiv.org/abs/0707.2059v2,0
high resolution deep imaging surveys are instrumental in setting constraints on semianalytical structure formation models in cold dark matter cdm cosmologies we show here that the lack of unresolved bband dropouts with v 25 mag in the hubble deep field hdf appears to be inconsistent with the expected number of quasars if massive black holes form with a constant universal efficiency in all cdm halos to reconcile the models with the data a mechanism is needed that suppresses the formation of quasars in halos with circular velocities vcirc 5075 kms this feedback naturally arises due to the photoionization heating of the gas by the uv background we consider several alternative effects that would help reduce the quasar number counts and find that these can not alone account for the observed lack of detections if reddening by dust can be neglected at early epochs consistency with the optical data also requires that the luminous extent of dwarf galaxies at high redshifts be larger than a few percent of the virial radii of their dark matter halos in order not to overpredict the number of pointlike bband dropouts future deep observations in the j and h bands with nicmos might reveal several z 5 objects per field or provide even stronger constraints on the models than existing b v and i data,http://arxiv.org/abs/astro-ph/9805258v1,0
using the results of recent optical surveys we conclude that the it nondetection of quasars down to faint magnitudes implies a significant flattening of the high redshift z6 optical active galactic nuclei agn luminosity function for m1450247 we find that all the data are consistent with a faintend slope for the optical agn luminosity function of beta22 and beta28 at the 90 and 99 confidence level respectively flatter than the brightend slope of beta 32 we also show that xray deep surveys have probed even fainter magnitudes than the optical ones yielding more significant constraints on the shallow faintend slope of the optical luminosity function the inclusion of type ii agn candidates detected in the chandra deep fields hints towards an higher normalization for the total agn luminosity function if these sources lie at 5z 65 we then discuss simple theoretical models of agn formation and evolution in the context of cold dark matter cosmology the comparison with the total agn luminosity function favors a redshiftdependent relation between black hole and dark matter halo masses of the type mbhm haloalpha with 13 alpha 17 compatible with independent studies from statistical analysis and rotation curve measurements finally we compute the quasar contribution to reionization to be 9 at z6 up to 30 when integrated within 55 z65 significantly smaller than that from galaxies,http://arxiv.org/abs/astro-ph/0701515v2,0
we report on a calculation of the growth of the mass of supermassive black holes at galactic centers from dark matter and eddington limited baryonic accretion assuming that dark matter halos are made of fermions and harbor compact degenerate fermi balls of masses from 103modot to 106modot we find that dark matter accretion can boost the mass of seed black holes from about sim 5modot to 1034modot black holes which then grow by eddington limited baryonic accretion to supermassive black holes of 106 9modot we then show that the formation of the recently detected supermassive black hole of 3times 109modot at a redshift of z 641 in the quasar sdss j114816645251503 could be understood if the black hole completely consumes the degenerate fermi ball and then grows by eddington limited baryonic accretion in the context of this model we constrain the dark matter particle masses to be within the range from 12 rm kevc2 to about 450 rm kevc2 finally we investigate the black hole growth dependence on the formation time of the seed bh and on the mass of the seed bh we find that in order to fit the observed data point of mbh sim 3 times 109modot and z sim 641 dark matter accretion cannot start later than about 2 times 108 years and the seed bh cannot be greater than about 104modot our results are in full agreement with the wmap observations that indicate that the first onset of star formation might have occurred at a redshift of z sim 15 20 for other models of dark matter particle masses corresponding constraints may be derived from the growth of black holes in the center of galaxies,http://arxiv.org/abs/astro-ph/0403511v3,0
here we review the recent evidence for dark energy dark matter and black holes as components of an expanding universe for the vantage point of a nonexpert we speculate on a specific dm particle,http://arxiv.org/abs/astro-ph/0510024v1,0
the transition between the nearly smooth initial state of the universe and its clumpy state today occurred during the epoch when the first stars and lowluminosity quasars formed for cold dark matter cosmologies the radiation produced by the first baryonic objects is expected to ionize the universe at z1020 and consequently suppress by 10 the amplitude of microwave anisotropies on angular scales 10 degrees future microwave anisotropy satellites will be able to detect this signature the production and mixing of metals by an early population of stars provides a natural explanation to the metallicity 1 solar found in the intergalactic medium at redshifts z5 the next generation space telescope ngst will be able to image directly the first light from these stars with its njy sensitivity ngst is expected to detect 103 star clusters per square arcminute at z10 the brightest sources however might be early quasars the infrared flux from an eddington luminosity 106 solar mass black hole at z10 is 10 njy at 1 micron easily detectable with ngst the time it takes a black hole with a radiative efficiency of 10 to double its mass amounts to more than a tenth of the hubble time at z10 and so a fair fraction of all systems which harbor a central black hole at this redshift would appear active the redshift of all sources can be determined from the lymanlimit break in their spectrum which overlaps with the ngst wavelength regime 135 micron for 10z35 absorption spectra of the first generation of star clusters or quasars would reveal the reionization history of the universe the intergalactic medium might show a significant opacity to infrared sources at z10 due to dust produced by the first supernovae,http://arxiv.org/abs/astro-ph/9704290v1,0
although the population of luminous quasars rises and falls over a period of 109 years the typical lifetime of individual quasars is uncertain by several orders of magnitude we show that quasar clustering measurements can substantially narrow the range of possible lifetimes with the assumption that luminous quasars reside in the most massive host halos if quasars are longlived then they are rare phenomena that are highly biased with respect to the underlying dark matter while if they are shortlived they reside in more typical halos that are less strongly clustered for a given quasar lifetime we calculate the minimum host halo mass by matching the observed space density of quasars using the pressschechter approximation we use the results of mo white to calculate the clustering of these halos and hence of the quasars they contain as a function of quasar lifetime a lifetime of tq 4 x 107 years the efolding timescale of an eddington luminosity black hole with accretion efficiency eps01 corresponds to a quasar correlation length r0 10 mpch in lowdensity cosmological models at z23 this value is consistent with current clustering measurements but these have large uncertainties highprecision clustering measurements from the 2df and sloan quasar surveys will test our key assumption of a tight correlation between quasar luminosity and host halo mass and if this assumption holds then they should determine tq to a factor of three or better an accurate determination of the quasar lifetime will show whether supermassive black holes acquire most of their mass during highluminosity accretion and it will show whether the black holes in the nuclei of typical nearby galaxies were once the central engines of highluminosity quasars,http://arxiv.org/abs/astro-ph/0002384v2,0
we study the expected distribution of massive black hole mbh spins and its evolution with cosmic time in the context of hierarchical galaxy formation theories our model uses monte carlo realizations of the merger hierarchy in a lcdm cosmology coupled to semianalytical recipes to follow the merger history of dark matter halos the dynamics of the mbhs they host and their growth via gas accretion and binary coalescences the coalescence of comparable mass holes increases the spin of mbhs while the capture of smaller companions in randomlyoriented orbits acts to spin holes down we find that given the distribution of mbh binary mass ratios in hierarchical models binary coalescences alone do not lead to a systematic spinup or spindown of mbhs with time the spin distribution retains memory of its initial conditions by contrast because of the bardeenpetterson effect gas accretion via a thin disk tends to spin holes up even if the direction of the spin axis changes randomly in time in our models accretion dominates over black hole captures and efficiently spins holes up the spin distribution is heavily skewed towards fastrotating kerr holes is already in place at early epochs and does not change much below redshift 5 if accretion is via a thin disk about 70 of all mbhs are maximally rotating and have radiative efficiencies approaching 30 assuming a standard spinefficiency conversion even in the conservative case where accretion is via a geometrically thick disk about 80 of all mbhs have spin parameters am 08 and accretion efficiencies 12 rapidly spinning holes with high radiative efficiencies may satisfy constraints based on comparing the local mbh mass density with the mass density inferred from luminous quasars soltans argument,http://arxiv.org/abs/astro-ph/0410342v1,0
different studies show that dark matter of nonbaryonic origin might exist there have been experimental evidences that at least one form of dark matter has been detected through microlensing effects this form of dark matter is named machos massive astrophysical compact halo objects the macho collaboration estimated that the masses of these objects are to be in the range 015095 solar masses where the most probable mass is of 05 solar masses some authors argue that machos could be black holes and that they could form binary systems bhmacho binaries as is well known binary systems are sources of gravitational waves the brazilian spherical antenna will operate in the frequency band of 3034 khz sensitive to binaries of a pair of 05 solar mass black holes just before coalescing in the present work we study the detectability of these putative bhmacho binaries by the brazilian spherical antenna mario schenberg,http://arxiv.org/abs/astro-ph/0503109v1,0
the high sensitivity of upcoming spacebased gravitational wave detectors suggests the possibility that if halo dark matter were composed of primordial black holes pbhs with mass between 1016 g and 1020 g the gravitational interaction with detector test masses will lead to a detectable pulselike signal during the flyby for an improved version of the laser interferometer space antenna with a reduced acceleration noise at the lowend of its frequency spectrum we find an event rate with signaltonoise ratios greater than 5 of sim a few per decade involving black holes of mass sim 1017 g the detection rate improves significantly for second generation space based interferometers that are currently envisioned though these events must be distinguished from those involving perturbations due to nearearth asteroids while the presence of primordial black holes below a mass of sim 1016 g is now constrained based on the radiation released during their evaporation the gravitational wave detectors will extend the study of pbhs to a several orders of magnitude higher masses,http://arxiv.org/abs/astro-ph/0405216v1,0
the processes are investigated by which gas loses its angular momentum during the protogalactic collapse phase leading to disk galaxies that are too compact with respect to the observations highresolution nbodysph simulations in a cosmological context are presented including cold gas and dark matter a halo with quiet merging activity since z38 and with a high spin parameter is analysed that should be an ideal candidate for the formation of an extended galactic disk we show that the gas and the dark matter have similar specific angular momenta until a merger event occurs at z2 with a mass ratio of 51 all the gas involved in the merger loses a substantial fraction of its specific angular momentum due to tidal torques and falls quickly into the center dynamical friction plays a minor rolein contrast to previous claims in fact after this event a new extended disk begins to form from gas that was not involved in the 51 merger event and that falls in subsequently we argue that the angular momentum problem of disk galaxy formation is a merger problem in cold dark matter cosmology substantial mergers with mass ratios of 11 to 61 are expected to occur in almost all galaxies we suggest that energetic feedback processes could in principle solve this problem however only if the heating occurs at the time or shortly before the last substantial merger event good candidates for such a coordinated feedback would be a mergertriggered star burst or central black hole heating if a large fraction of the low angular momentum gas would be ejected as a result of these processes latetype galaxies could form with a dominant extended disk component resulting from late infall a small bulgetodisk ratio and a low baryon fraction in agreement with observations,http://arxiv.org/abs/astro-ph/0602005v1,0
we have used gadget2 to simulate the formation of an elliptical galaxy in a cosmological dark matter halo with mass 3x1012msunh using a stellar population synthesis model has allowed us to compute magnitudes colours and surface brightness profiles we have included a model to follow the growth of a central black hole and we have compared the results of simulations with and without feedback from agns we have studied the interplay between cold gas accretion and merging in the development of galactic morphologies the link between colour and morphology evolution the effect of agn feedback on the photometry of early type galaxies the redshift evolution in the properties of quasar hosts and the impact of agn winds on the chemical enrichment of the intergalactic medium igm we have found that the early phases of galaxy formation are driven by the accretion of cold filamentary flows which form a disc at the centre of the dark matter halo when the dark matter halo is sufficiently massive to support the propagation of a stable shock cold accretion is shut down and the star formation rate begins to decline mergers transform the disc into an elliptical galaxy but also bring gas into the galaxy without a mechanism that removes gas from the merger remnants the galaxy ends up with blue colours atypical for its elliptical morphology agn feedback can solve this problem even with a fairly low heating efficiency we have also demonstrated that agn winds are potentially important for the metal enrichment of the igm a high redshiftabridged,http://arxiv.org/abs/0712.3289v2,0
if dark matter in the form of compact objects comprises a large fraction of the mass of the universe then gravitational lensing effects on gammaray bursts are expected we utilize batse and ulysses data to search for lenses of different mass ranges which cause lensing in the milli pico and femto regimes null results are used to set weak limits on the cosmological abundance of compact objects in mass ranges from 1016 to 109 modot a stronger limit is found for a much discussed omega 015 universe dominated by black holes of masses sim 1065 modot which is ruled out at the sim 90 confidence level,http://arxiv.org/abs/astro-ph/9810391v2,0
in popular cold dark matter cosmological scenarios stars may have first appeared in significant numbers around a redshift of 10 or so as the gas within protogalactic halos with virial temperatures in excess of 20000 k corresponding to masses comparable to those of presentday dwarf ellipticals cooled rapidly due to atomic processes and fragmented it is this second generation of subgalactic stellar systems aided perhaps by an early population of accreting black holes in their nuclei which may have generated the ultraviolet radiation and mechanical energy that ended the cosmic dark ages and reheated and reionized most of the hydrogen in the universe by a redshift of 6 the detailed history of the universe during and soon after these crucial formative stages depends on the powerspectrum of density fluctuations on small scales and on a complex network of poorly understood feedback mechanisms and is one of the missing link in galaxy formation and evolution studies the astrophysics of the epoch of first light is recorded in the thermal state ionization degree and chemical composition of the intergalactic medium the main repository of baryons at high redshifts,http://arxiv.org/abs/astro-ph/0212555v1,0
we place new constraints on sources of ionizing and resonance radiation at the epoch of the recombination process using the recent cmb temperature and polarization spectra coming from wmap we find that nonstandard recombination scenarios are still consistent with the current data in light of this we study the impact that such models can have on the determination of several cosmological parameters in particular the constraints on curvature and baryon density appear to be weakly affected by a modified recombination scheme however it may affect the current wmap constraints on inflationary parameters like the spectral index and its running physically motivated models like those based on primordial black hole or super heavy dark matter decay are able to provide a good fit to the current data future observations in both temperature and polarization will be needed to more stringently test these models,http://arxiv.org/abs/astro-ph/0306357v2,0
in this paper i review the main topics on the energetic universe that have been put forward as main science goals in the cosmic vision 20152025 exercise i discuss the study of matter under extreme conditions both under strong gravity and at ultrahigh densities the cosmology of baryons assembly of ordinary matter in darkmatter dominated structures and the creation of heavy elements and the coeval growth of supermassive black holes and stars in galaxies along cosmic history most of these topics can be addressed with a largeaperture deep universe xray space observatory that can be flown soon after 2015 complemented by gravitational wave observatories lisa a focussing gammaray observatory a far infrared highsensitivity observatory and an xray survey telescope,http://arxiv.org/abs/astro-ph/0508209v1,0
the supermassive short period black hole binary oj287 is discussed as a new precision testing ground for general relativity and alternate gravity theories like in the case of binary pulsars the relativistic gravity effects are considerably larger than in the solar system for instance the observed orbital precession is forty degrees per period the gravitational radiation energy losses are comparable to typical blazar electromagnetic radiation emission and it is about ten orders larger than that of the binary pulsar with significant orbit shrinking already apparent in the light curves this already tests einstein gravity to a few percent for objects at cosmological distances constraints on alternate gravity theories as well as possible detection of long term effects of dark matter and dark energy on this system are described,http://arxiv.org/abs/0803.2077v1,0
the braneworld description of our universe entails a large extra dimension and a fundamental scale of gravity that might be lower by several orders of magnitude compared to the planck scale an interesting consequence of the braneworld scenario is in the nature of spherically symmetric vacuum solutions to the brane gravitational field equations which could represent black holes with properties quite distinct compared to ordinary black holes in 4dimensions we discuss certain key features of some braneworld black hole geometries such black holes are likely to have diverse cosmological and astrophysical ramifications the cosmological evolution of primordial braneworld black holes is described highlighting their longevity due to modified evaporation and effective accretion of radiation during the early braneworld high energy era observational abundance of various evaporation products of the black holes at different eras impose constraints on their initial mass fraction surviving primordial black holes could be candidates of dark matter present in galactic haloes we discuss gravitational lensing by braneworld black holes observables related to the relativistic images of strong field gravitational lensing could in principle be used to distinguish between different braneworld black hole metrics in future observations,http://arxiv.org/abs/astro-ph/0503473v2,0
cosmological nucleosynthesis calculations imply that there should be both nonbaryonic and baryonic dark matter recent data suggest that some of the nonbaryonic dark matter must be hot ie massive neutrinos and there may also be evidence for cold dark matter ie wimps if the baryonic dark matter resides in galactic halos it is likely to be in the form of compact objects ie machos and these would probably be the remnants of a first generation of pregalactic or protogalactic population iii stars many candidates have been proposed brown dwarfs red dwarfs white dwarfs or black holes and at various times each of these has been in vogue we review the many types of observations which can be used to constrain or exclude both baryonic and nonbaryonic dark matter candidates,http://arxiv.org/abs/gr-qc/0008005v1,0
the existence of black holes remains open to doubt until other conceivable options are excluded with this motivation we consider a model of a compact star in which most of the mass consists of dark particles of some kind and a small fraction of the mass is in the form of ordinary nucleonic gas the gas does not interact with the dark matter other than via gravity but collects at the center as a separate fermionic fluid component depending on whether the dark mass is made of fermions or bosons the objects may be called fermionfermion stars or bosonfermion stars respectively for appropriate choices of the mass of the dark matter particles these objects are viable models of black hole candidates in xray binaries we consider models with a dark mass of 10 solar masses and a range of gas mass from 106 to nearly one solar mass and analyse the bursting properties of the models when they accrete gas we show that all the models would experience thermonuclear type i xray bursts at appropriate mass accretion rates since no type i bursts have been reported from black hole candidates the models are ruled out the case for identifying black hole candidates in xray binaries as true black holes is thus strengthened,http://arxiv.org/abs/astro-ph/0401549v1,0
the evolution of dark matter in central areas of galaxies is considered the milky way is taken as an example it is driven by scattering off of dark matter particles by bulge stars their absorption by the supermassive black hole and selfannihilation this process is described by diffusion equation in the phase space of energy and angular momentum the equation was integrated for several different models of initial dark matter distribution and using various assumptions about the dynamical factors it turns out that because the milky way center is rather dynamically old 5 relaxation times tr the difference in initial conditions almost vanishes the density attains a nearly universal profile and the gammaray flux from dark matter annihilation lies in rather narrow range which enables more robust determination of the dark matter parameters by present time the mass of dark matter inside the black hole sphere of influence r2 pc has been reduced approximately twice mostly because of heating by stars it is shown that the dynamics of dark matter for ttr is determined mainly by stars outside the sphere of influence,http://arxiv.org/abs/0803.0002v2,0
i review the properties of degenerate fermion balls and investigate the dark matter distribution at galactic centers using nfw moore and isothermal density profiles i show that dark matter becomes degenerate for particles masses of a few kev at distances less than a few parsec from the center of our galaxy to explain the galactic center black hole of mass of sim 35 times 106modot and a supermassive black hole of sim 3 times 109modot at a redshift of 641 in sdss quasars the mass of the fermion ball is assumed to be between 3 times 103 modot and 35 times 106modot this constrains the mass of the dark matter particle between 06 rm kev and 82 rm kev the lower limit on the dark matter mass is improved to about rm 6 kev if exact solutions of poissons equation are used in the isothermal power law case the constrained dark matter particle could be interpreted as a sterile neutrino,http://arxiv.org/abs/astro-ph/0702167v1,0
we discuss the possibility of detecting the presence of primordial black holes pbhs such as those that might account for galactic dark matter using modification of pulsar timing residuals when pbhs pass within 1000 au and impart impulse accelerations to the earth with this technique pbhs with masses around 1025 g 01 lunar mass can be detected currently the constraints on the abundance of such dark matter candidates are weak a 30 yearlong monitoring campaign with the proposed square kilometer array ska can rule out a pbh fraction more than 110 in the solar neighborhood in the form of dark matter with mass 1025 g,http://arxiv.org/abs/astro-ph/0702586v1,0
we discuss the possibilities for primordial black holes pbhs to grow via the accretion of dark matter in agreement with previous works we find that accretion during the radiationdominated era does not lead to a significant mass increase however during matterdomination pbhs may grow by up to two orders of magnitude in mass through the acquisition of large dark matter halos we discuss the possibility of pbhs being an important component in dark matter halos of galaxies as well as their potential to explain the ultraluminous xray sources ulxs observed in nearby galactic disks we point out that although pbhs are ruled out as the dominant component of dark matter there is still a great deal of parameter space open to them playing a role in the modernday universe for example a primordial halo population of pbhs each at 1025 modot making up 01 of the dark matter grow to 1045 modot via the accumulation of dark matter halos to account for sim 10 of the dark matter mass by a redshift of z approx 30 these intermediate mass black holes may then light up when passing through molecular clouds becoming visible as ulxs at the present day or they may form the seeds for supermassive black holes at the centers of galaxies,http://arxiv.org/abs/astro-ph/0608642v1,0
abridged the thermal state of the intracluster medium results from a competition between gas cooling and heating the heating comes from two distinct sources gravitational heating from the collapse of the dark matter halo and thermal input from galaxyblack hole formation however a long standing problem has been that cosmological simulations based on smoothed particle hydrodynamics sph and eulerian mesh codes predict different results even when cooling and galaxyblack hole heating are switched off clusters formed in sph simulations show near powerlaw entropy profiles while those formed in mesh simulations develop a core and do not allow gas to reach such low entropies since the cooling rate is closely connected to the minimum entropy of the gas the differences are of potentially key importance in this paper we investigate the origin of this discrepancy by comparing simulations run using the gadget2 sph code and the flash adaptive eulerian mesh code we show that the discrepancy arises during the idealised merger of two clusters the difference is not sensitive to the resolution of our simulations nor is it is due differences in the gravity solvers galilean noninvariance of the mesh code or an effect of unsuitable artificial viscosity in the sph code instead we find that the difference is inherent to the treatment of eddies and fluid instabilities these are suppressed in the sph simulations while the cluster mergers generate strong vortices in the mesh simulations that efficiently mix the fluid and erase the low entropy gas consequently particles in the sph simulations retain a close connection to their initial entropy while this connection is much weaker in the mesh simulations we discuss the potentially profound implications of these results,http://arxiv.org/abs/0812.1750v2,0
a comparison is made of a range of proposals to include free and bound black holes as either a small component or the dominant constituent of dark matter the hypothesis that dark matter consists of atomic gravitationally bound primordial black holes is closely examined in 3space as well as in nspace it is demonstrated that for dimensions greater than 3 stable gravitational or electrostatic atoms cannot be bound by energy constraints the total number of degrees of freedom of a ddimensional body in nspace is derived so that equipartition of energy may be applied in the early universe blackbody and hawking radiation are generalized to nspace a promising new approach to black hole entropy is presented,http://arxiv.org/abs/astro-ph/0303238v2,0
in this letter we study the process of hawking radiation of a black hole assuming the existence of a limiting physical curvature scale the particular model is constructed using the limiting curvature hypothesis lch and in the context of twodimensional dilaton gravity the black hole solution exhibits properties of the standard schwarzschild solution at large values of the radial coordinate however near the center the black hole is nonsingular and the metric becomes that of de sitter spacetime the hawking temperature is calculated using the method of complex paths we find that such black holes radiate eternally and never completely evaporate the final state is an eternally radiating relic near the fundamental scale which should make a viable dark matter candidate we briefly comment on the black hole information loss problem and the production of such black holes in collider experiments,http://arxiv.org/abs/hep-th/0210016v4,0
in the standard flat cosmological constant lambda cold dark matter cdm cosmology a model of two populations of lens halos for strong gravitational lensing can reproduce the results of the jodrellbank vla astrometric survey jvas and the cosmic lens allsky survey class radio survey in such a model lensing probabilities are sensitive to three parameters the concentration parameter c1 the cooling mass scale mmathrmc and the value of the cdm power spectrum normalization parameter sigma8 the value ranges of these parameters are constrained by various observations however we found that predicted lensing probabilities are also quite sensitive to the flux density brightness ratio qmathrmr of the multiple lensing images which has been in fact a very important selection criterion of a sample in any lensing survey experiments we reexamine the above mentioned model by considering the flux ratio and galactic central super massive black holes smbhs in flat lowdensity cosmological models with different cosmic equations of state omega and find that the predicted lensing probabilities without considering qmathrmr are overestimated a low value of qmathrmr can be compensated by raising the cooling mass scale mmathrmc in fitting the predicted lensing probabilities to jvasclass observations in order to determine the cosmic equation of state omega the uncertainty in mmathrmc must be resolved the effects of smbhs cannot be detected by strong gravitational lensing method when qmathrmrleq 10,http://arxiv.org/abs/astro-ph/0306445v3,0
we study the constraints that highredshift structure formation in the universe places on warm dark matter wdm dominated cosmological models we modify the extended pressschechter formalism to derive the halo mass function in wdm models we show that our predictions agree with recent numerical simulations at low redshift over the halo masses of interest applying our model to galaxy formation at high redshift we find that the loss of power on small scales together with the delayed collapse of lowmass objects results in strong limits on the rootmeansquare velocity dispersion vrms of the wdm particles at z0 for fermions decoupling while relativistic these limits are equivalent to constraints on the mass mx of the particles the presence of a 4 billion solar mass black hole at z58 believed to power the quasar sdss 10441215 implies mx 05 kev or vrms 010 kms assuming that the quasar is unlensed and radiating at or below the eddington limit reionization by z58 also implies a limit on mx if highredshift galaxies produce ionizing photons with an efficiency similar to their z3 counterparts we find mx 12 kev or vrms 003 kms however given the uncertainties in current measurements from the proximity effect of the ionizing background at redshift 3 values of mx as low as 075 kev vrms 006 kms are not ruled out the limit weakens further if instead the ionizingphoton production efficiency is greater at high z but this limit will tighten considerably if reionization is shown in the future to have occurred at higher redshifts wdm models with mx 004 kms produce a lowluminosity cutoff in the highredshift galaxy luminosity function which is directly detectable with the next generation space telescope abridged,http://arxiv.org/abs/astro-ph/0102304v2,0
we consider a class of models for the redshift evolution between 0lsim z lsim 4 of the observed optical and xray quasar luminosity functions lfs with the following assumptions i the massfunction of dark matter halos follows the pressschechter theory ii the black hole bh mass scales linearly with the halo mass iii quasars have a constant universal lifetime and iv a thin accretion disk provides the optical luminosity of quasars while the xrayoptical flux ratio is calibrated from a sample of observed quasars the mass accretion rate dotm onto quasar bhs is a free parameter of the models that we constrain using the observed lfs the accretion rate dot m inferred from either the optical or xray data under these assumptions generally decreases as a function of cosmic time from z simeq 4 to z simeq 0 we find that a comparable accretion rate is inferred from the xray and optical lf only if the xrayoptical flux ratio decreases with bh mass near zsimeq 0 dot m drops to substantially subeddington values at which advectiondominated accretion flows adafs exist such a decline of dot m possibly followed by a transition to radiatively inefficient adafs could explain both the absence of bright quasars in the local universe and the faintness of accreting bhs at the centers of nearby galaxies we argue that a decline of the accretion rate of the quasar population is indeed expected in cosmological structure formation models,http://arxiv.org/abs/astro-ph/9810426v2,0
the importance of angular momentum in regulating the sizes of galactic disks and by this their star formation history is highlighted tidal torques and accretion of satellites in principle provide enough angular momentum to form disks with sizes that are in agreement with observations however three major problems have been identified that challenge cold dark matter theory and affect models of galaxy evolution 1 too much angular momentum is transferred from the gas to the dark halos during infall leading to disks with scale lengths that are too small 2 bulgeless disks require more specific angular momentum than is generated cosmologically even if gas would not lose angular momentum during infall 3 gravitational torques and hierarchical merging produce a specific angular momentum distribution that does not match the distribution required to form exponential disks naturally some gas has exceptionally high angular momentum leading to extended outer disks while another large gas fraction will contain very little specific angular momentum and is expected to fall into the galactic center forming a massive and dominant bulge component any selfconsistent theory of galaxy formation will require to provide solutions to these questions selective mass loss of lowangularmomentum gas in an early phase of galaxy evolution currently seems to be the most promising scenario such a process would have a strong affect on the early protogalactic evolution phase the origin and evolution of galactic morphologies and link central properties of galaxies like the origin of central massive black holes with their global structure,http://arxiv.org/abs/astro-ph/0409540v1,0
using stateoftheart adaptive mesh refinement cosmological hydrodynamic simulations with a spatial resolution of proper 021kpch in refined subregions embedded within a comoving cosmological volume 274mpch3 we investigate the sizes of galaxies at z3 in the standard cold dark matter model where reionization is assumed to complete at zri6 our simulated galaxies are found to be significantly smaller than the observed ones while more than one half of the galaxies observed by hst and vlt ranging from restframe uv to optical bands with stellar masses larger than 2e10 msun have halflight radii larger than 2kpch none of the simulated massive galaxies in the same mass range have halflight radii larger than 2kpch after taking into account dust extinction corroborative evidence is provided by the rotation curves of the simulated galaxies with total masses of 1e111e12msun which display values 3001000kms at small radii 05kpch due to high stellar concentration in the central regions larger than those of any well observed galaxies possible physical mechanisms to resolve this serious problem include 1 an early reionization at zri6 to suppress gas condensation hence star formation 2 a strong internal energetic feedback from stars or central black holes to reduce the overall star formation efficiency or 3 a substantial smallscale cutoff in the matter power spectrum,http://arxiv.org/abs/0805.3150v3,0
recently rapid observational and theoretical progress has established that black holes bhs play a decisive role in the formation and evolution of individual galaxies as well as galaxy groups and clusters in particular there is compelling evidence that bhs vigorously interact with their surroundings in the central regions of galaxy clusters indicating that any realistic model of cluster formation needs to account for these processes this is also suggested by the failure of previous generations of hydrodynamical simulations without bh physics to simultaneously account for the paucity of strong cooling flows in clusters the slope and amplitude of the observed cluster scaling relations and the highluminosity cutoff of central cluster galaxies here we use highresolution cosmological simulations of a large cluster and group sample to study how bhs affect their host systems we focus on two specific properties the halo gas fraction and the xray luminositytemperature scaling relation both of which are notoriously difficult to reproduce in selfconsistent hydrodynamical simulations we show that bh feedback can solve both of these issues bringing them in excellent agreement with observations without alluding to the cooling only solution that produces unphysically bright central galaxies by comparing a large sample of simulated agnheated clusters with observations our new simulation technique should make it possible to reliably calibrate observational biases in cluster surveys thereby enabling various highprecision cosmological studies of the dark matter and dark energy content of the universe,http://arxiv.org/abs/0808.0494v2,0
we present the twopoint correlation function 2pcf of narrowline active galactic nuclei agn selected within the first data release of the sloan digital sky survey using a sample of 13605 agn in the redshift range 0055 z 02 we find that the agn autocorrelation function is consistent with the observed galaxy autocorrelation function on scales 02h1mpc to 100h1mpc the agn hosts trace an intermediate population of galaxies and are not detected in either the bluest youngest diskdominated galaxies or many of the reddest oldest galaxies we show that the agn 2pcf is dependent on the luminosity of the narrow oiii emission line loiii with low loiii agn having a higher clustering amplitude than high loiii agn this is consistent with lower activity agn residing in more massive galaxies than higher activity agn and loiii providing a good indicator of the fueling rate using a model relating halo mass to black hole mass in cosmological simulations we show that agn hosted by 1012 modot dark matter halos have a 2pcf that matches that of the observed sample this mass scale implies a mean black hole mass for the sample of mbh 108 modot,http://arxiv.org/abs/astro-ph/0406357v1,0
we propose the model of first quasars formation around the cluster of rimordial black holes pbhs it is supposed that mass fraction of the universe 103 is composed of the compact clusters of pbhs produced during the phase transitions in the early universe the clusters of pbhs become the centers of dark matter condensation as a result the galaxies with massive central black holes are formed in the process of galaxies formation the central black holes are growing due to accretion this accretion is acompaned by the early quasar activity,http://arxiv.org/abs/astro-ph/0412418v2,0
we study the observational signatures of a potential population of lowluminosity quasars at high redshifts in a lambdacdm cosmology we derive the evolution of the quasar luminosity function at fainter luminosities and higher redshifts than currently detected based on three assumptions 1 the formation of darkmatter halos follows the pressschechter theory 2 the ratio of central black hole mass to halo mass is the same for all halos and 3 the lightcurve of quasars in eddington units is universal we show that a universal lightcurve provides an excellent fit to the observed quasar luminosity function at redshifts 26 z 45 by extrapolating the evolution of this luminosity function to higher redshifts 45 z 20 we find that the associated early population of lowluminosity quasars reionizes the universe around redshift z12 the reprocessing of the uv light of these quasars by dust from early type ii supernovae distorts the microwave background spectrum by a compton yparameter y 1e5 comparable to the upper limit set by cobe the next generation space telescope could detect tens of quasars from redshifts z 10 per square arcminute with its proposed 1njy sensitivity at 135 microns absorption spectra of several such quasars would reveal the reionization history of the universe,http://arxiv.org/abs/astro-ph/9710208v1,0
xeus the xray evolving universe spectroscopy mission is at present an esaisas initiative for the study of the evolution of the hot universe in the postchandraxmmnewton era the key science objectives of xeus are search for the origin and subsequent study of growth of the first massive black holes in the early universe assessment of the formation of the first gravitationally bound dark matter dominated systems and their evolution study of the evolution of metal synthesis up till the present epoch characterization of the true intergalactic medium to reach these ambitious science goals the two salient characteristics of the xeus observatory entail 1 its effective spectroscopic grasp combining a sensitive area 20 m2 below 2 kev with a spectral resolution better than 2 ev this allows significant detection of the most prominent xray emission lines eg ovii sixiii and fexxv in cosmologically distant sources against the sky background 2 its angular resolving power between 2 and 5 arc seconds to minimize source confusion as well as noise due to the galactic xray foreground emission to accommodate these instrument requirements a mission concept has been developed featuring an xray telescope of 50m focal length comprising two laserlocked separate mirror and detector spacecrafts the telescope is injected in a low earth orbit with an inclination commensurate with the iss at present an onorbit growth of the mirror spacecraft is foreseen with the aid of the iss raising the mirror diameter from 45 to 10 m the detector spacecraft will be replaced at 5 year intervals after runout of consumables with an associated upgrade of the focal plane package,http://arxiv.org/abs/astro-ph/0207283v1,0
lensing probabilities of quasars with image separations greater than deltatheta and flux density ratios less than qmathrmr are calculated by foreground dark matter halos in a flat cosmological constant dominated lambdacdm universe the mass density of the lenses is taken to be the navarrofrenkwhite nfw profile on all mass scales plus a central point mass for lowmass halos with mmc5times 1013h1modot we introduce a quantity mmathrmeff which is a point mass ranging from 1 to 1000 times the mass mbullet of a supermassive black hole smbh inhabiting the center of each galaxy to describe the contributions of galactic central smbhs and galactic bulges to lensing probabilities the lensing cross section and thus the lensing probability are quite sensitive to the flux density ratio qmathrmr of multiple images in our calculations it is shown that to reproduce the lensing survey results of jvasclass for qmathrmr10 about 20 of the bulge mass is needed as a point mass for each galaxy since there is still considerable uncertainty regarding the value of the spectrum normalization parameter sigma8 we investigate the effect of varying this parameter within its entire observational range from 07 to 11 and find that low sigma8 values leq 07 are ruled out and the best fit value is sigma8simeq 10,http://arxiv.org/abs/astro-ph/0301037v2,0
we present detailed clustering measurements from the 2df qso redshift survey 2qz in the redshift range 08z21 using a flux limited sample of 14000 objects with effective redshift zeff147 we estimate the quasar projected correlation function for separations 1rh1 mpc20 we find that the 2point correlation function in real space is well approximated by a power law with slope gamma15 pm 02 and comoving correlation length r0480915 h1 mpc splitting the sample into three subsets based on redshift we find evidence for an increase of the clustering amplitude with lookback time for a fixed gamma evolution of r0 is detected at the 36 sigma confidence level the ratio between the quasar correlation function and the mass autocorrelation function derived adopting the concordance cosmological model is found to be scale independent for a linear massclustering amplitude sigma808 the bias parameter decreases from b39 at zeff189 to b18 at zeff106 from the observed abundance and clustering we infer how quasars populate darkmatter haloes of different masses we find that 2qz quasars sit in haloes with m1012 msun and that the mean mass of their host haloes is of the order of 1013 msun the observed clustering is consistent with assuming that the locally observed correlation between blackhole mass and hostgalaxy circular velocity is still valid at z1 from the fraction of haloes which contain active quasars we infer that the characteristic quasar lifetime is tq a few x 107 yr at z1 and approaches 108 yr at higher redshifts,http://arxiv.org/abs/astro-ph/0406036v2,0
popular models for describing the luminositydensity profiles of dynamically hot stellar systems eg jaffe hernquist dehnen were constructed with the desire to match the deprojected form of an r14 lightprofile real galaxies however are now known to have a range of different lightprofile shapes that scale with mass consequently although highly useful the above models have implicit limitations and this is illustrated here through their application to a number of real galaxy density profiles on the other hand the analytical density profile given by prugniel simien 1997 closely matches the deprojected form of sersic r1n lightprofiles including deprojected exponential lightprofiles it is thus applicable for describing bulges in spiral galaxies dwarf elliptical galaxies and both ordinary and giant elliptical galaxies here we provide simple equations in terms of elementary and special functions for the gravitational potential and force associated with this density profile furthermore to match galaxies with partially depleted cores and better explore the supermassive black hole galaxy connection we have added a powerlaw core to this density profile and derived similar expressions for the potential and force of this hybrid profile expressions for the mass and velocity dispersion assuming isotropy are also given these models may also prove appropriate for describing the dark matter distribution in halos formed from lcdm cosmological simulations,http://arxiv.org/abs/astro-ph/0506192v2,0
a new view on our galaxy has recently emerged with large consequences on its formation scenarios not only new dwarf satellites have been detected still orbiting and tidally disrupting but also a multitude of stellar streams or tidal debris have been observed suggesting the formation of the halo through successive accretions the large scatter in the agemetallicity relation in the solar neighborhood points towards several accretion episodes while the chemical evolution of the disk requires a more or less continuous gas infall the global kinematics and morphology refined by large surveys such as 2mass suggest the existence of two embedded bars as is frequently observed in external galaxies the mass of the central black hole has been refined through stellar proper motions and is compatible with the mbhsigma relation valid for all bulges the baryonic dark matter is no longer thought to lie in compact objects and on the contrary more dark cold gas is revealed by gammaray observations the star forming history can be built and confronted to numerical models of galaxy evolution both through hierarchical and secular scenarios our galaxy plays thus the role of a prototype to probe galaxy formation theories and in particular thin and thick disk formation,http://arxiv.org/abs/astro-ph/0507280v1,0
abridged the observatory for multiepoch gravitational lens astrophysics omega is a mission concept for a 15m nearuv through nearir space observatory that will be dedicated to frequent imaging and spectroscopic monitoring of 100 multiplyimaged active galactic nuclei over the whole sky using wavelengthtailored dichroics with extremely high transmittance efficient imaging in six channels will be done simultaneously during each visit to each target the separate spectroscopic mode engaged through a flipin mirror uses an image slicer spectrograph after a period of many visits to all targets the resulting multidimensional movies can then be analyzed to a measure the mass function of dark matter substructure b measure precise masses of the accreting black holes as well as the structure of their accretion disks and their environments over several decades of physical scale and c measure a combination of the local hubble expansion and cosmological distances to unprecedented precision,http://arxiv.org/abs/0806.1884v1,0
it is now established that several of the local group dwarf spheroidal galaxies dsphs have large masstolight ratios we consider the possibility that the dark matter in the halos of dsphs is composed of massive black holes with masses in the range 105 to 107 solar masses we use direct nbody simulations to determine the long term evolution of a twocomponent dsph composed of a pressuresupported stellar population in a black hole halo the black holes are initially distributed according to a navarro frenk white profile for black hole masses between 105 and 106 solar masses the dark matter halo evolves towards a shallower inner profile in less than a hubble time this suggests that black holes in this mass range might provide an explanation for the origin of the dark matter cores inferred from observations of low surface brightness galaxy rotation curves we compare the simulated evolution of the stellar population with observed data for the draco dsph the dependence of the heating rate on the black hole mass is determined and an upper limit of 105 solar masses is placed on the individual black holes comprising the dark matter halo of draco if its present stellar distribution is representative of the initial one we also present a simple scaling argument which demonstrates that the dynamical heating of an initially compact though not selfgravitating stellar distribution might produce a remnant distribution similar in extent to draco after 10 gyr even for black hole masses somewhat in excess of 105 solar masses,http://arxiv.org/abs/astro-ph/0311457v2,0
the cosmological features of primordial black holes formed from collapsed cosmic string loops are studied observational restrictions on a population of primordial black holes are used to restrict f the fraction of cosmic string loops which collapse to form black holes and mu the cosmic string massperunitlength using a realistic model of cosmic strings we find the strongest restriction on the parameters f and mu is due to the energy density in 100 mev photons radiated by the black holes we also find that inert black hole remnants cannot serve as the dark matter if earlier crude estimates of f are reliable our results severely restrict mu and therefore limit the viability of the cosmic string largescale structure scenario,http://arxiv.org/abs/hep-ph/9306221v1,0
are black holes elementary particles are they fermions or bosons we investigate the remarkable possibility that quantum black holes are the smallest and heaviest elementary particles we are able to construct various fundamental quantum black holes the spin0 spin 12 spin1 and the planckcharge cases using the results in general relativity quantum black holes in the neighborhood of the galaxy could resolve the paradox posed by the greisenzatsepinkuzmin limit on the energy of cosmic rays from distant sources they could also play a role as dark matter in cosmology,http://arxiv.org/abs/0812.5012v1,0
the comparison of the black hole mass function bhmf of active galactic nuclei agn relics with the measured mass function of the massive black holes in galaxies provides strong evidence for the growth of massive black holes being dominated by mass accretion we derive the eddington ratio distributions as functions of black hole mass and redshift from a large agn sample with measured eddington ratios given by kollmeier et al we find that even at the low mass end most black holes are accreting at eddington ratio 02 which implies that the objects accreting at extremely high rates should be rare or such phases are very short using the derived eddington ratios we explore the cosmological evolution of massive black holes with an agn bolometric luminosity function lf it is found that the resulted bhmf of agn relics is unable to match the measured local bhmf of galaxies for any value of constant radiative efficiency motivated by volonteri sikora lasotas study on the spin evolution of massive black holes we assume the radiative efficiency to be dependent of black hole mass ie it is low for m108 solar masses and it increases with black hole mass for black holes with 108 solar masses we find that the bhmf of agn relics can roughly reproduce the local bhmf of galaxies if the radiative efficiency 008 for the black holes with 108 solar masses and it increases to 018 for black holes with 109 solar masses which implies that most massive black holes 109 solar masses are spinning very rapidly,http://arxiv.org/abs/0808.0759v2,0
we use semianalytic models implemented in the millennium simulation to analyze the merging histories of dark matter haloes and of the galaxies that reside in them we assume that supermassive black holes only exist in galaxies that have experienced at least one major merger only a few percent of galaxies with stellar masses less than m 1010 modot are predicted to have experienced a major merger and to contain a black hole the fraction of galaxies with black holes increases very steeply at larger stellar masses this agrees well with the observed strong mass dependence of the fraction of nearby galaxies that contain either lowluminosity linertype or higherluminosity seyfert or compositetype agn we then investigate when the major mergers that first create the black holes are predicted to occur high mass galaxies are predicted to have formed their black holes at very early epochs the majority of low mass galaxies never experience a major merger and hence do not contain a black hole but a significant fraction of the supermassive black holes that do exist in low mass galaxies are predicted to have formed recently,http://arxiv.org/abs/0801.3530v1,0
if primordial black holes pbh saturate the present upper limit on the dark matter density in our solar system and if their radiation spectrum is discrete the sensitivity of modern detectors is close to that necessary for detecting this radiation this conclusion is not in conflict with the upper limits on the pbh evaporation rate,http://arxiv.org/abs/astro-ph/0604003v2,0
equilibrium configurations of weakly interacting fully degenerate fermionic dark matter are considered at various scales in the universe we treat the general situations for the gravity from newtonian to general relativity and the degeneracy from nonrelativistic to relativistic a dimensionless equilibrium configuration is specified by a single parameter regardless of particle properties the fermi velocity at the center and the scalings of mass and length are specified by the rest mass and statistical weight of the dark matter particle we focus our attention to the flattop nature of the mass column density profile of the cluster of galaxies a1689 recently reported by broadhurst et al using gravitational lensing we convert the column density profile to a volume density profile assuming spherical symmetry and derive a 3d encircled mass profile of a1689 which is compared with the model profiles of degenerate fermion structures the flattop profile is reproduced the corresponding fermion mass ranges from 2 ev to 30 ev depending on the actual scale of the degenerate structure if massive neutrinos are the dominant dark matter the rest mass will be about 47 or 23 ev respectively for majorana or dirac neutrinos the mass and size of the degenerate structure are 1014modot and 100 kpc for majorana neutrinos and 5times1014modot and 300 kpc for dirac neutrinos if we identify the fermions as heavier sterile neutrinos they yield the characteristic mass hierarchy of black holes giant black hole at the center of a galaxy and the intermediate mass black holes thus we propose the possibility that the mass hierarchy of fermions determines that of black holes in the universe,http://arxiv.org/abs/astro-ph/0506623v1,0
we explore possible effects of vacuum energy on the evolution of black holes if the universe contains a cosmological constant and if black holes can absorb energy from the vacuum then black hole evaporation could be greatly suppressed for the magnitude of the cosmological constant suggested by current observations black holes larger than sim 4 times 1024 g would accrete energy rather than evaporate in this scenario all stellar and supermassive black holes would grow with time until they reach a maximum mass scale of sim 6 times 1055 g comparable to the mass contained within the present day cosmological horizon,http://arxiv.org/abs/astro-ph/9902118v1,0
the gravitational field and radiation from quantized gravitational atoms and little black holes lbh are analyzed in nspace ie in all dimensions to develop insights into possible additional compacted dimensions as predicted by hierarchy and string theory it is shown that the entropy of lbh is significantly greater in higher dimensional space with potential implications to the initial entropy of the universe a case is made that lbh are the dark matter of the universe and can manifest themselves as the core energy source of ball lightning bl the lbh incidence rate on earth is related to bl occurrence and has the potential of aiding in the determination of the distribution of lbh and hence dark matter in the universe examination of lbh interactions with the atmosphere are found to be in accord with observations of bl possibilities are explored as to why hawking radiation has been undetected in over 25 years an alternate lbh tunneling radiation model is described,http://arxiv.org/abs/astro-ph/0104026v2,0
the total number of degrees of freedom of a ddimensional body in nspace is derived so that equipartition of energy may be applied to a possibly ndimensional early universe a comparison is made of a range of proposals to include free and bound black holes as either a small component or the dominant constituent of dark matter in the universe the hypothesis that dark matter consists in part of atomic gravitationally bound primordial black holes is closely examined in 3space as well as in nspace and the chavda and chavda holeum hypothesis is found to be flawed blackbody and hawking radiation are generalized to nspace and hawking radiation is shown to be simply proportional to the black hole density the importance of quantum gravity in understanding the role of early universe dark matter is undermined because present approaches to a theory of quantum gravity violate the equivalence principle a general heuristic proof for geodesics on an expanding hypersphere is presented classical limits of einsteins general relativity are considered a novel approach to the accelerated expansion of the universe is discussed an anomalous surprising aspect of 4space is demonstrated,http://arxiv.org/abs/astro-ph/0701358v2,0
we investigate a model in which galactic nuclei form via the coalescence of preexisting stellar systems containing supermassive black holes merger simulations are carried out using nbody algorithms that can follow the formation and decay of a blackhole binary and its effect on the surrounding stars down to subparsec scales our initial stellar systems have steep central density cusps similar to those in lowluminosity elliptical galaxies formation of a blackhole binary transfers energy to the stars and lowers the central density continued decay of the binary creates a 1r density cusp similar to those observed in bright elliptical galaxies with a break radius that extends well beyond the sphere of gravitational influence of the black holes the decay of the black hole binary is followed over a factor of 20 in separation after formation of a hard binary considerably farther than in previous simulations we see almost no dependence of the binarys decay rate on number of particles in the simulation contrary to earlier studies in which a lower initial density of stars led to a more rapid depletion of the binarys loss cone we nevertheless argue that the decay of a black hole binary in a real galaxy would be expected to stall at separations of 0011 pc unless some additional mechanism is able to extract energy from the binary our results support a picture in which the observed dependence of nuclear cusp slope on galaxy luminosity is a consequence of galaxy interactions we also discuss the implications of our results for the survivability of darkmatter cusps,http://arxiv.org/abs/astro-ph/0103350v1,0
we examine the formation and evolution of primordial black holes pbhs after hybrid inflation our goal is to assess the effects of various theoretical uncertainties on the extrapolation from a given inflation model to a spectrum of primordial black hole masses the context of our work is an examination of the possibility chen and adler 2003 macgibbon 1987 that the dark matter is comprised of planckmass black hole remnants bhrs as an example we focus on a particular scenario chen 2003 in which the black holes form from quantum perturbations that were generated during hybrid inflation we find the correspondence between hybrid inflation parameters and the range of initial pbh masses that would allow bhrs to comprise the dark matter taking account of the possible early presence of radiation and its accretion onto the pbhs,http://arxiv.org/abs/astro-ph/0504606v1,0
gravitational waves from the coalescence of binary black holes carry away linear momentum causing center of mass recoil this radiation rocket effect has important implications for systems with escape speeds of order the recoil velocity we revisit this problem using black hole perturbation theory treating the binary as a test mass spiraling into a spinning hole for extreme mass ratios q m1m2 1 we compute the recoil for the slow inspiral epoch of binary coalescence very accurately these results can be extrapolated to q 04 with modest accuracy although the recoil from the final plunge contributes significantly to the final recoil we are only able to make crude estimates of its magnitude we find that the recoil can easily reach 100200 kms but most likely does not exceed 500 kms though much lower than previous estimates this recoil is large enough to have important astrophysical consequences these include the ejection of black holes from globular clusters dwarf galaxies and highredshift dark matter halos,http://arxiv.org/abs/astro-ph/0402056v2,0
there is compelling evidence that supermassive black holes exist yet the origin of these objects or their seeds is still unknown we discuss several plausible scenarios for forming the seeds of supermassive black holes these include the catastrophic collapse of supermassive stars the collapse of relativistic clusters of collisionless particles or stars the gravothermal evolution of dense clusters of ordinary stars or stellarmass compact objects and the gravothermal evolution of selfinteracting dark matter halos einsteins equations of general relativity are required to describe key facets of these scenarios and largescale numerical simulations are performed to solve them,http://arxiv.org/abs/astro-ph/0304202v1,0
one of the most intriguing problem of modern physics is the question of the endpoint of black hole evaporation based on einsteindilatongaussbonnet four dimensional string gravity model we show that black holes do not disappear and that the end of the evaporation process leaves some relic the possibility of experimental detection of the remnant black holes is investigated if they really exist such objects could be a considerable part of the non baryonic dark matter in our universe,http://arxiv.org/abs/gr-qc/0201069v5,0
the observed 511 kev line from the galactic bulge is a real challenge for theoretical astrophysics despite a lot of suggested mechanisms there is still no convincing explanation and the origin of the annihilated positrons remains unknown here we discuss the possibility that a population of slowly evaporating primordial black holes with the mass around 10161017 g ejects among other particles lowenergy positrons into the galaxy in addition to positrons we have also calculated the spectrum and number density of photons and neutrinos produced by such black holes and found that the photons are potentially observable in the near future while the neutrino flux is too weak and below the terrestrial and extraterrestrial backgrounds depending on their mass distribution such black holes could make a small fraction or the whole cosmological dark matter,http://arxiv.org/abs/0801.2786v4,0
kelso hooper and buckley arxiv11105338 found cresst cogent and damalibra results are consistent with 10 15 gev dark matter particles hennawi and ostriker arxivastroph0108203 analyzed supermassive black hole formation in the centers of galaxies finding a best fit for dark matter selfinteraction crosssectionmass ratio 002 cm2g with roundoff error of 25 combining the hennawiostriker result with the strong gravity model for dark matter arxiv07063050 requires dark matter particles with mass between 105 gev and 175 gev overlapping the kelsohooperbuckley dark matter particle mass range,http://arxiv.org/abs/0706.3050v3,0
velocitydependent interactions in a fundamentalstring dominated universe lead quite naturally with reasonable assumptions on initial conditions to an accelerating expanding universe without assuming the existence of a cosmological constant this result also holds generically for a universe dominated by moving extremal black holes owing to a repulsive velocity dependent force this interaction however does not preclude structure formation here we discuss a toy model including both ordinary and extremal matter in which the latter accounts for dark matter while simultaneously acting as effective dark energy eternal acceleration is once more seen to arise in this case,http://arxiv.org/abs/astro-ph/0303422v1,0
we study the distribution of fermionic dark matter at the center of galaxies using nfw moore and isothermal density profiles and show that dark matter becomes degenerate for particle masses of a few rm kev and for distances less than a few parsec from the center of our galaxy a compact degenerate core forms after galaxy merging and boosts the growth of supermassive black holes at the center of galaxies to explain the galactic center black hole of mass of sim 35 times 106modot and a supermassive black hole of sim 3 times 109modot at a redshift of 641 in sdss quasars we require a degenerate core of mass between 3 times 103 modot and 35 times 106modot this constrains the mass of the dark matter particle between 06 rm kev and 82 rm kev the lower limit on the dark matter mass is improved to rm 7 kev if exact solutions of poissons equation are used in the isothermal power law case we argue that the constrained particle could be the long sought dark matter of the universe that is interpreted here as a sterile neutrino,http://arxiv.org/abs/astro-ph/0609388v1,0
we consider the consequences of sidm for a velocity dependent cross section per unit mass accretion of sidm onto seed black holes can produce supermassive black holes that are too large for certain combinations of parameterswhich is used to obtain a new constraint on the dark matter interaction constraints due to other considerations are presented and previous ones are generalized the black hole constraint is extremely sensitive to the slope alpha of the inner density profile of dark halos for the most probable value of alpha13 there exists a narrow range in parameter space such that all constraints are satisfied however the adiabatic compression of the dark halo by baryons as they cool and contract in normal galaxies yields a steeper cusp alpha17 this gives a tighter constraint which would exclude sidm as a possible solution to the purported problems with cdm in the absence of other dynamical processes nevertheless sidm with parameters consistent with this stronger constraint can explain the ubiquity of supermassive black holes in the centers of galaxies a best fit model is presented which reproduces the supermassive black hole masses and their observed correlations with the velocity dispersion of the host bulges specifically the fourth power dependence of black hole mass on velocity dispersion is a direct consequence of the power spectrum having an index of n2 although the dark matter collision rates for this model are too small to directly remedy problems with cdm mergers between dark halos harboring supermassive black holes at high redshift could ameliorate the cuspy halo problem this scenario also explains the lack of comparable supermassive black holes in bulgeless galaxies like m33,http://arxiv.org/abs/astro-ph/0108203v2,0
the hess array of cherenkov telescopes has performed from 2004 to 2007 a survey of the inner galactic plane at photon energies above 100 gev about 400 hours of data have been accumulated in the region between 30 and 60 degrees in galactic longitude and between 3 and 3 degrees in galactic latitude assuming that dark matter is composed of weakly interacting massive particles we calculate here the hess sensitivity map for dark matter annihilations and derive the first experimental constraints on the minispikes scenario in which a gammaray signal arises from dark matter annihilation around intermediate mass black holes the data exclude the proposed scenario at a 90 confidence level for dark matter particles with velocityweighted annihilation cross section sigma v above 1028 cm3s1 and mass between 800 gev and 10 tev,http://arxiv.org/abs/0806.2981v2,0
we investigate the evolution of high redshift seed black hole masses at late times and their observational signatures the massive black hole seeds studied here form at extremely high redshifts from the direct collapse of pregalactic gas discs populating dark matter halos with seeds formed in this way we follow the mass assembly of these black holes to the present time using a montecarlo merger tree using this machinery we predict the black hole mass function at high redshifts and at the present time the integrated mass density of black holes and the luminosity function of accreting black holes as a function of redshift these predictions are made for a set of three seed models with varying black hole formation efficiency given the accuracy of current observational constraints all 3 models can be adequately fit discrimination between the models appears predominantly at the low mass end of the present day black hole mass function which is not observationally well constrained however all our models predict that low surface brightness bulgeless galaxies with large discs are least likely to be sites for the formation of massive seed black holes at high redshifts the efficiency of seed formation at high redshifts has a direct influence on the black hole occupation fraction in galaxies at z0 this effect is more pronounced for low mass galaxies this is the key discriminant between the models studied here and the population iii remnant seed model we find that there exists a population of low mass galaxies that do not host nuclear black holes our prediction of the shape of the black hole mass velocity dispersion relation at the low mass end is in agreement with the recent observational determination from the census of low mass galaxies in the virgo cluster,http://arxiv.org/abs/0709.0529v2,0
we present a clustering analysis of qsos using over 20000 objects from the final catalogue of the 2df qso redshift survey 2qz measuring the zspace correlation function xis when averaged over the range 03z22 we find that xis is flat on small scales steepening on scales above 25h1mpc in a wmap2df cosmology we find a best fit power law with s0548042048h1mpc and gamma120010 on scales s125h1mpc a cdm model assuming wmap2df cosmological parameters is a good description of the qso xis after accounting for nonlinear clustering and zspace distortions and a linear bias of bqsoz135202007 we subdivide the 2qz into 10 redshift intervals from z053 to 248 and find a significant increase in clustering amplitude at high redshift in the wmap2df cosmology we derive the bias of the qsos which is a strong function of redshift with bqsoz053113018 and bqsoz248424053 we use these bias values to derive the mean dark matter halo dmh mass occupied by the qsos at all redshifts 2qz qsos inhabit approximately the same mass dmhs with mdh3016x1012h1msun which is close to the characteristic mass in the pressschechter mass function m at z0 if the relation between black hole bh mass and mdh or host velocity dispersion does not evolve then we find that the accretion efficiency lledd for l qsos is approximately constant with redshift thus the fading of the qso population from z2 to 0 appears to be due to less massive bhs being active at low redshift we apply different methods to estimate tqso the active lifetime of qsos and constrain this to be in the range 4x1066x108 years at z2 abridged,http://arxiv.org/abs/astro-ph/0409314v1,0
a study is made of the behavior of massive black holes in disk galaxies that have received an impulsive kick from a merger or a sustained acceleration from an asymmetric jet the motion of the gas stars dark matter and massive black hole are calculated using the gadget2 simulation code the massive black hole escapes the galaxy for kick velocities above about 600 kms or accelerations above about 4108 cms2 over timescales of the order of 108 yr for smaller velocity kicks or smaller accelerations the black hole oscillates about the center of mass with a frequency which decreases as the kick velocity or acceleration increases the black hole displacements may give rise to observable nonaxisymmetries in the morphology and dynamics of the stellar and gaseous disk of the galaxy in some cases the dynamical center of the galaxy is seen to be displaced towards the direction of the bh acceleration with a characteristic tongue shaped extension of the velocity contours on the side of the galaxy opposite the acceleration,http://arxiv.org/abs/0802.2058v1,0
supermassive black holes are investigated as possible sources for lowfrequency bursts of gravity waves the event rate for known supermassive black holes at intermediate and high redshifts inferred from the quasar luminosity function is low sim 01 yr1 a spacebased interferometer could therefore only see several events per year from supermassive black holes if an additional population of supermassive black holes existed and emitted gravitational waves efficiently these might reside in the population of dwarf galaxies or in a transient population of small dark matter haloes that have mostly merged into larger haloes hosting the galaxies seen today the proposed spacebased gravitationalwave interferometer lisasagittarius should detect most gravitationalwave events involving supermassive black holes above 104 msol out to redshifts of z sim 100,http://arxiv.org/abs/astro-ph/9405032v1,0
there is overwhelming evidence for the presence of supermassive black holes smbhs in the centers of most nearby galaxies the mass estimates for these remnant black holes from the stellar kinematics of local galaxies and the quasar phenomenon at high redshifts point to the presence of assembled smbhs the accretion history of smbhs can be reconstructed using observations at high and low redshifts as model constraints observations of galaxies and quasars in the submillimeter infrared optical and xray wavebands are used as constraints along with data from the demography of local black holes theoretical modeling of the growth of black hole mass with cosmic time has been pursued thus far in two distinct directions a phenomenological approach that utilizes observations in various wavebands and a semianalytic approach that starts with a theoretical framework and a set of assumptions with a view to matching observations both techniques have been pursued in the context of the standard paradigm for structure formation in a cold dark matter dominated universe here we examine the key issues and uncertainties in the theoretical understanding of the growth of smbhs,http://arxiv.org/abs/astro-ph/0404504v1,0
nonbaryonic or dark matter is believed to be a major component of the total mass budget of the universe we review the candidates for particle dark matter and discuss the prospects for direct detection via interaction of dark matter particles with laboratory detectors and indirect detection via observations of the products of dark matter selfannihilations focusing in particular on the galactic center which is among the most promising targets for indirect detection studies the gravitational potential at the galactic center is dominated by stars and by the supermassive black hole and the dark matter distribution is expected to evolve on subparsec scales due to interaction with these components we discuss the dominant interaction mechanisms and show how they can be used to rule out certain extreme models for the dark matter distribution thus increasing the information that can be gleaned from indirect detection searches,http://arxiv.org/abs/astro-ph/0504422v1,0
many questions in physical cosmology regarding the thermal history of the intergalactic medium chemical enrichment reionization etc are thought to be intimately related to the nature and evolution of pregalactic structure in particular the efficiency of primordial star formation and the primordial imf are of special interest we present results from high resolution threedimensional adaptive mesh refinement simulations that follow the collapse of primordial molecular clouds and their subsequent fragmentation within a cosmologically representative volume comoving scales from 128 kpc down to 05 pc are followed accurately dark matter dynamics hydrodynamics and all relevant chemical and radiative processes cooling are followed selfconsistently for a cluster normalized cdm structure formation model primordial molecular clouds with 100000 solar masses are assembled by mergers of multiple objects that have formed hydrogen molecules in the gas phase with a fractional abundance of 104 as the subclumps merge cooling decreases the temperature to 200 kelvin in multiple cold pockets of the merger product within these cold pockets quasihydrostatically contracting cores with masses 100 msun and number densities 100000cm3 are found we find that less than 1 of the primordial gas in such small scale structures cools and collapses to sufficiently high densities to be available for primordial star formation furthermore our results indicate that the formation of very massive objects massive black holes fragmentation of a large fraction of baryons into brown dwars or jupiter size fragments seems in contrast to various claims in the literature very unlikely the expected escape fraction of uv photons with h nu 11ev is very small,http://arxiv.org/abs/astro-ph/9810215v1,0
the hypothesis that dark matter is converted into visible particles in active galactic nuclei is investigated if dark matter consists of stable superheavy neutral particles and active galactic nuclei are rotating black holes then due to the penrose process superheavy particles can decay into unstable particles with larger mass whose decay into quarks and leptons leads to events in cosmic rays observed by the auger group similar processes of decay of superheavy particles of dark matter into visible matter occurred in the early universe numerical estimates of the processes in active galactic nuclei and in the early universe are given,http://arxiv.org/abs/0810.1724v2,0
in this lecture we investigate the formation and evolution of black holes in star clusters the star clusters under consideration are generally rich containing more than 104 stars and with a density exceeding 104 starspc3 among these are young dense clusters yodecs globular cluster and the nuclei of galaxies we will also address the the possible evolutionary link between stellar mass black holes via intermediate mass black holes to supermassive black holes mainly focus on the ecology of star clusters,http://arxiv.org/abs/astro-ph/0406550v1,0
relativistic outflows are a common phenomenon in accreting black holes despite the enormous differences in scale stellarmass black holes in binaries and supermassive black holes in galactic nuclei produce jets with analogous properties in both are observed two types of relativistic outflows 1 steady compact jets with flatspectrum and 2 sporadic extended jets with steepspectrum and apparent superluminal motions besides the most common class of gammaray bursts are afterglows from ultrarelativistic jets associated to the formation of black holes at cosmological distances,http://arxiv.org/abs/astro-ph/0005203v1,0
recently there has been a growing evidence in favor of the presence of an intermediate mass black hole in the globular cluster g1 in andromeda galaxy in this paper we explore whether the adiabatic growth in the dark matter density due to the presence of a black hole could result in an observable gamma ray signal due to dark matter annihilation in this globular cluster starting from an initial nfw matter profile with density parameters consistent with g1 observations we find that indeed if the spike in the density has been formed and has survived till present the signal could be observed by glast and current act detectors,http://arxiv.org/abs/0712.4181v2,0
abridged we present a model in which the seeds of supermassive black holes form from the lowest angular momentum gas in protogalaxies at high redshift we show that this leads to a correlation between black hole masses and spheroid properties as observed today we assume that gas in earlyforming rare halos has a distribution of specific angular momentum similar to that expected for dark matter halos this distribution has a significant low angular momentum tail which implies that every protogalaxy should contain gas that ends up in a highdensity disc in halos more massive than a critical threshold of 108msun the discs are gravitationally unstable and experience an efficient linpringle viscosity that allows mass inflow this process continues until the first massive stars disrupt the disc black holes are created with a characteristic mass of 105 msun independent of the redshift of formation this serves as a lower bound for blackhole masses in galaxies today the comoving mass density of black hole seeds grows with time and saturates when cosmic reionization stops gas cooling in these lowmass systems by z15 the comoving mass density becomes comparable to that inferred from observations with room for appropriate additional luminous growth during a later quasar accretion phase hierarchical merging after z15 naturally leads to a linear correlation between blackhole mass and stellar spheroid mass with negligible black hole masses in discdominated galaxies,http://arxiv.org/abs/astro-ph/0311487v2,0
we describe a mechanism by which supermassive black holes can form directly in the nuclei of protogalaxies without the need for seed black holes left over from early star formation selfgravitating gas in dark matter halos can lose angular momentum rapidly via runaway global dynamical instabilities the socalled bars within bars mechanism this leads to the rapid buildup of a dense selfgravitating core supported by gas pressure surrounded by a radiation pressuredominated envelope which gradually contracts and is compressed further by subsequent infall these conditions lead to such high temperatures in the central region that the gas cools catastrophically by thermal neutrino emission leading to the formation and rapid growth of a central black hole we estimate the initial mass and growth rate of the black hole for typical conditions in metalfree halos with tvir 104 k which are the most likely to be susceptible to runaway infall the initial black hole should have a mass of 20 solar masses but in principle could grow at a supereddington rate until it reaches 104106 solar masses rapid growth may be limited by feedback from the accretion process andor disruption of the mass supply by star formation or halo mergers even if supereddington growth stops at 103104 solar masses this process would give black holes ample time to attain quasarsize masses by a redshift of 6 and could also provide the seeds for all supermassive black holes seen in the present universe,http://arxiv.org/abs/astro-ph/0602363v2,0
we present new velocity dispersion measurements of sample of 12 spiral galaxies for which extended rotation curves are available these data are used to refine a recently discovered correlation between the circular velocity and the central velocity dispersion of spiral galaxies we find a slightly steeper slope for our larger sample we confirm the negligible intrinsic scatter on this correlation and we find a striking agreement with a corresponding relation for elliptical galaxies we combine this correlation with the wellknown mbhsigma relation to obtain a tight correlation between the circular velocities of galaxies and the masses of the supermassive black holes they host this correlation is the observational evidence for an intimate link between dark matter haloes and supermassive black holes apart from being an important ingredient for theoretical models of galaxy formation and evolution the relation between mbh and circular velocity can serve as a practical tool to estimate black hole masses in spiral galaxies,http://arxiv.org/abs/astro-ph/0303628v2,0
we investigate the relation between circular velocity vc and bulge velocity dispersion sigma in spiral galaxies based on literature data and new spectroscopic observations we find a strong nearly linear vcsigma correlation with a negligible intrinsic scatter and a striking agreement with the corresponding relation for elliptical galaxies the least massive galaxies sigma 80 kms significantly deviate from this relation we combine this vcsigma correlation with the wellknown mbhsigma relation to obtain a tight correlation between circular velocity and supermassive black hole mass and interpret this as observational evidence for a close link between supermassive black holes and the dark matter haloes in which they presumably formed apart from being an important ingredient for theoretical models of galaxy formation and evolution the relation between mbh and circular velocity has the potential to become an important practical tool in estimating supermassive black hole masses in spiral galaxies,http://arxiv.org/abs/astro-ph/0404465v1,0
while there exist various candidates the identification of dark matter remains unresolved recently it was argued that the generalized uncertainty principle gup may prevent a black hole from evaporating completely and as a result there should exist a plancksize bhr at the end of its evaporation we speculate that the stability of bhr may be further protected by supersymmetry in the form of extremal black hole if this is indeed the case and if a sufficient amount of small black holes can be produced in the early universe then the resultant bhrs can be an interesting candidate for dm we demonstrate that this is the case in the hybrid inflation model by assuming bhr as dm our notion imposes a constraint on the hybrid inflation potential we show that such a constraint is not finetuned possible observational signatures are briefly discussed,http://arxiv.org/abs/astro-ph/0406514v1,0
we show that the hierarchical assembly of cold dark matter cdm haloes preserves the memory of the initial conditions using nbody cosmological simulations we demonstrate that the presentday spatial distribution and kinematics of objects that formed within early z 10 protogalactic systems old stars satellite galaxies globular clusters massive black holes etc depends mostly on the rarity of the peak of the primordial density field which they originally belonged to only for objects forming at lower redshifts the exact formation site within the progenitor halo eg whether near the center or in an extended disk becomes important in presentday haloes material from the rarer early peaks is more centrally concentrated and falls off more steeply with radius compared to the overall mass distribution it has a lower velocity dispersion moves on more radial orbits and has a more elongated shape population ii stars that formed within protogalactic haloes collapsing from 25 sigma fluctuations would follow today a r35 density profile with a halflight radius of 17 kpc and a velocity anisotropy that increases from isotropic in the inner regions to nearly radial at the halo edge this agrees well with the radial velocity dispersion profile of galaxy halo stars from battaglia et al 2005 and with the anisotropic orbits of nearby halo stars,http://arxiv.org/abs/astro-ph/0506615v4,0
the annihilation of neutralino dark matter may result in observable signals in different wavelength in the present paper we will discuss the effect of neutralino annihilation in the halo of our galaxy and in its center according to high resolution cold dark matter simulations large virialized halos are formed through the constant merging of smaller halos appeared at previous times at each epoch dark matter halos have then a clumpy component which is made of these merging subhalos the annihilation of dark matter in these clumps always present in the halo of our galaxy may be responsible for appreciable fluxes of gammarays potentially detectable we find that depending on the fundamental parameters of the clump density profile and on the distribution of clumps in the galactic halo the contribution to the diffuse gammaray background from clumps could be used to obtain constraints on the neutralino properties such as mass and annihilation cross section on the other hand the annihilation of neutralino dark matter in the galactic center may result in radio signals at the galactic center infact the accretion flow onto the central black hole sustains strong magnetic fields that can induce synchrotron emission in the radio wavelength by electrons and positrons generated in neutralino annihilations during advection onto the black hole we find that the observed emission from the galactic center is consistent with neutralinos following a navarro frenk and white density profile at the galactic center while it is inconsistent with the presence of a spike density profile supposed to be generated by the formation history of the central black hole,http://arxiv.org/abs/astro-ph/0405110v1,0
shortened weakly interacting massive particles are often said to be the best dark matter candidates studies have shown however that rather large dark matterphoton or dark matterbaryon interactions could be allowed by cosmology here we address the question of the role of the dark matter interactions in more detail to determine at which extent dark matter has to be necessarily weakly interacting to this purpose we compute the collisional damping and freestreaming lengths of generic interacting dark matter candidates and compare them to the scale of the smallest primordial structures known to exist in the universe we obtain necessary conditions that any candidate must satisfy we point out the existence of new dark matter scenarios and exhibit new damping regimes for example an interacting candidate may bear a similar damping than that of collisionless warm dark matter particles the main difference is due to the dark matter coupling to interacting or even freelypropagating species our approach yields a general classification of dark matter candidates which extends the definitions of the usual cold warm and hot dark matter scenarios when interactions weak or strong are considered,http://arxiv.org/abs/astro-ph/0410591v1,0
we investigate how the ghost condensate reacts to black holes immersed in it a ghost condensate defines a hypersurfaceorthogonal congruence of timelike curves each of which has the tangent vector umugmunupartialnuphi it is argued that the ghost condensate in this picture approximately corresponds to a congruence of geodesics in other words the ghost condensate accretes into a black hole just like a pressureless dust correspondingly if the energy density of the ghost condensate at large distance is set to an extremely small value by cosmic expansion then the latetime accretion rate of the ghost condensate should be negligible the accretion rate remains very small even if effects of higher derivative terms are taken into account provided that the black hole is sufficiently large it is also discussed how to reconcile the black hole accretion with the possibility that the ghost condensate might behave like dark matter,http://arxiv.org/abs/hep-th/0502189v2,0
this work investigates the dark matters structures that form on the smallest cosmological scales we find that the types and abundances of structures which form at approximately earthmass scales are very sensitive to the nature of dark matter we explore various candidates for dark matter and determine the corresponding properties of smallscale structure in particular we discuss possibilities for indirect detection of dark matter through smallscale structure and comment on the potential of these methods for discriminating between dark matter candidates,http://arxiv.org/abs/astro-ph/0611864v1,0
dark matter annihilation in socalled spikes near black holes is believed to be an important method of indirect dark matter detection in the case of circular particle orbits the density profile of dark matter has a plateau at small radii the maximal density being limited by the annihilation crosssection however in the general case of arbitrary velocity anisotropy the situation is different particulary for isotropic velocity distribution the density profile cannot be shallower than r12 in the very centre indeed a detailed study reveals that in many cases the term annihilation plateau is misleading as the density actually continues to rise towards small radii and forms a weak cusp rho rbeta12 where beta is the anisotropy coefficient the annihilation flux however does not change much in the latter case if averaged over an area larger than the annihilation radius,http://arxiv.org/abs/0707.3334v2,0
the computation of pbh primordial black hole production from primordial perturbations has recently been improved by considering a more accurate relation between the primordial power spectrum and the pbh mass variance we present here exact expressions which are valid for primordial spectra of arbitrary shape and which allow accurate numerical calculations we then consider the possibility to have a significant part of dark matter in the form of pbhs produced by a primordial spectrum of inflationary origin possessing a characteristic scale we show that in this model the relevant pbh mass is constrained to lie in the range 5times 1015 rm glesssim m lesssim 1021 rm g this is much less than the mass range coming from the qcd phase transition allowing the two mechanisms to be easily distinguished,http://arxiv.org/abs/astro-ph/0203520v2,0
dark matter dm annihilation could in principle contribute to the diffuse cosmic gammaray back ground cgb while with standard assumptions for cosmological and particle physics parameters this contribution is expected to be rather small a number of processes could boost it including a largerthanexpected dm annihilation crosssection or the occurance of dm substructures such as dm minispikes around intermediatemass black holes we show that angular correlations of the cgb provide a tool to disentangle the signal induced by dm annihilation in minispikes from a conventional astrophysical component treating blazars as a known background we study the prospects for detecting dm annihilations with the fermi gammaray space telescope for different choices of dm mass and annihilation channels,http://arxiv.org/abs/0811.4493v1,0
we describe a quasiequilibrium profile of dark matter particles in the inner parsec of the galaxy rho r32 this minicusp profile is caused by scattering with the dense stellar cluster around the supermassive black hole in sgr a and is independent of the initial conditions the implications for detection of gamma rays from wimp dark matter annihilation in the galactic center are a mild enhancement of the flux and a characteristic central feature in the angular distribution which could be detectable by highresolution atmospheric cherenkov telescopes,http://arxiv.org/abs/astro-ph/0308385v2,0
formation and disruption of darkmatter cusps are reviewed accumulation of baryons at the center of a halo can displace the dark matter converting singular density cusps into lowdensity cores the displaced mass can be of order 10 mb with mb the mass of the infalling population if mb is identified with the masses of the black holes currently observed at the centers of bright galaxies predicted core radii are a few hundred pc other mechanisms such as early mass outflow may explain the large darkmatter cores in dwarf and lowsurfacebrightness galaxies predictions of dark matter annihilation radiation from the center of the milky way galaxy are shown to be strongly dependent on the galaxys merger history,http://arxiv.org/abs/astro-ph/0205140v1,0
the prediction that black holes radiate due to quantum effects is often considered one of the most secure in quantum field theory in curved spacetime yet this prediction rests on two dubious assumptions that ordinary physics may be applied to vacuum fluctuations at energy scales increasing exponentially without bound and that quantumgravitational effects may be neglected various suggestions have been put forward to address these issues that they might be explained away by lessons from sonic black hole models that the prediction is indeed successfully reproduced by quantum gravity that the success of the link provided by the prediction between black holes and thermodynamics justifies the prediction this paper explains the nature of the difficulties and reviews the proposals that have been put forward to deal with them none of the proposals put forward can so far be considered to be really successful and simple dimensional arguments show that quantumgravitational effects might well alter the evaporation process outlined by hawking thus a definitive theoretical treatment will require an understanding of quantum gravity in at least some regimes until then no compelling theoretical case for or against radiation by black holes is likely to be made the possibility that nonradiating mini black holes exist should be taken seriously such holes could be part of the dark matter in the universe attempts to place observational limits on the number of mini black holes independent of the assumption that they radiate would be most welcome,http://arxiv.org/abs/gr-qc/0304042v1,0
first results of a numerical investigation of primordial black hole formation in the radiation dominated phase of the early universe are presented the simulations follow the gravitational collapse of three different families of highamplitude density fluctuations imposed at the time of horizon crossing the threshold for black hole formation deltac approx 07 is found to be nearly identical for all perturbation families if the control parameter delta is chosen as the total excess mass within the initial horizon volume furthermore we demonstrate that the scaling of black hole mass with distance from the formation threshold known to occur in nearcritical gravitational collapse applies to primordial black hole formation,http://arxiv.org/abs/astro-ph/9806043v1,0
the existence of quasars at redshift z 5 indicates that supermassive black holes were present since the very early times if they grew by accretion the seeds of mass 105 msun must have formed at z 9 these seed black holes may result from the collapse and dissipation of primordial gas during the early stages of galaxy formation i discuss the effects of magnetic fields on the fragmentation of cold gas clouds embedded into a hot diffuse phase and a virialized dark matter halo the field of 104 g ejected by supernova remnants can halt cloud breakup at 104 msun high star formation rates keep the clouds partially ionized making ambipolar diffusion inefficient the magneticallysupported clouds collapse into black holes which later spiral via dynamical friction into a central cluster with the total mass mbh 6 106 msun as the cluster collapses the black holes merge emitting gravitational radiation that should be detectable by lisa,http://arxiv.org/abs/astro-ph/0108070v1,0
we search for nearby isolated accreting stellarmass 3 to 100modot black holes models suggest a synchrotron spectrum in visible wavelengths and some emission in xray wavelengths of 37 million objects in the sloan digital sky survey early data release about 150000 objects have colors and properties consistent with such a spectrum and 87 of these objects are xray sources from the rosat all sky survey thirtytwo of these have been confirmed not to be blackholes using optical spectra we give the positions and colors of these 55 blackhole candidates and quantitatively rank them on their likelihood to be black holes we discuss uncertainties the expected number of sources and the contribution of blackholes to local dark matter,http://arxiv.org/abs/astro-ph/0205138v2,0
we investigate the recently proposed hybrid inflation models with two stages of inflation we show that quantum fluctuations at the time corresponding to the phase transition between the two inflationary stages can trigger the formation of a large number of inflating topological defects in order to study density perturbations in these models we develop a new method to calculate density perturbations in a system of two scalar fields we show that density perturbations in hybrid inflation models of the new type can be very large on the scale corresponding to the phase transition the resulting density inhomogeneities lead to a copious production of black holes this could be an argument against hybrid inflation models with two stages of inflation however we find a class of models where this problem can be easily avoided the number of black holes produced in these models can be made extremely small but in general it could be sufficiently large to have important cosmological and astrophysical implications in particular for certain values of parameters these black holes may constitute the dark matter in the universe it is also possible to have hybrid models with two stages of inflation where the black hole production is not suppressed but where the typical masses of the black holes are very small such models lead to a completely different thermal history of the universe where postinflationary reheating occurs via black hole evaporation,http://arxiv.org/abs/astro-ph/9605094v3,0
blue primordial power spectra have spectral index n1 and arise naturally in the recently proposed hybrid inflationary scenario an observational upper limit on em n is derived by normalizing the spectrum at the quadrupole scale and considering the possible overproduction of planck mass relics formed in the final stage of primordial black hole evaporation in the inflationary universe with the maximum reheating temperature compatible with the observed quadrupole anisotropy the upper limit is n14 but it is slightly weaker for lower reheat temperatures this limit applies over 57 decades of mass and is therefore insensitive to cosmic variance and any gravitational wave contribution to the quadrupole anisotropy it is also independent of the dark matter content of the universe and therefore the bias parameter in some circumstances there may be an extended dustlike phase between the end of inflation and reheating in this case primordial black holes form more abundantly and the upper limit is n13,http://arxiv.org/abs/astro-ph/9405027v1,0
the origin of supermassive black holes in the galactic nuclei is quite uncertain in spite of extensive set of observational data we review the known scenarios of galactic and cosmological formation of supermassive black holes the common drawback of galactic scenarios is a lack of time and shortage of matter supply for building the supermassive black holes in all galaxies by means of accretion and merging the cosmological scenarios are only fragmentarily developed but propose and pretend to an universal formation mechanism for supermassive black holes,http://arxiv.org/abs/0709.0070v2,0
we explore the possibility that massive black holes comprise a significant fraction of the dark matter of our galaxy by studying the dissolution of galactic globular clusters bombarded by them in our simulations we evolve the clusters along a sequence of king models determined by changes of state resulting from collisions with the black holes the results divide naturally into regimes of small and large black hole mass small black holes do not destroy clusters in single collisions their effect is primarily cumulative leading to a relation between mbh and fhalo the fraction of the halo in black holes of mass mbh which is fhalombh constant up to logarithmic corrections for fhalo1 we find mbh simless 103 msun by requiring survival of the same clusters studied by moore 1993 who neglected cluster evolution mass loss and stochasticity of energy inputs in his estimates but reached a similar conclusion large black holes may not penetrate a cluster without disrupting it their effect is mainly catastrophic close collisions but also partly cumulative distant collisions in the large mbh limit fhalo but not mbh can be constrained by computing the probability that a cluster survives a combination of close destructive encounters and distant nondestructive encounters we find that it is unlikely that fhalo simgreat 03 by requiring 50 per cent survival probability for moores clusters over 1010 years,http://arxiv.org/abs/astro-ph/9811370v1,0
we investigate how central black holes bhs inhabited in galactic dark halos could affect strong gravitational lensing the distribution of integral lensing probability with image separations are calculated for quasars of redshift 15 by foreground dark matter halos the mass density of dark halos is taken to be the navarrofrenkwhite nfw profile such that when the mass of a halo is less than 1014 msun its central black holes or a bulge is included as a point mass the relationship between the masses mbullet of supermassive black holes and the total gravitational mass mmathrmdm of their host galaxy is adopted from the most recent literature only a flat lambdacdm model is considered here it is shown that while a single black hole for each galaxy contributes considerable but not sufficient lensing probabilities at small image separations compared with those without black holes the bulges which are about 1001000 times larger in mass than a typical black hole would definitely contribute enough probabilities at small image separations although it gives too high probabilities at large separation angles compared with lensing observations,http://arxiv.org/abs/astro-ph/0207268v2,0
black holes with masses mbh1085 msun dominate the accretion history of the universe these black hole masses are typical of those found in radioselected galaxies today suggesting that the giant elliptical hosts of low redshift radio galaxies were the hosts of powerful mostly radioquiet quasars in the high redshift universe the reason that all radio galaxies are found in such hosts may be the correlation of black hole mass with radio luminosity but it is emphasized that accretion rate too plays an important role in the production of powerful radio jets the tight kz relation of luminous high redshift radio galaxies is probably a selection effect due to the selection on high black hole masses and high accretion rates luminous radio galaxies are the radioloud part of the quasar2 population and the ratio of radioloud quasars to luminous radio galaxies about 11 is so far our only good estimate of the relative numbers of quasar1s and quasar2s the numbers of radioquiet and radiointermediate quasar2s are still uncertain but a much larger population than the quasar1s would conflict with constraints from the presentday black hole mass density a comparison of the number densities of dark matter haloes and the high redshift quasars however suggests that there are plenty of dark haloes capable of hosting the known high redshift agn and thus room for a significant but not huge quasar2 population,http://arxiv.org/abs/astro-ph/0303348v1,0
recent observations of nearinfrared and xray flares from sagittarius a which is believed to be a supermassive black hole at the galactic center show that the source exhibits about 20minute periodic variability here we provide arguments based on a quantitative analysis that supermassive objects at galactic centers may be bubbles of dark matter axions rather than black holes an oscillating axion bubble can explain periodic variability of sagittarius a and yields the axion mass about 06 mev which fits in the open axion mass window the bubble scenario with no other free parameters explains lack of supermassive black holes with mass m106 msun lowmass bubbles decay fast and as a result are very rare we also found that the mass of an axion bubble can not exceed 15times 109 msun in agreement with the upper limit on the supermassive black hole mass obtained from observations our finding if confirmed suggests that einstein general relativity is invalid for strong gravity and the gravitational field for the bubble effectively becomes repulsive at large potential imaging a shadow of the black hole at the galactic center with vlbi in the next decade can distinguish between the black hole and the oscillating axion bubble scenarios in the case of axion bubble a steady shadow will not be observed instead the shadow will appear and disappear periodically with a period of about 20 min,http://arxiv.org/abs/astro-ph/0607179v2,0
we present three reasons for the formation of gravitational bound states of primordial black holescalled holeumsin the early universeusing newtonian gravity and nonrelativistic quantum mechanics we find a purely quantum mechanical massdependant exclusion property for the nonoverlap of the constituent black holes in a holeumthis ensures that the holeum occupies space just like ordinary mattera holeum emits only gravitational radiation whose spectrum is an exact analogue of that of a hydrogen atom a part of this spectrum lies in the region accessible to the detectors being builtthe holeums would form haloes around the galaxies and would be an important component of the dark matter in the universe todaythey may also be the constituents of the invisible domain walls in the universe,http://arxiv.org/abs/gr-qc/0308054v1,0
if supermassive black holes smbhs are the energy sources that power quasars and active galactic nuclei then qso sdss 11485251 the quasar with the highest redshift zqso643 hosts a supermassive black hole formed within 09 gyr after the big bang this requirement places constraints on the cosmological formation of smbhs believed to grow from smaller initial seeds by a combination of accretion and mergers we focus on gas accretion onto seeds produced by the collapse of pop iii stars at high redshift we incorporate the results of recent relativistic mhd simulations of disk accretion onto kerr black holes to track the coupled evolution of the masses and spins of the holes we allow for an additional amplification of 104 in the mass of a typical seed due to mergers consistent with recent monte carlo simulations of hierarchical mergers of cold dark matter halos containing black hole seeds we find that the growth of pop iii black hole seeds to 109 msun by zqso 643 favors mhd accretion disks over standard thin disks mhd disks tend to drive the holes to a submaximal equilibrium spin rate am 095 and radiation efficiency epsilonm 02 while standard thin disks drive them to maximal spin am 1 and efficiency epsilonm 042 this small difference in efficiency results in a huge difference in mass amplification by accretion at the eddington limit the mhd equilibrium efficiency is consistent with the observed ratio of the qso plus agn luminosity density to the local smbh mass density our prototype analysis is designed to stimulate the incorporation of results from relativistic stellar collapse and mhd accretion simulations in future monte carlo simulations of hierarchical structure formation to better determine the cosmological role of smbhs and their mass and spin distributions,http://arxiv.org/abs/astro-ph/0411156v1,0
we investigate the possibility that presentday galaxies and their dark matter haloes contain a population of massive black holes mbhs that form by hierarchical merging of the black hole remnants of the first stars in the universe some of the mbhs may be large enough or close enough to the centre of the galactic host that they merge within a hubble time we estimate to what extent this process could contribute to the mass of the supermassive black holes smbhs observed in galactic centres today the relation between smbh and galactic bulge mass in our model displays the same slope as that found in observations many mbhs will not reach the centre of the host halo however but continue to orbit within it in doing so mbhs may remain associated with remnants of the satellite halo systems of which they were previously a part using a semianalytical approach that explicitly accounts for dynamical friction tidal disruption and encounters with galactic disks we follow the hierarchical merging of mbh systems and their subsequent dynamical evolution inside the respective host haloes we predict the mass and abundance of mbhs in presentday galactic haloes and also estimate the mbh mass accretion rates as well as bolometric luminosities for two different accretion scenarios mbhs that have not undergone recent merging will retain associated dark matter cusps that were enhanced by black hole accretion growth and may be possible sources of gamma rays via neutralino annihilations,http://arxiv.org/abs/astro-ph/0307171v1,0
we determine the total enclosed mass profile from 07 to 35 kpc in the elliptical galaxy ngc 4636 based on the hot interstellar medium temperature profile measured using the chandra xray observatory and other xray and optical data the total mass increases as radius to the power 12 to a good approximation over this range in radii attaining a total of 15 trillion solar masses corresponding to a masstolight ratio of 40 solar masses per solar visual luminosity at 35 kpc we find that at least half and as much as 80 of the mass within the optical halflight radius is nonluminous implying that ngc 4636 has an exceptionally low baryon fraction the large inferred dark matter concentration and central dark matter density consistent with the upper end of the range expected for standard cold dark matter halos imply that mechanisms proposed to explain low dark matter densities in less massive galaxies eg selfinteracting dark matter warm dark matter explosive feedback are not effective in elliptical galaxies and presumably by extension in galaxy clusters the composite black hole stars and dark matter mass distribution has a generally steep slope with no core consistent with gravitational lensing studies,http://arxiv.org/abs/astro-ph/0208090v1,0
the babichevdokuchaeveroshenko model for the accretion of dark energy onto black holes has been extended to deal with black holes with nonstatic metrics the possibility that for an asymptotic observer a black hole with large mass will rapidly increase and eventually engulf the universe at a finite time in the future has been studied by using reasonable values for astronomical parameters it is concluded that such a phenomenon is forbidden for all black holes in quintessential cosmological models,http://arxiv.org/abs/astro-ph/0603761v1,0
in light of recent evidence suggesting a nonzero presentday cosmological constant adams mbonye laughlin 1999 have considered the evolution of black holes in the presence of vacuum energy using the assumption that lambda remains constant with time and a conjecture based on a paper by mallett 1986 they reach the remarkable conclusion that black holes with current mass greater than about 2109 msun will not hawking evaporate in the distant future but will instead absorb vacuum energy and grow to roughly the de sitter horizon size in this letter we reexamine black hole evaporation in the presence of vacuum energy and find instead that all known black holes will eventually evaporate,http://arxiv.org/abs/astro-ph/0008260v1,0
we consider the cosmological model in which a part of the universe omegahsim 105 is in the form of primordial black holes with mass sim 105modot these primordial black holes would be centers for growing protogalaxies which experienced multiple mergers with ordinary galaxies this process of galaxies formation is accompanied by the merging of central black holes in the galactic nuclei it is shown that recently discovered correlations between the central black holes and bulges of galaxies are naturally reproduced in this scenario,http://arxiv.org/abs/astro-ph/0202019v1,0
a reliable method for estimating the blackhole masses of highredshift quasars would provide crucial new information for understanding the nature and cosmological evolution of quasars in this proceedings we summarize the results of our recent paper mclure jarvis 2002 which provides a virial blackhole mass estimator based on restframe uv observables thereby allowing reliable blackhole mass estimates to be obtained out to redshifts of zsim25 from optical spectra alone,http://arxiv.org/abs/astro-ph/0302457v1,0
based on a high resolution cosmological nbody simulation we track the hierarchical growth of black holes in galaxy clusters from z20 to z0 we present a census of black holes as function of redshift and will determine their mass assembly history under a variety of assumptions regarding the importance of gas accretion in black hole growth from early supercritical eddington accretion to gaspoor hierarchical assembly following a galaxy merger black holes are expected to form inspiral and merge after strongly radiating energy via gravitational waves for each binary black hole inspiral and merger we determine the expected gravitational wave signal for the laser interferometer space antenna lisa and calculate the lisa event rate as a function of time we will calculate the black hole mass assembly history for several black hole growth scenerios so that we can explore tests to characterize each model observationally in particular we will study how well lisa observations will be able to distinguish between these very different assembly scenarios,http://arxiv.org/abs/astro-ph/0608493v2,0
eliptical and bulge galaxies share a tight correlation of velocity distribution to both luminosity and black hole mass there are similar orbital speeds for all galaxies of a given luminosity including dark matter dm at large radii the halo surface density of dm is constant for almost all types of galaxies and ranges 14 mag down to dwarf spherical galaxies there are supermassive black holes or giant pure disk galaxies at high redshift inexplicable with hierarchical clustering or collapse dynamics these and a myriad of other galaxy formation problems are explainable by an initial shell which caused the planck cosmic microwave background radiation a reduction in the energydensity of primordial galactic black holes is necessary to explain dark energy,http://arxiv.org/abs/astro-ph/9904320v2,0
there are multiple examples of gravitational losses in neutron stars and black holes protons and neutrons have been found to have enormous repulsive pressures that highly squeezed collapsing matter cannot overcome the case against singularities follows galactic black hole gravitational losses can supply the missing dark energy with highly squeezed nucleons the big bang could begin as a hot core and a cold dark matter shell the 21 cm radiation data has identified baryon sized particles as cold dark matter highly squeezed nucleons will not decompose to produce antimatter the flatness of the universe is due to a baryonic bounce the highly correlated galaxies originated from primordial black holes capturing hot core gassesthere is evidence that galaxies have not grown nor merged significantly since formation,http://arxiv.org/abs/astro-ph/0008166v8,0
the influence of dark matter dm on the growth of supermassive black holes smbhs in galaxies is studied it is shown that gravitational scattering of dm particles on bulge stars leads to diffusion of dm in phase space m mz i m denotes the angular momentum and i is the radial action appropriate diffusion coefficients are calculated for different bulge models and it is argued that the diffusion along m axis is the most important effect it is shown that this process leads to noticeable flow of dm into the black hole bh resulting in its powerlaw growth mbh t916 comparison with observational data shows that in principle this effect may explain observed masses of smbhs special attention is paid to the corrections related to the innermost region of bh gravitational influence and the diffusion along i axis their influence on the bh growth law is shown to be negligible in most cases,http://arxiv.org/abs/astro-ph/0307524v2,0
we investigate the relation between the dark matter distribution of galaxies and their central supermassive black holes which is suggested by the vcsigma relation since earlytype galaxies appear to have larger black holes than latetype ones we look for an equivalent pattern in the dark matter distribution as a function of hubble type to achieve our goal we use a stateoftheart modelling code that allows a variety of geometries to be fitted to a combination of radio and optical observations of galaxies with different morphology,http://arxiv.org/abs/astro-ph/0412123v1,0
intermediate mass black holes imbhs are candidates to seed the supermassive black holes smbhs and some could still wander in the galaxy in the context of annihilating dark matter dm they are expected to drive huge annihilation rates and could therefore significantly enhance the primary cosmic rays crs expected from annihilation of the dm of the galactic halo in this proceeding the original paper is brun et al 2007 we briefly explain the method to derive estimates of such exotic contributions to the antiproton and positron cr spectra and the associated statistical uncertainties connected to the properties of imbhs we find boost factors of order 104 to the exotic fluxes but associated with very large statistical uncertainties,http://arxiv.org/abs/0712.0677v1,0
the hypothesis that dark matter consists of superheavy particles with the mass close to the grand unification scale is investigated these particles were created from vacuum by the gravitation of the expanding universe and their decay led to the observable baryon charge some part of these particles with the lifetime larger than the time of breaking of the grand unification symmetry became metastable and survived up to the modern time as dark matter however in active galactic nuclei due to large energies of dark matter particles swallowed by the black hole the opposite process can occur dark matter particles become interacting their decay on visible particles at the grand unification energies leads to the flow of ultra high energy cosmic rays observed by the auger group numerical estimates of the effect leading to the observable numbers are given,http://arxiv.org/abs/0712.2667v1,0
many lines of evidence suggest that nonbaryonic dark matter constitutes roughly 30 of the critical closure density but the composition of this dark matter is unknown one class of candidates for the dark matter is compact objects formed in the early universe with typical masses m between 01 and 1 solar masses to correspond to the mass scale of objects found with microlensing observing projects specific candidates of this type include black holes formed at the epoch of the qcd phase transition quark stars and boson stars here we show that accretion onto these objects produces substantial ionization in the early universe with an optical depth to thomson scattering out to z1100 of approximately tau24 fcoepsilon1mmsun12 h0651 where epsilon1 is the accretion efficiency epsilonequiv ldot mc2 divided by 01 and fco is the fraction of matter in the compact objects the current upper limit to the scattering optical depth based on the anisotropy of the microwave background is approximately 04 therefore if accretion onto these objects is relatively efficient they cannot be the main component of nonbaryonic dark matter,http://arxiv.org/abs/astro-ph/0003176v1,0
the glast large area telescope scheduled for launch in 2006 is a next generation space based gamma ray telescope which will improve in point source sensitivity by a factor of 30 over that of egret below 10 gev and extend beyond egret up to 300 gev thus glast offers a unique opportunity to discover wimp dark matter through precision studies of gamma rays produced in pair annihilations the most dense region of dark matter in our galaxy is currently thought to occur at the center in particular dark matter should concentrate within 3 pc of the putative supermassive black hole located at the sgra radio source in fact the 2nd and 3rd egret catalogs contain a significant point source coincident with the milky way galactic center within a resolution of 12 arcminutes the egret team has determined that the spectral and temporal characteristics of this point source are consistent with dark matter wimp annihilations more detailed analysis has determined that the magnitude and spectrum of the egret source is consistent with relic wimps concentrated within 3 pc of the central supermassive black hole furthermore the sgra radio emission is consistent with the synchrotron radiation expected from electrons and positrons produced in wimp annihilations if true then glast should be able to constrain the particle properties of the postulated wimp with 1 month of data,http://arxiv.org/abs/astro-ph/0209121v1,0
in the first two of these lectures i present the evidence for baryonic dark matter and describe possible forms that it may take the final lecture discusses formation of baryonic dark matter and sets the cosmological context,http://arxiv.org/abs/astro-ph/9407024v1,0
the idea that dark matter in the form of small compact bodies most plausibly planetary mass primordial black holes betrays its presence by the microlensing of quasars has been stimulated by statistical studies of quasar light curves and gravitationally lensed quasar systems in this paper we review this evidence and discuss the significance of the recent detection of a planetary mass microlensing event in the galaxy by the macho group we also look in more detail at the light curves themselves and make the case that many show the characteristic morphology of caustic crossing events associated with a large optical depth of lenses this supports the idea that a large fraction of the critical density is in the form of planetary mass compact bodies the nature of such bodies is discussed and it is argued that current work on the qcd phase transition suggests they are primordial black holes although it is just possible that they are compact gas clouds,http://arxiv.org/abs/astro-ph/9811019v1,0
virial blackhole mass estimates are presented for 12698 quasars in the redshift interval 01z21 based on modelling of spectra from the sloan digital sky survey sdss first data release the blackhole masses of the sdss quasars are found to lie between simeq107msun and an upper limit of simeq 3times 109msun entirely consistent with the largest blackhole masses found to date in the local universe the estimated eddington ratios of the broadline quasars fwhmgeq 2000 kms1 show a clear upper boundary at lbolleddsimeq 1 suggesting that the eddington luminosity is still a relevant physical limit to the accretion rate of luminous broadline quasars at zleq 2 by combining the blackhole mass distribution of the sdss quasars with the 2df quasar luminosity function the number density of active black holes at zsimeq 2 is estimated as a function of mass in addition we independently estimate the local blackhole mass function for earlytypes using the mbhsigma and mbhlbulge correlations based on the sdss velocity dispersion function and the 2mass kband luminosity function both estimates are found to be consistent at the highmass end mbhgeq 108msun by comparing the estimated number density of active black holes at zsimeq 2 with the local mass density of dormant black holes we set lower limits on the quasar lifetimes and find that the majority of black holes with mass geq 1085msun are in place by simeq 2,http://arxiv.org/abs/astro-ph/0310267v2,0
we demonstrate that the luminosity function of the recently detected population of starforming galaxies and the qso luminosity function at z3 can be matched with the mass function of dark matter haloes predicted by hierarchical cosmogonies for lifetimes of optically bright qsos in the range 106 to 108 yr we suggest that the mass of supermassive black holes may be limited by the backreaction of the emitted energy on the accretion flow in a selfgravitating disc this would imply a relation of black hole to halo mass of the form mbh vhalo5 and a typical duration of the optically bright qso phase of a few times 107 yr the high integrated mass density of black holes inferred from recent black hole mass estimates in nearby galaxies may indicate that the overall efficiency of supermassive black holes for producing blue light is smaller than previously assumed we discuss three possible accretion modes with low optical emission efficiency i accretion at far above the eddington rate ii accretion obscured by dust and iii accretion below the critical rate leading to an advection dominated accretion flow lasting for a hubble time we further argue that accretion with low optical efficiency might be closely related to the origin of the hard xray background and that the ionizing background might be progressively dominated by stars rather than qsos at higher redshift,http://arxiv.org/abs/astro-ph/9712259v1,0
this paper reviews the subject of intermediatemass black holes imbhs with masses between those of stellarmass and supermassive black holes the existence of imbhs is a real possibility they might plausibly have formed as remnants of the first generation of stars population iii as the result of dense star cluster evolution or as part of the formation process of supermassive black holes their cosmic mass density could exceed that of supermassive black holes omega 1057 and observations do not even rule out that they may account for all of the baryonic dark matter in the universe omega 1017 unambiguous detections of individual imbhs currently do not exist but there are observational hints from studies of microlensing events ultraluminous xray sources and centers of nearby galaxies and globular clusters gravitational wave experiments will soon provide another method to probe their existence imbhs have potential importance for several fields of astrophysics and are likely to grow as a focus of research attention,http://arxiv.org/abs/astro-ph/0302101v2,0
gravitational lensing is a well known phenomenon predicted by the general theory of relativity it is now a welldeveloped observational technique in astronomy and is considered to be a fundamental tool for acquiring information about the nature and distribution of dark matter in particular gravitational lensing experiments may be used to search for black holes it has been proposed that a schwarzschild black hole may act as a retrolens holz wheeler citehw which if illuminated by a powerful light source eg the sun deflects light ray paths to large bending angles so that the light may reach the observer here by considering the strong field limit in the deflection angle and confining our analysis to the black hole equatorial plane we extend the holzwheeler results to slowly spinning kerr black holes by considering the holzwheeler geometrical configuration for the lens source and observer we find that the inclusion of rotation does not substantially change the brightness of the retrolensing images with respect to the schwarzschild case we also discuss the possibility that the next generation spacebased telescopes may detect such retroimages and eventually put limits on the rotational parameter of the black hole,http://arxiv.org/abs/astro-ph/0401384v1,0
large dynamic range numerical simulations of atomic cooling driven collapse of gas in pregalactic dm haloes with tvir 10000 k show that the gas loses 90 and more of its angular momentum before rotational support sets in in a fraction of these haloes where the metallicity is low and uv radiation suppresses h2 cooling conditions are thus very favourable for the rapid buildup of massive black holes depending on the progression of metal enrichment the continued suppression of h2 cooling by external and internal uv radiation and the ability to trap the entropy produced by the release of gravitational energy the gas at the centre of the halo is expected to form a supermassive star a stellarmass black hole accreting at supereddington accretion rates or a compact starcluster undergoing collisional runaway of massive stars at its centre in all three cases a massive black hole of initially modest mass finds itself at the center of a rapid inflow of gas with inflow rates of 1 msolaryr the massive black hole will thus grow quickly to a mass of 105 to 106 msolar until further inflow is halted either by consumption of gas by star formation or by the increasing energy and momentum feedback from the growing massive black hole conditions for the formation of massive seed black holes in this way are most favourable in haloes with tvir 15000 k and vvir 20 kms with less massive haloes not allowing collapse of gas by atomic cooling and more massive haloes being more prone to fragmentation this should imprint a characteristic mass on the mass spectrum of an early population of massive black hole seeds in pregalactic haloes which will later grow into the observed population of supermassive black holes in galactic bulges,http://arxiv.org/abs/0810.2802v1,0
we investigate the spatial clustering properties of primordial black holes pbhs with minimal assumptions we show that pbhs created in the radiation era are highly clustered using the peaks theory model of bias we compute the pbh twopoint correlation function and power spectrum for creation from an initially adiabatic power spectrum of perturbations the pbh power spectrum contains both isocurvature and adiabatic components the absence of observed isocurvature fluctuations today constrains the mass range in which pbhs may serve as dark matter we briefly discuss other consequences of pbh clustering,http://arxiv.org/abs/astro-ph/0509141v2,0
we study a double inflation model a hybrid inflation a new inflation in supergravity and discuss the formation of primordial black holes pbhs with mass sim 1020105modot we find that in a wide range of parameter space we obtain pbhs which amount to omega simeq 1 ie pbh dark matter also we find a set of inflation parameters which produces pbhs evaporating now those pbhs may be responsible for antiproton fluxes observed by the bess experiment,http://arxiv.org/abs/hep-ph/0002236v2,0
dark matter has been recognized as an essential part of matter for over 70 years now and many suggestions have been made what it could be most of these ideas have centered on cold dark matter particles that are expected in extensions of standard particle physics such as supersymmetry here we explore the concept that dark matter is sterile neutrinos a concept that is commonly referred to as warm dark matter such particles have kev masses and decay over a very long time much longer than the hubble time in their decay they produce xray photons which modify the ionization balance in the early universe increasing the fraction of molecular hydrogen and thus help early star formation sterile neutrinos may also help to understand the baryonasymmetry the pulsar kicks the early growth of black holes the minimum mass of dwarf spheroidal galaxies as well as the shape of dark matter halos as soon as all these tests have been quantitative in its various parameters we may focus on the creation mechanism of these particles and could predict the strength of the sharp xray emission line expected from any large dark matter assembly a measurement of this xray emission line would be definitive proof for the existence of may be called weakly interacting neutrinos or wins,http://arxiv.org/abs/astro-ph/0702164v1,0
dark matter has been recognized as an essential part of matter for over 70 years now and many suggestions have been made what it could be most of these ideas have centered on cold dark matter particles that are predicted in extensions of standard particle physics such as supersymmetry here we explore the concept that dark matter is sterile neutrinos particles that are commonly referred to as warm dark matter such particles have kev masses and decay over a very long time much longer than the hubble time in their decay they produce xray photons which modify the ionization balance in the very early universe increasing the fraction of molecular hydrogen and thus help early star formation sterile neutrinos may also help to understand the baryonasymmetry the pulsar kicks the early growth of black holes the minimum mass of dwarf spheroidal galaxies as well as the shape and smoothness of dark matter halos as soon as all these tests have been made quantitative in their various parameters we may focus on the creation mechanism of these particles and could predict the strength of the sharp xray emission line expected from any large dark matter assembly a measurement of this xray emission line would be definitive proof for the existence of may be called weakly interacting neutrinos or wins,http://arxiv.org/abs/astro-ph/0702173v1,0
hawking has shown that if black holes were to exist in a universe that expands forever black holes would completely evaporate violating unitarity i argue this means unitarity requires that the universe exist for only a finite future proper time i develop this argument showing that unitarity also requires the boundaries of all future sets to be cauchy surfaces and so no event horizons can exist thus the null generators of the surfaces of astrophysical black holes must leave the surface in both time directions allowing nonspherical topologies for black hole surfaces since all information eventually escapes astrophysical black holes and since the null surfaces defining astrophysical black holes are cauchy surfaces holography automatically holds i further show that unitarity requires the effective cosmological constant to be zero eventually since otherwise the universe would expand forever,http://arxiv.org/abs/astro-ph/0104011v1,0
these lectures on nonbaryonic dark matter matter are divided into two parts in the first part i discuss the need for nonbaryonic dark matter in light of recent results in cosmology and i present some of the most popular candidates for nonbaryonic dark matter these include neutrinos axions neutralinos wimpzillas etc in the second part i overview several observational techniques that can be employed to search for wimps weakly interacting massive particles as nonbaryonic dark matter among these techniques i discuss the direct detection of wimp dark matter and its indirect detection through highenergy neutrinos gammarays positrons etc,http://arxiv.org/abs/astro-ph/0403064v1,0
we discuss the process of adiabatic growth of central black holes in the presence of a stationary preexisting distribution of collisionless stars within the limitations of the assumptions the resulting models make robust physical predictions for the presence of a central cusp in the stellar and dark matter density a keplerian rise in the velocity dispersion and a significant tangential polarization of the velocity tensor new generations of numerical models have confirmed and extended previous results permit the study of axisymmetric and triaxial systems and promise new insight into the dynamics of the central regions of galaxies these studies enable detailed comparisons with observations further our understanding on the fueling processes for agns and quiescent black holes and help elucidate the secular evolution of the inner regions and spheroids of galaxies,http://arxiv.org/abs/astro-ph/0303311v1,0
vacuum polarization in a strong field such as hawking radiation from black holes is constrained by the directional metric of strong field vectors relative localization of the quanta and the schwarzschild radius is examined concept of thermal equilibrium of excited vacuum is critically analyzed and judged as disequilibrium because virtual modes or particles are mutually independent the result shows that primordial microsize black holes pmbhs cannot produce much hawking radiation via fermions bosons or thermal photons primordial black holes produced in the early epoch of the big bang should be very stable over hubble time and consequently may constitute a significant part of the cold dark matter in the structural formation of the universe pmbhs should be gravitationally highlycondensed as missing mass around galaxies and galaxy clusters and remain very difficult to observe,http://arxiv.org/abs/astro-ph/0407440v1,0
the relationships between supermassive black holes and the properties of their associated darkmatter halos imply that outflows from accreting black holes provide a feedback mechanism regulating galaxy formation accreting black holes with weak or undetectable radio jets radioquiet quasars outnumber those with powerful jets radioloud quasars by a factor 10100 so powerfuljet outflows are often neglected however whenever powerful jets are triggered there is a dramatic factor 100 stepfunction increase in the efficiency of feedback we use a feedback model together with the measured space density of flatspectrum radioloud quasars to show that a powerfuljet episode probably occurred in every protocluster in the universe before jet triggering there was time for gravitational collapse to create many 10100 surrounding protogalaxies massive enough to host radioquiet quasars after triggering the powerful jet pushes back and heats ionized gas so that it cannot fall onto these protogalaxies and cool once neutralmolecular gas reservoirs become exhausted there is a synchronized shut down in both starformation and blackhole activity throughout the protocluster these considerations imply that radioloud quasars have a profound influence on the evolution of all the galaxies seen in clusters today,http://arxiv.org/abs/astro-ph/0409687v1,0
a variety of observational approaches have provided evidence for extended halos of dark matter surrounding elliptical galaxies and for massive dark objects in some of their nuclei what are the properties of the dark halos how do they relate to the parameters of the luminous matter what is the demographics of massive central black holes and what is their dynamical role in shaping the host galaxy recent work in this area is described with special attention to the construction of stateoftheart dynamical models that incorporate all available observational constraints,http://arxiv.org/abs/astro-ph/9704095v1,0
we present new velocity dispersion measurements for a set of 12 spiral galaxies and use them to derive a more accurate vc sigma relation which holds for a wide morphological range of galaxies combined with the mbh sigma relation this relation can be used as a tool to estimate supermassive black hole smbh masses by means of the asymptotic circular velocity together with the tullyfisher relation it serves as a constraint for galaxy formation and evolution models,http://arxiv.org/abs/astro-ph/0311579v1,0
we present results from our magnetohydrodynamical simulations of accretion flows onto black holes our main focus is the interplay between inflows and related outflows we consider applications of such flows to the galatic center and low luminosity active galactic nuclei,http://arxiv.org/abs/astro-ph/0409475v1,0
we analyzed a sample of high and low surface brightness hsb and lsb disc galaxies and elliptical galaxies to investigate the correlation between the circular velocity vc and the central velocity dispersion sigma we better defined the previous vcsigma correlation for hsb and elliptical galaxies especially at the lower end of the sigma values elliptical galaxies with vc based on dynamical models or directly derived from the hi rotation curves follow the same relation as the hsb galaxies in the vsigma plane on the contrary the lsb galaxies follow a different relation since most of them show either higher vc or lower sigma with respect to the hsb galaxies this argues against the relevance of baryon collapse in the radial density profile of the dark matter haloes of lsb galaxies moreover if the vcsigma relation is equivalent to one between the mass of the dark matter halo and that of the supermassive black hole these results suggest that the lsb galaxies host a supermassive black hole with a smaller mass compared to hsb galaxies of equal dark matter halo on the other hand if the fundamental correlation of smbh mass is with the halo vc then lsbs should have larger black hole masses for given bulge sigma,http://arxiv.org/abs/astro-ph/0512640v1,0
this work considers the idea of massive black holes being the constituents of the galactic dark matter halo it constrains the maximum black hole mass to mbh sil 5 times 104 msun by examining their influence on the population of globular clusters in our milky way in the adopted halo model globular clusters are exposed to constant bombardment of halo objects on their orbits through the galaxy and thus will steadily gain internal energy depending on the mass of these halo objects and the structural parameters of the globular clusters they can be disrupted on time scales of a few billion years and below these disruption time scales are calculated using a modification of the well known classical impulsive approximation and compared with direct nbody simulations of such encounter events to ensure the method works correctly for a set of ten prototypical globular cluster models and black hole masses ranging from 103 to 107msun montecarlosimulations of 10000 encounter histories over the period of 10 billion years were calculated each at three different galactocentric distances mboxr 5 10 and 15 kpc these data were compared with the real globular cluster population in our galaxy and used to obtain the above constraint of mbh sil 5 times 104 msun,http://arxiv.org/abs/astro-ph/9511032v1,0
we have obtained deep multiband imaging observations around three of the most distant known quasars at redshifts z6 standard accretion theory predicts that the supermassive black holes present in these quasars were formed at a very early epoch if a correlation between black hole mass and dark matter halo mass is present at these early times then these rare supermassive black holes will be located inside the most massive dark matter halos these are therefore ideal locations to search for the first clusters of galaxies we use the lymanbreak technique to identify starforming galaxies at high redshifts our observations show no overdensity of starforming galaxies in the fields of these quasars the lack of dustfree luminous starburst companions indicates that the quasars may be the only massive galaxies in their vicinity undergoing a period of intense activity,http://arxiv.org/abs/astro-ph/0410306v1,0
we investigate the relation between the mass of supermassive black holes mbh in qsos and the mass of the dark matter halos hosting them mdh we measure the widths of broad emission lines mgii lambda 2798 civ lambda 1549 from qso composite spectra as a function of redshift these widths are then used to determine virial black hole mass estimates we compare our virial black hole mass estimates to dark matter halo masses for qso hosts derived by croom et al 2005 based on measurements of qso clustering this enables us to trace the mbhmdh relation over the redshift range z05 to 25 we calculate the mean zeropoint of the mbhmdh relation to be mbh108402msun for an mdh10125msun these data are then compared with several models connecting mbh and mdh as well as recent hydrodynamical simulations of galaxy evolution we note that the flux limited nature of qso samples can cause a malmquisttype bias in the measured zeropoint of the mbhmdh relation the magnitude of this bias depends on the scatter in the mbhmdh relation and we reevaluate the zeropoint assuming three published values for this scatter abridged,http://arxiv.org/abs/astro-ph/0609270v1,0
we use threedimensional highresolution adaptivemeshrefinement simulations to investigate if mechanical feedback from active galactic nucleus jets can halt a massive cooling flow in a galaxy cluster and give rise to a selfregulated accretion cycle we start with a 3 x 109msun black hole at the centre of a spherical halo with the mass of the virgo cluster initially all the baryons are in a hot intracluster medium in hydrostatic equilibrium within the dark matters gravitational potential the black hole accretes the surrounding gas at the bondi rate and a fraction of the accretion power is returned into the intracluster medium mechanically through the production of jets the accretion initially slow 00002msunyr becomes catastrophic as the gas cools and condenses in the dark matters potential therefore it cannot prevent the cooling catastrophe at the centre of the cluster however after this rapid phase where the accretion rate reaches a peak of 02msunyr the cavities inflated by the jets become highly turbulent the turbulent mixing of the shockheated gas with the rest of the intracluster medium puts a quick end to this shortlived rapidgrowth phase after dropping by almost two orders of magnitudes the black hole accretion rate stabilises at 0006msunyr without significant variations for several billions of years indicating that a selfregulated steadystate has been reached this accretion rate corresponds to a negligible increase of the black hole mass over the age of the universe but is sufficient to create a quasiequilibrium state in the cluster core,http://arxiv.org/abs/astro-ph/0611914v2,0
we present a simple scenario where the formation of galactic bulges was regulated by the dark halo gravity and regulated the growth of the central supermassive black hole assuming the angular momentum is low we suggest that bulges form in a runaway collapse due to the gravothermal instability once the central gas density or pressure exceeds certain threshold xu zhao 2007 we emphasize that the threshold is nearly universal set by the background nfw dark matter gravity gdm sim 12 times 108rm cm rm sec2 in the central cusps of halos unlike known thresholds for gradual formation of galaxy disks we show that the universal haloregulated star formation threshold for spheroids matches the very high star formation rate and star formation efficiency shown in highredshift observations of central starburst regions the starburst feedback also builds up a pressure shortly after the collapse this large pressure could both act outward to halt further infall of gas from larger scale and act inward to counter the comptonthick wind launched from the central black hole in an eddington accretion assuming the feedback balancing inward and outward forces our scenario naturally gives rise to the black holebulge relationships observed in the local universe,http://arxiv.org/abs/astro-ph/0701792v1,0
cosmological gammaray bursts grbs are probably powered by systems harboring a rotating black hole these may result from hypernovae or black holeneutron star coalescence we identify shortlong bursts with hyper and suspendedaccretion states around slowlyrapidly spinning black holes baryon poor jets as input to the observed grbafterglow emissions may result from dissipation in a gap along an open fluxtube on the axis of rotation of the black hole the torus is expected to radiate a major fraction of the black hole luminosity into gravitational waves which suggests that long grbs may be the most powerful ligovirgo burst sources in the universe abbreviated,http://arxiv.org/abs/astro-ph/0109429v1,0
we study the inspiral of double black holes orbiting inside a massive rotationally supported gaseous disk with masses in the laser interferometer space antenna lisa window of detectability using highresolution sph simulations we follow the black hole dynamics in the early phase when gasdynamical friction acts on the black holes individually and continue our simulation until the form a close binary we find that in the early sinking the black holes loose memory of their initial orbital eccentricity if they corotate with the gaseous disk as a consequence the massive black holes form a binary with very low eccentricity during the inspiral gravitational capture of gas by the black holes occurs mainly when they move on circular orbits and may ignite agn activity eccentric orbits imply instead high relative velocities and weak gravitational focusing,http://arxiv.org/abs/astro-ph/0602013v1,0
we investigate the conjecture by sikora stawarz lasota 2007 that the observed agnradioloudness bimodality can be explained by the morphologyrelated bimodality of blackhole spin distribution in the centers of galaxies central black holes in giant elliptical galaxies may have on average much larger spins than black holes in spiraldisc galaxies we study how accretion from a warped disc influences the evolution of black hole spins and conclude that within the cosmological framework where the most massive bhs have grown in mass via merger driven accretion one indeed expects most supermassive black holes in elliptical galaxies to have on average higher spin than black holes in spiral galaxies where random small accretion episodes eg tidally disrupted stars accretion of molecular clouds might have played a more important role,http://arxiv.org/abs/0706.3900v1,0
a unified model is developed within the context of the braneworld paradigm where a single scalar field can act as both the inflaton field in the very early universe and also as strong selfinteracting dark matter in the postinflationary universe reheating proceeds due to the overproduction and subsequent evaporation of primordial black holes observational constraints most notably from gravitational waves are satisfied if the probability of pbh formation is sufficiently high,http://arxiv.org/abs/astro-ph/0111292v2,0
the dynamics of the universe may be dominated by novel weakly interacting elementary particles by baryons in an invisible form by black holes and globally by vacuum energy the main arguments for and against such hypotheses are reviewed,http://arxiv.org/abs/astro-ph/9511041v1,0
1 overview of neutrino astronomy multidisciplinary science 2 cosmic accelerators the highest energy cosmic rays 3 neutrino beam dumps supermassive black holes and gamma ray bursts 4 neutrino telescopes water and ice 5 indirect dark matter detection 6 towards kilometerscale detectors,http://arxiv.org/abs/astro-ph/9810368v1,0
four ongoing microlensing experiments have produced important new results but also big puzzles the major one being that the expected classes of lenses cannot account for the observed distribution of time scales i discuss future experiments that could resolve these puzzles by far the most important would be to launch a parallax satellite into solar orbit i also discuss a number of nondarkmatter applications of microlensing including searching for planets measuring the rotation speeds of giant stars and imaging a black hole at the center of a quasar,http://arxiv.org/abs/astro-ph/9611136v1,0
virial blackhole mass estimates are presented for 12698 quasars in the redshift interval 01z21 based on modelling of spectra from the sloan digital sky survey sdss first data release the blackhole masses of the sdss quasars are found to lie between simeq107msun and an upper limit of simeq 3times 109msun entirely consistent with the largest blackhole masses found to date in the local universe the estimated eddington ratios of the broadline quasars fwhm geq2000 km s1 show a clear upper boundary at lbolledd1 suggesting that the eddington luminosity is still a relevant physical limit to the accretion rate of luminous broadline quasars at zleq 2 by combining the blackhole mass distribution of the sdss quasars with the 2df quasar luminosity function the number density of active black holes at zsimeq 2 is estimated as a function of mass by comparing the estimated number density of active black holes at zsimeq 2 with the local mass density of dormant black holes we set lower limits on the quasar lifetimes and find that the majority of black holes with mass geq 1085msun are in place by simeq 2,http://arxiv.org/abs/astro-ph/0405393v1,0
the tight relationship between the masses of black holes and galaxy spheroids in nearby galaxies implies a causal connection between the growth of these two components optically luminous quasars host the most prodigious accreting black holes in the universe and can account for 30 of the total cosmological blackhole growth as typical quasars are not however undergoing intense star formation and already host massive black holes 108 msun there must have been an earlier prequasar phase when these black holes grew mass range 106108 msun the likely signature of this earlier stage is simultaneous blackhole growth and star formation in distant ie z1 8 billion light years away luminous galaxies here we report ultradeep xray observations of distant starforming galaxies that are bright at submillimetre wavelengths we find that the black holes in these galaxies are growing almost continuously throughout periods of intense star formation this activity appears to be more tightly associated with these galaxies than any other coeval galaxy populations we show that the blackhole growth from these galaxies is consistent with that expected for the prequasar phase,http://arxiv.org/abs/astro-ph/0503453v1,0
the first stars in the universe forming at redshifts z15 in minihalos with masses of order 106 msun may leave behind black holes as their remnants these objects could conceivably serve as seeds for much larger black holes observed at redshifts z6 we study the growth of the remnant black holes through accretion including for the first time the emitted accretion radiation with adaptive mesh refinement cosmological radiationhydrodynamical simulations the effects of photoionization and heating dramatically affect the accretion flow from large scales resulting in negligible mass growth of the black hole we compare cases with the accretion luminosity included and neglected to show that the accretion radiation drastically changes the environment within 100 pc of the black hole where gas temperatures are increased by an order of magnitude the gas densities are reduced and further star formation in the same minihalo prevented for the two hundred million years of evolution we followed these calculations show that even without the radiative feedback included most seed black holes do not gain mass as efficiently as has been hoped for in previous theories implying that black hole remnants of pop iii stars that formed in minihalos are not likely to be the origin of miniquasars most importantly however these calculations demonstrate that if early stellar mass black holes are indeed accreting close to the bondihoyle rate with ten percent efficiency they have a dramatic local effect in regulating star formation in the first galaxies,http://arxiv.org/abs/0811.0820v2,0
the tight correlation between the masses of central black holes and their host spheroids in nearby galaxies and active galactic nuclei suggests that black hole growth is closely related to their spheroid formation based on our previous work regarding such a joint evolutionary scheme and the consequential black hole to bulge mass correlation we use the xray luminosity function of agn and the cosmological evolution rate which are from rosat xray surveys to estimate the cosmic star formation history associated with the black hole growth by the basic assumption that the major black hole growth occurs during the luminous agn phase the luminosity function of agns as a function of redshift traces not only the accretion history of the black holes but also the cosmic star formation history of the spheroids although the space density of the especially luminous qsos is very low we show that the total amount of star formation associated with the massive black hole growth is almost the same as that of lyman break galaxies detected by the current optical deep surveys we thus argue that the optical deep surveys may miss about half of the net star formation in our universe this is probably due to in part significant dust extinction as well as the small field of view of previous optical surveys which cannot sample such rare events with relatively short time scale however the far infrared emission from the dust heated by star fomation ongoing during the black hole growth could sufficiently account for the observed scuba number counts and would be the probable dominating energy source of the suba population,http://arxiv.org/abs/astro-ph/0212534v1,0
the ubiquity of supermassive black holes smbhs at the centers of nearby luminous galaxies can arise from the multiple mergers experienced by dark matter halos in hierarchical structure formation models even if only a small fraction of these galaxies harbor smbhs at high redshifts we illustrate this possibility using cosmological monte carlo simulations of the merger history of dark matter halos and their associated smbhs in our most extreme models in order to populate nearly every bright galaxy with a smbh at z0 only a few percent of the halos with virial temperatures above 104 k are required to harbor a smbh at high redshift this possibility must be included in studies of the luminosity function and the clustering properties of quasars we predict the number of smbh merger events that are detectable by the gravitational wave experiment lisa as a function of redshift out to z5 although the event rates can be significantly reduced in scenarios with rare smbhs a minimum of 10 detectable merger events per year is predicted if smbh binaries coalesce efficiently the observed distribution of events with redshift could yield valuable information on the smbh formation process if smbh binaries do not coalesce we find that at least several smbh slingshot ejections probably occurred from z5 to the present in each galaxy more massive than 1011 msun at z0 although our results are sensitive to the minimum cooling mass assumed for the formation of smbhs we expect the qualitative predictions of our models to be robust,http://arxiv.org/abs/astro-ph/0101196v2,0
the extractable energy from a black hole as origin of the gammaray burst grb phenomenon is reviewed,http://arxiv.org/abs/astro-ph/0403221v1,0
if cold dark matter is present at the galactic center as in current models of the dark halo it is accreted by the central black hole into a dense spike particle dark matter then annihilates strongly inside the spike making it a compact source of photons electrons positrons protons antiprotons and neutrinos the spike luminosity depends on the density profile of the inner halo halos with finite cores have unnoticeable spikes while halos with inner cusps may have spikes so bright that the absence of a detected neutrino signal from the galactic center already places interesting upper limits on the density slope of the inner halo future neutrino telescopes observing the galactic center could probe the inner structure of the dark halo or indirectly find the nature of dark matter,http://arxiv.org/abs/astro-ph/9906391v1,0
abridged using the number and sizes of observed gravitational lenses i derive upper limits on the dark matter content of elliptical galaxies galaxies built from cold dark matter cdm mass distributions are too concentrated to comfortably satisfy these limits scdm is ruled out and lcdm is only marginally consistent with the data thus lensing adds to the evidence that cdm mass distributions are too concentrated on kiloparsec scales to agree with real galaxies and extends the argument to elliptical galaxies by contrast the lack of central images in radio lenses implies that the central densities of cdm galaxies are too low on 10 parsec scales even if supermassive black holes are included selfinteracting dark matter or any other modification to regular cold dark matter must simultaneously reduce the densities on kiloparsec scales and increase the densities on parsec scales in order to satisfy the unique constraints from lensing,http://arxiv.org/abs/astro-ph/0105200v2,0
observations of the galactic center region with the hess telescopes have established the existence of a steady extended source of gammaray emission coinciding with the position of the super massive black hole sgr a this is a remarkable finding given the expected presence of dense selfannihilating dark matter in the galactic center region the selfannihilation process is giving rise to gammaray production through hadronization including the production of neutral pions which decay into gammarays but also through loopsuppressed annihilation into final states of almost monoenergetic photons we study the observed gammaray signal spectrum and shape from the galactic center in the context of dark matter annihilation and indicate the prospects for further indirect dark matter searches with hess,http://arxiv.org/abs/astro-ph/0702373v1,0
the exactly solvable scalar hairy black hole model originated from the modern highenergy theory is proposed it turns out that the existence of black holes bh is strongly correlated to global scalar field in a sense that they mutually impose bounds upon their physical parameters like the bh mass lower bound or the cosmological constant upper bound we consider the same model also as a cosmological one and show that it agrees with recent experimental data additionally it provides a unified quintessencelike description of dark energy and dark matter,http://arxiv.org/abs/hep-th/0408163v3,0
the cosmological qcd transition affects primordial density perturbations if the qcd transition is first order the sound speed vanishes during the transition and density perturbations fall freely for scales below the hubble radius at the transition the primordial harrisonzeldovich spectrum of density fluctuations develops large peaks and dips these peaks grow with wave number for both the hadronphotonlepton fluid and for cold dark matter at the horizon scale the enhancement is small this by itself does not lead to the formation of black holes at the qcd transition the peaks in the hadronphotonlepton fluid are wiped out during neutrino decoupling for cold dark matter that is kinetically decoupled at the qcd transition eg axions or primordial black holes these peaks lead to the formation of cdm clumps of masses 1020 modot mrm clump 1010 modot,http://arxiv.org/abs/astro-ph/9807257v2,0
supermassive black holes with masses of 106 to more than 109 solar masses are among the most spectacular objects in the universe and are laboratories for physics in extreme conditions the primary goal of arise advanced radio interferometry between space and earth is to use the technique of space vlbi to increase our understanding of black holes and their environments by imaging the havoc produced in the near vicinity of the black holes by their enormous gravitational fields the mission will be based on a 25meter spaceborne radio telescope operating at frequencies between 8 and 86 ghz roughly equivalent to an orbiting element of the very long baseline array in an elliptical orbit with an apogee height of 40000100000 km arise will provide resolution of 15 microarcseconds or better 510 times better than that achievable on the ground at frequencies of 43 and 86 ghz the resolution of light weeks to light months in distant quasars will complement the gammaray and xray observations of highenergy photons which come from the same regions near the massive black holes at 22 ghz arise will image the water maser disks in active galaxies more than 15 mpc from earth probing accretion physics and giving accurate measurements of blackhole masses arise also will study gravitational lenses at resolutions of tens of microarcseconds yielding important information on the darkmatter distribution and on the possible existence of compact objects with masses of 103 to 106 solar masses,http://arxiv.org/abs/astro-ph/9901374v1,0
the observed number counts of quasars may be explained either by longlived activity within rare massive hosts or by shortlived activity within smaller more common hosts it has been argued that quasar lifetimes may therefore be inferred from their clustering length which determines the typical mass of the quasar host here we point out that the relationship between the mass of the blackhole and the circular velocity of its host darkmatter halo is more fundamental to the determination of the clustering length in particular the clustering length observed in the 2df quasar redshift survey is consistent with the galactic halo blackhole relation observed in local galaxies provided that quasars shine at 10100 of their eddington luminosity the slow evolution of the clustering length with redshift inferred in the 2df quasar survey favors a blackhole mass whose redshiftindependent scaling is with halo circular velocity rather than halo mass these results are independent from observations of the number counts of bright quasars which may be used to determine the quasar lifetime and its dependence on redshift we show that if quasar activity results from galaxy mergers then the number counts of quasars imply an episodic quasar lifetime that is set by the dynamical time of the host galaxy rather than by the salpeter time our results imply that as the redshift increases the central blackholes comprise a larger fraction of their host galaxy mass and the quasar lifetime gets shorter,http://arxiv.org/abs/astro-ph/0403714v1,0
after being destroyed by a binary supermassive black hole a stellar density cusp can regrow at the center of a galaxy via energy exchange between stars moving in the gravitational field of the single coalesced hole we illustrate this process via highaccuracy nbody simulations regeneration requires roughly one relaxation time and the new cusp extends to a distance of roughly onefifth the black holes influence radius with density rho r74 the mass in the cusp is of order 10 the mass of the black hole growth of the cusp is preceded by a stage in which the stellar velocity dispersion evolves toward isotropy and away from the tangentiallyanisotropic state induced by the binary we show that density profiles similar to those observed at the center of the milky way and m32 can regenerate themselves in several gyr following infall of a second black hole the presence of density cusps at the centers of these galaxies can therefore not be used to infer that no merger has occurred we argue that bahcallwolf cusps are ubiquitous in stellar spheroids fainter than mv 185 that contain supermassive black holes but the cusps have not been detected outside of the local group since their angular sizes are less than 01 we show that the presence of a cusp implies a lower limit of 104 per year on the rate of stellar tidal disruptions and discuss the consequences of the cusps for gravitational lensing and the distribution of dark matter on subparsec scales,http://arxiv.org/abs/astro-ph/0510498v4,0
we show that with a next generation large telescope one can detect the accelerated motions of 100 stars orbiting the massive black hole at the galactic center the positions and velocities of these stars will be measured to astrometric and spectroscopic precision several times better than currently attainable enabling detailed measurements of the gravitational potential in the neighborhood of the massive black hole we show that the monitoring of stellar motions with such a telescopes enables 1 a measurement of the galactic center distance r0 to better than 01 accuracy 2 a measurement of the extended matter distribution near the black hole including that of the exotic dark matter 3 a detection of general relativistic effects due to the black hole including the prograde precession of stars and possibly the black hole spin and 4 a detection of gravitational encounters between monitored stars and stellar remnants that accumulate near the galactic center such encounters probe the mass function of the remnants,http://arxiv.org/abs/astro-ph/0512621v2,0
we solve dynamical equations of motion to determine the conditions under which an overdense region in the early universe will lead to collapse to a black hole starting from horizon crossing of the overdense region to the point of gravitational instability here we focus on the sensitivity to qcd and electroweak phase transitions we then solve rate equations to determine the mass distribution of black holes in the present universe a second order phase transition or rapid crossover would have significant consequences only if the index of primordial density fluctuations n 125 however a first order transition would lead to a black hole dominated universe for any realistic value of n including n1,http://arxiv.org/abs/0706.1111v1,0
the structure of dark matter halos as predicted from cosmological models is discussed and compared with observed rotation curves of dark matterdominated dwarf galaxies the theoretical models predict that dark matter halos represent a oneparameter family with a universal density profile observations of dark matterdominated rotation curves indeed confirm the universal structure of dark halos they are even in excellent agreement with the expected massradius scaling relations for the currently favoured cosmological model standard cold dark matter with omega0025 and omegalambda075 the rotation profiles however disagree with the predicted dark matter density distributions secular processes which might affect the inner halo structure do not seem to provide a good solution to this problem we discuss as an alternative the possibility that dark halos consist of two separate components a dark baryonic and a dark nonbaryonic component,http://arxiv.org/abs/astro-ph/9904159v1,0
we investigate contributions to the extragalactic gammaray background egb due to neutralino dark matter dm pairannihilation into photons from dm density enhancements minispikes surrounding intermediatemass black holes imbhs we focus on two imbh formation scenarios our conservative scenario where imbhs are remnants of populationiii stars and our optimistic scenario here imbhs are formed in protogalactic disks in both scenarios their formation in pregalactic halos at high redshift lead to the formation of minispikes that are bright sources of gammaray photons taking into account minispike depletion processes we only sum contributions from a cosmological distribution of imbhs with maintained minispikes our conservative scenario bh mass 102 msun with a r32 minispike predicts gammaray fluxes that are an order larger than the equivalent flux using the same dm parameters mass 100 gev and annihilation crosssection 3 times 1026 cm3 s1 from the host halo without imbh minispikes our optimistic scenario bh mass 105 msun with a r73 minispike predicts fluxes that are three orders larger that can reach current egb observations taken by egret dm parameters as above this fact may serve interesting consequences for constraining dm parameters and elucidating the true nature of imbhs additionally we determine the spectra of dm annihilation into monochromatic gammarays and show that its flux can be within observational range of glast providing a potential smokinggun signature of dm,http://arxiv.org/abs/astro-ph/0607042v2,0
quasars are powered by accretion onto supermassive black holes but the problem of the duty cycle related to the episodic activity of the black holes remains open as one of the major questions of cosmological evolution of quasars in this letter we obtain quasar duty cycles based on analyses of a large sample composed of 10979 quasars with redshifts zle21 from the sloan digital sky survey sdss data release three we estimate masses of quasar black holes and obtain their mass function mf of the present sample we then get the duty cycle bardeltaz103sim 1 based on the soltans argument implying that black holes are undergoing multiple episodic activity we find that the duty cycle has a strong evolution by comparison we show that evolution of the duty cycle follows the history of cosmic star formation rate sfr density in the universe providing intriguing evidence for a natural connection between star formation and triggering of black hole activity feedback on star formation from black hole activity is briefly discussed,http://arxiv.org/abs/astro-ph/0606704v1,0
we estimate the energy release of black hole formation to the intracluster medium of the coma cluster and find that this is comparable to the present day energy content therefore an energetic and maybe hydrostatic influence is possible our calculations rely on the assumption of an universal black hole to galaxy mass ratio more exactly spheroidal mass component of the stellar population for which there is growing evidence on a cosmological scale there is also an energy release of black hole formation comparable to what is expected to be present within the thermal gas caused by the process of structure formation this indicates an important dynamical influence neglected by present day structure formation simulations this estimate of cosmological black hole energy release is independent of the black hole to galaxy mass ratio but consistent with its value,http://arxiv.org/abs/astro-ph/9803105v1,0
we investigate the characteristic radiative efficiency epsilon eddington ratio lambda and duty cycle p0 of highredshift active galactic nuclei agn drawing on measurements of the agn luminosity function at z36 and especially on recent measurements of quasar clustering at z345 from the sloan digital sky survey the free parameters of our models are epsilon lambda and the normalization scatter and redshift evolution of the relation between black hole mass mbh and halo virial velocity vvir we compute the luminosity function from the implied growth of the black hole mass function and the quasar correlation length from the bias of the host halos we test our adopted formulae for the halo mass function and halo bias against measurements from the large nbody simulation developed by the mice collaboration the strong clustering of agns observed at z3 and especially at z4 implies that massive black holes reside in rare massive dark matter halos reproducing the observed luminosity function then requires high efficiency epsilon andor low eddington ratio lambda with a lower limit based on 2sigma agreement with the measured z4 correlation length epsilon 07lambda107lambda implying epsilon 017 for lambda 025 successful models predict high duty cycles p002 05 and 09 at z31 45 and 6 respectively and they require that the fraction of halo baryons locked in the central black hole is much larger than the locally observed value the rapid drop in the abundance of the massive and rare host halos at z7 implies a proportionally rapid decline in the number density of luminous quasars much stronger than simple extrapolations of the z36 luminosity function would predict abridged,http://arxiv.org/abs/0810.4919v2,0
we suggest that accretion of planetbound dark matter by the jovian planets and by hotjupiter exoplanets could be a significant source of their internal heat the anomalously low internal heat of uranus would then be explained if the collision believed to have tilted the axis of uranus also knocked it free of most of its associated dark matter cloud our considerations focus on the efficient capture of nonselfannihilating dark matter but could also apply to selfannihilating dark matter provided the capture efficiency is small enough that the earth heat balance constraint is obeyed,http://arxiv.org/abs/0808.2823v4,0
braneworld gravity is a model that endows physical space with an extra dimension in the type ii randallsundrum braneworld gravity model the extra dimension modifies the spacetime geometry around black holes and changes predictions for the formation and survival of primordial black holes we develop a comprehensive analytical formalism for farfield black hole lensing in this model using invariant quantities to compute all geometric optics lensing observables we then make the first analysis of wave optics in braneworld lensing working in the semiclassical limit we show that wave optics offers the only realistic way to observe braneworld effects in black hole lensing we point out that if primordial braneworld black holes exist have mass m and contribute a fraction f of the dark matter then roughly 3e5 x f m1e18 msun1 of them lie within our solar system these objects which we call attolenses would produce interference fringes in the energy spectra of gammaray bursts at energies 100 m1e18 msun1 mev which will soon be accessible with the glast satellite primordial braneworld black holes spread throughout the universe could produce similar interference effects the probability for attolensing may be nonnegligible if interference fringes were observed the fringe spacing would yield a simple upper limit on m detection of a primordial black hole with m 1e19 msun would challenge general relativity and favor the braneworld model further work on lensing tests of braneworld gravity must proceed into the physical optics regime which awaits a description of the full spacetime geometry around braneworld black holes,http://arxiv.org/abs/gr-qc/0603061v3,0
depending on various assumptions on the energy scale of inflation and assuming a primordial power spectrum of a broken scale invariance bsi type we explore the possibility for primordial black holes pbh and planck relics to contribute substantially to cold dark matter in the universe a recently proposed possibility to produce planck relics in 4dimensional string gravity is considered possible experimental detection through gravitational waves is further explored we stress that inflation with a low energy scale and also possibly when planck relics are produced leads unavoidably to relics originating from pbhs that are not effectively classical during their formation rendering the usual formalism inadequate for them,http://arxiv.org/abs/astro-ph/0303330v2,0
cosmic ray antiprotons can originate from dark matter annihilating into quarks that subsequently decay into antiprotons evaporation of primordial black holes also can produce a significant antiproton flux since the spectrum of secondary antiprotons from cosmic ray interactions peaks at 2 gev and goes down sharply at lower energy there is a window at energies 1 gev in which to look for excess antiprotons as a signature of these exotic antiproton sources however in the vicinity of the earth low energy particles are strongly modulated by the solar wind which makes any analysis ambiguous the adverse effects of the solar wind can be avoided by placing a low energy antiproton spectrometer aboard an interstellar probe the theoretical predictions are reviewed and the preliminary design of a lightweight lowpower instrument to make the measurements and a summary of the anticipated results are given in this paper,http://arxiv.org/abs/astro-ph/9912140v1,0
the discovery of dark matter particles would conclusively reject the mond theory mond may violate einsteins strong equivalence principle however as we show there is already evidence that mond is likely not required mond was invented to explain the rotation velocities of stars far into the galactic halos dark matter also explains this same effect these both use a gravity probe of the ir2 law we show that non gravity probes determine the same value for the amount of dark matter that does not involve modifications of gravity using occams razor this coincidence is best explained by the existence of dark matter,http://arxiv.org/abs/astro-ph/0510576v1,0
caustics are a generic feature of the nonlinear growth of structure in the dark matter distribution if the dark matter were absolutely cold its mass density would diverge at caustics and the integrated annihilation probability would also diverge for individual particles participating in them for realistic dark matter candidates this behaviour is regularised by small but nonzero initial thermal velocities we present a mathematical treatment of evolution from hot warm or cold dark matter initial conditions which can be directly implemented in cosmological nbody codes it allows the identification of caustics and the estimation of their annihilation radiation in fully general simulations of structure formation,http://arxiv.org/abs/0809.0497v2,0
gravitational tunneling radiation gtr from a black hole is similar to electric field emission of electrons from a metal there is always a second body to give the gravitational barrier of a black hole a finite rather than infinite width so that a particle can escape by tunneling thus gtr may be emitted from black holes in a process differing from that of hawking radiation hr with broad ranging astrophysical ramifications hr of an isolated black hole is an abstraction which has not been detected in three decades and which may not exist gtr is beamed in contrast to hr which is isotropic little black holes may account for much of the dark matter of the universe and hence the anomalous rotation of spiral galaxies as well as contribute to the accelerated expansion of the universe the absence of hr permits the existence of little black hole gravitationally bound atoms with quantized orbits and quantized gravitational radiation since some theories speculate that the early universe may have existed in ndimensional space certain results will be presented both for general nspace and for the 3space in which we live an inconsistency between quantum mechanics and the equivalence principle is demonstrated it is shown that angular momentum cannot be quantized in the usual manner in 4space with possible consequences for kaluzaklein theory an hypothesis is presented for reducing the enormously toolarge vacuum zeropoint energy string theory may be impacted by the finding that atoms are not energetically stable in higher than 3space gtr avoids the black hole lost information paradox,http://arxiv.org/abs/astro-ph/0302469v4,0
we discuss the link between dark matter halos hosting the first popiii stars and the rare massive halos that are generally considered to host bright quasars at high redshift z6 the main question that we intend to answer is whether the supermassive black holes powering these qsos grew out from the seeds planted by the first intermediate massive black holes created in the universe this question involves a dynamical range of 1013 in mass and we address it by combining nbody simulations of structure formation to identify the most massive halos at z6 with a monte carlo method based on linear theory to obtain the location and formation times of the first light halos within the whole simulation box we show that the descendants of the first 106 msun virialized halos do not on average end up in the most massive halos at z6 but rather live in a large variety of environments the oldest popiii progenitors of the most massive halos at z6 form instead from density peaks that are on average one and a half standard deviations more common than the first popiii star formed in the volume occupied by one bright highz qso the intermediate mass black hole seeds planted by the very first popiii stars at z40 can easily grow to masses mbh1095 msun by z6 assuming eddington accretion with radiative efficiency epsilon01 quenching of the black hole accretion is therefore crucial to avoid an overabundance of supermassive black holes at lower redshift this can be obtained if the mass accretion is limited to a fraction eta6103 of the total baryon mass of the halo hosting the black hole the resulting high end slope of the black hole mass function at z6 is alpha 37 a value within the 1sigma error bar for the bright end slope of the observed quasar luminosity function at z6,http://arxiv.org/abs/0705.3843v1,0
we study the generation of a stochastic gravitational wave gw background produced from a population of corecollapse supernovae which form black holes in scenarios of structure formation we obtain for example that the formation of a population population iii of black holes in cold dark matter scenarios could generate a stochastic gw background with a maximum amplitude of hrm bg simeq 1024 and corresponding closure energy density of omegarmgwsim 107 in the frequency band nurmobs simeq 30470 rm hz assuming a maximum efficiency of generation of gws namely epsilonrm gwrm max 7times 104 for stars forming at redshifts zsimeq 3010 we show that it will be possible in the future to detect this isotropic gw background by correlating signals of a pair of advanced ligo observatories ligo iii at a signaltonoise ratio of simeq 40 we discuss what astrophysical information could be obtained from a positive or even a negative detection of such a gw background generated in scenarios such as those studied here one of them is the possibility of obtaining the initial and final redshifts of the emission period from the observed spectrum of gws,http://arxiv.org/abs/astro-ph/0202037v1,0
spherical gravitational collapse of a cold gas of annihilating particles involves a competition between the freefall rate proptosqrtrho and the swave annihilation rate proptorho thus there is a critical density rhoann above which annihilation proceeds faster than free fall gravitational collapse of a cloud of initial mass m to a black hole is only possible if 332pi g3m2lesssimrhoann or mgtrsimmannequiv 332pi g3rhoann12 for a particle mass m and freezeout temperature tfmxf the minimum black hole mass is mannapprox 1010msun timesxfsqrtgstar100omcdm gstar smrm gev where gstar s and gstar are degeneracy factors the formation of a black hole of initial mass mbh is accompanied by the annihilation of about mann released in a burst lasting a time sim gmbh that could reach a total annihilation luminosity sim 1059 rm erg s1 the absence of astronomical observations of such spectacular events suggests either i the branching ratio for cdm annihilation to ee pairs or quarks lesssim 1010 while the branching ratio to nubarnu is lesssim 105 or ii cdm is not made of annihilating particles but may be in some nonannihilating form such as axions or iii cdm black holes never form,http://arxiv.org/abs/astro-ph/0505497v1,0
the rotation curves of dark matter dominated dwarf galaxies are analysed the observations show that dark matter halos represent a oneparameter family with self similar density profiles the global halo parameters like total mass and scale length are coupled by simple scaling relations the inner halo regions resemble nonsingular isothermal spheres with constant density cores the observations are compared with dark matter halos resulting from cosmological cold dark matter simulations the theoretical models predict that dark matter halos represent a oneparameter family in agreement with the observations however in contradiction to the observations the calculations lead to dark matter halos with r1 density cusps in the center and nonisothermal velocity dispersion profiles processes which might affect the inner halo structure resulting in isothermal constant density cores are discussed,http://arxiv.org/abs/astro-ph/9703057v1,0
we discuss a numerical model for black hole growth and its associated feedback processes that for the first time allows cosmological simulations of structure formation to selfconsistently follow the build up of the cosmic population of galaxies and active galactic nuclei our model assumes that seed black holes are present at early cosmic epochs at the centres of forming halos we then track their growth from gas accretion and mergers with other black holes in the course of cosmic time for black holes that are active we distinguish between two distinct modes of feedback depending on the black hole accretion rate itself black holes that accrete at high rates are assumed to be in a quasar regime where we model their feedback by thermally coupling a small fraction of their bolometric luminosity to the surrounding gas for black holes with low accretion rates we conjecture that most of their feedback occurs in mechanical form where agndriven bubbles are injected into a gaseous environment using our new model we carry out treesph cosmological simulations on the scales of individual galaxies to those of massive galaxy clusters both for isolated systems and for cosmological boxes we demonstrate that our model produces results for the black hole and stellar mass densities in broad agreement with observational constraints we find that the black holes significantly influence the evolution of their host galaxies changing their star formation history their amount of cold gas and their colours also the properties of intracluster gas are affected strongly by the presence of massive black holes in the cores of galaxy clusters leading to shallower metallicity and entropy profiles and to a suppression of strong cooling flows abridged,http://arxiv.org/abs/0705.2238v2,0
a longstanding question is whether active galactic nuclei agn vary like galactic black hole systems when appropriately scaled up by mass refs 13 if so we can then determine how agn should behave on cosmological timescales by studying the brighter and much faster varying galactic systems as xray emission is produced very close to the black holes it provides one of the best diagnostics of their behaviour a characteristic timescale which potentially could tell us about the mass of the black hole is found in the xray variations from both agn and galactic black holes refs 16 but whether it is physically meaningful to compare the two has been questioned ref 7 here we report that after correcting for variations in the accretion rate the timescales can be physically linked revealing that the accretion process is exactly the same for small and large black holes strong support for this linkage comes perhaps surprisingly from the permitted optical emission lines in agn whose widths in both broadline agn and narrowemissionline seyfert 1 galaxies correlate strongly with the characteristic xray timescale exactly as expected from the agn black hole masses and accretion rates so agn really are just scaledup galactic black holes,http://arxiv.org/abs/astro-ph/0612273v1,0
the standard cold dark matter cdm model has recently been challenged by the claim that dwarf galaxies have dark matter halos with constant density cores consequently numerous alternative dark matter candidates have recently been proposed in this paper we scrutinize the observational evidence for the incongruity between dwarf galaxies and the cdm model to this end we analyze the rotation curves of 20 latetype dwarf galaxies studied by swaters 1999 taking the effects of beamsmearing and adiabatic contraction into account we fit mass models to these rotation curves with dark matter halos with different cusp slopes ranging from constant density cores to r2 cusps uncertainties in the stellar masstolight ratio and the limited spatial sampling of the halos density distribution hamper a unique mass decomposition consequently the rotation curves in our sample cannot be used to discriminate between dark halos with constant density cores and r1 cusps we show that the dwarf galaxies analyzed here are consistent with cold dark matter halos in a lcdm cosmology and that there is thus no need to abandon the idea that dark matter is cold and collisionless however the data is also consistent with any alternative dark matter model that produces dark matter halos with central cusps less steep than r15 in fact we argue that based on existing rotation curves alone at best weak limits can be obtained on cosmological parameters andor the nature of the dark matter,http://arxiv.org/abs/astro-ph/0006048v2,0
this conference covered dark matter particle properties and searches lensing by dark matter and dark matter distributions in galaxies and clusters dark energy phenomenology and theoretical models surveys of structure and galaxy formation large scale simulations and neutrino cosmology astronomy and star formation cosmic rays cosmic microwave background inflation in the primordial universe cosmic strings brane cosmology and the finestructure constant,http://arxiv.org/abs/astro-ph/0405625v1,0
combining a theoretical model of mass accretion onto a galactic center with a highresolution nbodysph simulation we investigate the formation of an intermediate massive black hole imbh during the hierarchical formation of a small spiral galaxy with a total mass of 1010modot in the highz universe we found that the rate of average mass accretion to the nucleus due to the radiation drag exerted by newly formed stars in the forming galaxy is approx 105modotyr1 as a result of this accretion an imbh with approx 104modot can be formed in the center of the spiral galaxy at zsim 4 we found that a central bh coevolves with the dark matter halo from zsim 15 to zsim 2 the mass ratio of the bh to the dark matter halo is nearly constant approx 13 times 106 from zsim 10 to zsim 2 this is because that change in the dark matter potential enhances star formation in the central part of the galaxy and as a result the bh evolves due to mass accretion via the radiation drag therefore our model naturally predicts a correlation between massive bhs and dark matter halos moreover it is found that the final bhtobulge mass ratio approx 5times 105 in a small spiral galaxy at highz is much smaller than that in the large galaxies approx 103 our results also suggest that the scatter in the observed scaling relations between the bulge mass and black hole mass are caused by a time lag between bh growth and growth of bulge we also predict that the xray luminosity of agn is positively correlated with the co luminosity in the central region by comparing our results with the properties of lyman break galaxies lbgs it is predicted that some lbgs have massive bhs of approx 106107modot,http://arxiv.org/abs/astro-ph/0504202v1,0
we show that the mass function of black holes expected from the past quasar activity both visible and obscured is consistent with the number of dormant black holes found in the bulges of nearby galaxies the joint formation of quasars and bulges is addressed by means of an analytical model for galaxy formation based on the hierarchical clustering of cold dark matter halos the model is able to reproduce the main statistical properties of both populations under the hypotheses that i star formation and quasar shining follow an antihierarchical order and ii galaxy morphology and final black hole mass are determined by the same physical process,http://arxiv.org/abs/astro-ph/9909267v2,0
after discussion of the properties of degenerate fermion balls we analyze the orbit of the star s01 which has a projected distance of 5 lightdays to sgr a in the supermassive black hole as well as in the fermion ball scenarios of the galactic center it is shown that both scenarios are consistent with the data as measured during the last 6 years by genzel and coworkers and by ghez and coworkers we show that the zvz phase space which fits the data is much larger for the fermion ball than for the black hole scenario future measurements of the positions or radial velocities of s01 and s02 which could be orbiting within such a fermion ball may reduce this allowed phase space and eventually rule out one of the currently acceptable scenarios,http://arxiv.org/abs/astro-ph/0207424v1,0
after a discussion of the properties of degenerate fermion balls we analyze the orbits of the stars s01 and s02 which have the smallest projected distances to sgr a in the supermassive black hole as well as in the fermion ball scenarios of the galactic center it is shown that both scenarios are consistent with the data as measured during the last six years by genzel et al and ghez et al the free parameters of the projected orbit of a star are the unknown components of its velocity vz and distance z to sgr a in 19954 with the zaxis being in the line of sight we show in the case of s01 and s02 that the zvz phasespace which fits the data is much larger for the fermion ball than for the black hole scenario future measurements of the positions or radial velocities of s01 and s02 could reduce this allowed phasespace and eventually rule out one of the currently acceptable scenarios this may shed some light into the nature of the supermassive compact dark object or dark matter in general at the center of our galaxy,http://arxiv.org/abs/astro-ph/0103466v1,0
we investigate the hypothesis that the cores of elliptical galaxies and bulges are created from the binding energy liberated by the coalescence of supermassive binary black holes during galaxy mergers assuming that the central density profiles of galaxies were initially steep power laws we define the mass deficit as the mass in stars that had to be removed from the nucleus in order to produce the observed core we use nonparametric deprojection to compute the mass deficit in a sample of 35 earlytype galaxies with highresolution imaging data we find that the mass deficit correlates well with the mass of the nuclear black hole consistent with the predictions of merger models we argue that core sizes in halos of noninteracting dark matter particles should be comparable to those observed in the stars,http://arxiv.org/abs/astro-ph/0110185v3,0
we present a physical model for the coevolution of massive spheroidal galaxies and active nuclei at their centers supernova heating is increasingly effective in slowing down the star formation and in driving gas outflows in smaller and smaller dark matter halos thus the more massive protogalaxies virializing at early times are the sites of faster star formation the correspondingly higher radiation drag causes a faster angular momentum loss by the gas and induces a larger accretion rate onto the central black hole in turn the kinetic energy of the outflows powered by the active nuclei can unbind the residual gas in a time shorter for larger halos the model accounts for a broad variety of dynamical photometric and metallicity properties of earlytype galaxies for the mbh sigma relation and for the local supermassive blackhole mass function,http://arxiv.org/abs/astro-ph/0409585v1,0
the gravitational waves generated during supermassive black hole smbh coalescence are prime candidates for detection by the satellite lisa we use the extended pressschechter formalism combined with empirically motivated estimates for the smbhdark matter halo mass relation and smbh occupation fraction to estimate the maximum coalescence rate for major smbh mergers assuming efficient binary coalescence and guided by the lowest nuclear black hole mass inferred in local galactic bulges and nearby lowluminosity active galactic nuclei 105 msun we predict approximately 15 detections per year at a signal to noise greater than five in each of the inspiral and ringdown phases rare coalescences between smbhs having masses in excess of 107 msun will be more readily detected via gravitational waves from the ringdown phase,http://arxiv.org/abs/astro-ph/0503210v1,0
nearby masses can have a high probability of lensing stars in a distant background field highprobability lensing or mesolensing can therefore be used to dramatically increase our knowledge of dark and dim objects in the solar neighborhood where it can discover and study members of the local dark population freefloating planets lowmass dwarfs white dwarfs neutron stars and stellar mass black holes we can measure the mass and transverse velocity of those objects discovered or already known and determine whether or not they are in binaries with dim companions we explore these and other applications of mesolensing including the study of forms of matter that have been hypothesized but not discovered such as intermediatemass black holes dark matter objects freestreaming through the galactic disk and planets in the outermost regions of the solar system in each case we discuss the feasibility of deriving results based on presentday monitoring systems and also consider the vistas that will open with the advent of allsky monitoring in the era of the panoramic survey telescope and rapid response system panstarrs and the large synoptic survey telescope lsst,http://arxiv.org/abs/0712.3558v1,0
we investigate the clumping of cold dark matter cdm at small scales if the cdm particle is the neutralino we find that collisional damping during its kinetic decoupling from the radiation fluid and free streaming introduce a smallscale cutoff in the primordial power spectrum of cdm this cutoff sets the scale for the very first cdm objects in the universe which we expect to have a mass of sim 1012 modot for nonthermal cdm candidates such as axions wimpzillas or primordial black holes the cosmological qcd transition might induce features in the primordial spectrum at similar mass scales,http://arxiv.org/abs/astro-ph/9912343v1,0
we study the origin of cores and density profiles of gaseous baryonic structures in cosmology by treating the baryons as a viscous gas we find that both spheres and disks are possible solutions we find analytically that the density profiles have inner and outer solutions which in general are different for disks we identify a central core with density profile rhod constant and the outer profile rhod r3 for spherical structures we find the profile rhos r6 in the presence of a dominating central black hole we find the inner profile rho r32 when the mass is dominated by a dark matter component then the baryonic density profile will depend on the dark matter profile and we point out how one can use this connection to infer the dm profile directly by observing the baryonic density profile,http://arxiv.org/abs/astro-ph/0303416v2,0
a model of inhomogeneous baryogenesis based on the affleck and dine mechanism is described a simple coupling of the scalar baryon field to the inflaton allows for formation of astronomically significant bubbles with a large baryon or antibaryon asymmetry during the farther evolution these domains form compact stellarlike objects or lower density clouds or primordial black holes of different size according to the scenario such high baryonic number objects occupy relatively small fraction of space but despite that they may significantly contribute to the cosmological mass density for some values of parameters the model allows the possibility the whole dark matter in the universe to be baryonic furthermore the model allows the existence of the antibaryonic bbubbles ie a significant fraction of the mass density in the universe can be in the form of the compact antimatter objects eg antistars,http://arxiv.org/abs/0806.2986v2,0
the formation and growth of black holes can strongly influence the distribution of dark matter around them i discuss here the different types of dark matter overdensities around black holes including dark matter cusps spikes mounds crests and gravitational atoms i then review recent results on the evolution of a black holes binary in presence of dark matter focusing on the energy transfer between binary and dark matter induced by dynamical friction finally i present the prospects for studying dark matter with gravitational wave observations and argue that future interferometers might be able to detect and characterise dark matter overdensities around black holes,http://arxiv.org/abs/2404.11513v1,0
we have developed a consistent analytical model to describe the observed evolution of the quasar luminosity function our model combines black hole mass distributions based on the press schechter theory of the structure formation in the universe with quasar luminosity functions resulting from a physicsbased emission model that takes into account the timedependent phenomena occurring in the accretion disks quasar evolution and cdm models are mutually constraining therefore our model gives an estimation of the exponent n of the power spectrum pk which is found to be 18 n 16 we were able to reject a generally assumed hypothesis of a constant ratio between dark matter halo and the black hole mass since the observed data could not be fitted under this assumption we found that the relation between the dark matter halos and black hole masses is better described by mbhmdmh0668 this model provides a reasonable fit to the observed quasar luminosity function at redshifts higher than 20 we suggest that the disagreement at lower redshift is due to mergers based on the agreement at high redshift we estimated the merger rate at lower redshift and argue that this rate should depend on the redshift like 1z3,http://arxiv.org/abs/astro-ph/0009105v1,0
the origin of the galaxies represents an important focus of current cosmological research both observational and theoretical its resolution involves a comprehensive understanding of star formation galaxy dynamics the cosmology of the very early universe and the nature of the dark matter in this review i will focus on those aspects of dark matter that are relevant for understanding galaxy formation and describe the outlook for detecting the most elusive component nonbaryonic dark matter,http://arxiv.org/abs/astro-ph/0412297v1,0
we performed cosmological simulations based upon both a cold dark matter cdm and a warm dark matter wdm model the focus of our investigations lies with selected spatial and kinematic properties of substructure halos subhalos orbiting within host halos that form in both darkmatter cosmologies we aim at using the dynamics of the subhalos as a probe of the respective cosmology,http://arxiv.org/abs/0811.1851v2,0
we investigate the cosmological effects of a neutrino interaction with cold dark matter we postulate a neutrino that interacts with a neutrino interacting dark matter nidm particle with an elasticscattering cross section that either decreases with temperature as t2 or remains constant with temperature the neutrinodarkmatter interaction results in a neutrinodarkmatter fluid with pressure and this pressure results in diffusiondamped oscillations in the matter power spectrum analogous to the acoustic oscillations in the baryonphoton fluid we discuss the bounds from the sloan digital sky survey on the nidm opacity ratio of cross section to nidmparticle mass and compare with the constraint from observation of neutrinos from supernova 1987a if only a fraction of the dark matter interacts with neutrinos then nidm oscillations may affect current cosmological constraints from measurements of galaxy clustering we discuss how detection of nidm oscillations would suggest a particleantiparticle asymmetry in the darkmatter sector,http://arxiv.org/abs/astro-ph/0606190v1,0
the annihilation of neutralino dark matter in the galactic center gc may result in radio signals that can be used to detect or constrain the dark matter halo density profile or dark matter particle properties at the galactic center the accretion flow onto the central black hole bh sustains strong magnetic fields that can induce synchrotron emission by electrons and positrons generated in neutralino annihilations during advection onto the bh here we reanalyze the radiative processes relevant for the neutralino annihilation signal at the gc with realistic assumptions about the accretion flow and its magnetic properties we find that neglecting these effects as done in previous papers leads to the incorrent electron and photon spectra we find that the magnetic fields associated with the flow are significantly stronger than previously estimated we derive the appropriate equilibrium distribution of electrons and positron and the resulting radiation considering adiabatic compression in the accretion flow inverse compton scattering off synchrotron photons synchrotron selfcompton scattering and synchrotron selfabsorption of the emitted radiation we derive the signal for a navarrofrenkwhite nfw dark matter halo profile and a nfw profile with a dark matter spike due to the central bh we find that the observed radio emission from the gc is inconsistent with the scenario in which a spiky distribution of neutralinos is present we discuss several important differences between our calculations and those previously presented in the literature,http://arxiv.org/abs/astro-ph/0402588v2,0
this work develops and explores a quantumbased theory which enables the nature and origin of cold dark matter cdm to be understood without need to introduce exotic particles the quantum approach predicts the existence of certain macroscopic quantum structures that are wimplike even when occupied by traditional baryonic particles these structures function as dark matter candidates for cdm theory on large scales where it has been most successful and retain the potential to yield observationally compliant predictions on galactic cluster and subcluster scales relatively pure high angular momentum eigenstate solutions obtained from schrodingers equation in weak gravity form the structural basis they have no classical analogue and properties radically different from those of traditional localised matter whose eigenstate spectra contain negligible quantities of such states salient features include radiative lifetimes that can exceed the age of the universe energies and sizes consistent with galactic halos and negligible interaction rates with radiation and macroscopic galactic objects this facilitates the formation of sparsely populated macroscopic quantum structures that are invisible and stable viable structure formation scenarios are based on the seed potential wells of primordial black holes formed at the ee phase transition the structures can potentially produce suitable internal density distributions and have capacity to accommodate the required amount of halo dark matter the formation scenarios show that it is possible to incorporate structures into universal evolutionary scenarios without significantly compromising the results of wmap or the measurements of elemental bbn ratios,http://arxiv.org/abs/astro-ph/0406139v1,0
observations by the macho collaboration indicate that a significant fraction of the galactic halo dark matter may be in form of compact objects with masses msim 05modot identification of these objects as red or white dwarfs is problematic due to stringent observational upper limits on such dwarf populations primordial black hole pbh formation from preexisting density fluctuations is facilitated during the cosmic qcd transition due to a significant decrease in pressure forces for generic initial density perturbation spectra this implies that essentially all pbhs may form with masses close to the qcdhorizon scale mhqcdsim 1modot it is possible that such qcd pbhs contribute significantly to the closure density today i discuss the status of theoretical predictions for the properties of qcd pbh dark matter observational signatures of and constraints on a cosmic solar mass pbh population are also discussed,http://arxiv.org/abs/astro-ph/9805147v1,0
nerd abstract observational constraints on spacetime are reviewed focusing on how the underlying physics dark matter dark energy gravity can be tested rather than assumed popular abstract space is not a boring static stage on which events unfold over time but a dynamic entity with curvature fluctuations and a rich life of its own which is a booming area of study spectacular new measurements of the cosmic microwave background gravitational lensing type ia supernovae largescale structure spectra of the lyman alpha forest stellar dynamics and xray binaries are probing the properties of spacetime over 22 orders of magnitude in scale current measurements are consistent with an infinite flat everlasting universe containing about 30 cold dark matter 65 dark energy and at least two distinct populations of black holes,http://arxiv.org/abs/astro-ph/0207199v1,0
warm dark matter wdm models have recently been resurrected to resolve apparent conflicts of cold dark matter dm models with observations endowing the dm particles with nonnegligible velocities causes freestreaming which suppresses the primordial power spectrum on small scales the choice of a rootmeansquare velocity dispersion vrms 005 kms at redshift z0 corresponding to a particle mass of 1 kev if the wdm particles are fermions decoupling while relativistic helps alleviate most but probably not all of the smallscale problems faced by cdm an important sideeffect of the particle velocities is the severe decrease in the number of collapsed halos at high redshift this is caused both by the loss of smallscale power and by the delay in the collapse of the smallest individual halos with masses near the effective jeans mass of the dm the presence of early halos is required in order 1 to host either early quasars or galaxies that can reionize the universe by redshift z58 and 2 to allow the growth of the supermassive black hole believed to power the recently discovered quasar sdss 10441215 at this redshift we quantify these constraints using a modified pressschechter formalism and find vrms 1 kev if future observations uncover massive black holes at z 10 or reveal that reionization occurred at z 10 this could conclusively rule out wdm models as the solution to the smallscale crisis of the cdm paradigm,http://arxiv.org/abs/astro-ph/0103050v1,0
we investigate the relationship between the mass and velocity fields of the intergalactic medium igm and dark matter although the evolution of the igm is dynamically governed by the gravity of the underlying dark matter field some statistical properties of the igm inevitably decouple from those of the dark matter once the nonlinearity of the dynamical equations and the stochastic nature of the field is considered with simulation samples produced by a hybrid cosmological hydrodynamicnbody code which is effective in capturing shocks and complicated structures with high precision we find that the onepoint distributions of the igm field are systematically different from that of dark matter as follows 1 the onepoint distribution of the igm peculiar velocity field is exponential at least at redshifts less than 2 while the dark matter velocity field is close to a gaussian field 2 although the onepoint distributions of the igm and dark matter are similar the pointbypoint correlation between the igm and dark matter density fields significantly differs on all scales and redshifts analyzed 3 the onepoint density distributions of the difference between igm and dark matter fields are highly nongaussian and long tailed these discrepancies violate the similarity between the igm and dark matter and cannot be explained simply as jeans smoothing of the igm however these statistical discrepancies are consistent with the fluids described by stochasticforce driven nonlinear dynamics,http://arxiv.org/abs/astro-ph/0405508v1,0
cold dark matter cdm has become the standard modern theory of cosmological structure formation its predictions appear to be in good agreement with data on large scales and it naturally accounts for many properties of galaxies but despite its many successes there has been concern about cdm on small scales because of the possible contradiction between the linearly rising rotation curves observed in some darkmatterdominated galaxies vs the 1r density cusps at the centers of simulated cdm halos other cdm issues on small scales include the very large number of small satellite halos in simulations far more than the number of small galaxies observed locally and problems concerning the angular momentum of the baryons in dark matter halos the latest data and simulations have lessened although not entirely resolved these concerns meanwhile the main alternatives to cdm that have been considered to solve these problems selfinteracting dark matter sidm and warm dark matter wdm have been found to have serious drawbacks,http://arxiv.org/abs/astro-ph/0205391v1,0
it is shown that contrary to the widespread opinion particle collisions considerably increase accretion rate from the cosmological background onto 5d primordial black holes formed during the highenergy phase of the randallsundrum type ii braneworld scenario increase of accretion rate leads to much tighter constraints on initial primordial black hole mass fraction imposed by the critical density limit and measurements of highenergy diffuse photon background and antiproton excess,http://arxiv.org/abs/astro-ph/0510212v2,0
we discuss the various ways in which primordial black holes may have formed in the early universe and how the effects of such black holes can be used to place constraints on cosmological models we show that such constraints may be severely modified if the value of the gravitational constant g varies with cosmological epoch a possibility which arises in many scenarios for the early universe the nature of the modification depends upon whether the value of g near a black hole maintains the value it had at its formation epoch corresponding to gravitational memory or whether it tracks the background cosmological value this is still uncertain but we discuss various approaches which might help to resolve the issue,http://arxiv.org/abs/astro-ph/0003027v1,0
the formation of quasar black holes during the hydrodynamic collapse of protogalactic gas clouds is discussed the dissipational collapse and long term dynamical evolution of these systems is analysed using threedimensional numerical simulations the calculations focus on the final collapse stages of the inner baryonic component and therefore ignore the presence of dark matter two types of initial conditions are considered uniformly rotating spherical clouds and irrotational ellipsoidal clouds in both cases the clouds are initially cold homogeneous and not far from rotational support tw approx 01 although the details of the dynamical evolution depend sensitively on the initial conditions the qualitative features of the final configurations do not most of the gas is found to fragment into small dense clumps that eventually make up a spheroidal component resembling a galactic bulge about 5 of the initial mass remains in the form of a smooth disk of gas supported by rotation in the gravitational potential well of the outer spheroid if a central seed black hole of mass gsim106modot forms it can grow by steady accretion from the disk and reach a typical quasar black hole mass sim108modot in less than 5times 108yr in the absence of a sufficiently massive seed dynamical instabilities in a strongly self gravitating inner region of the disk will inhibit steady accretion of gas and may prevent the immediate formation of a quasar,http://arxiv.org/abs/astro-ph/9401026v1,0
we study the imprints of agn feedback and physical viscosity on the properties of galaxy clusters using hydrodynamical simulation models carried out with the treesph code gadget2 besides selfgravity of dark matter and baryons our approach includes radiative cooling and heating processes of the gas component and a multiphase model for star formation and sne feedback additionally we introduce a prescription for physical viscosity in gadget2 based on a sph discretization of the navierstokes and general heat transfer equations adopting the braginskii parameterization for the shear viscosity coefficient we explore how gas viscosity influences the properties of agndriven bubbles we also introduce a novel selfconsistent agn feedback model where we simultaneously follow the growth and energy release of massive black holes embedded in a cluster environment we assume that black holes accreting at low rates with respect to the eddington limit are in a radiatively inefficient regime and that most of the feedback energy will appear in a mechanical form thus we introduce agndriven bubbles into the icm with properties such as radius and energy content that are directly linked to the black hole physics this model leads to a selfregulated mechanism for the black hole growth and overcomes the cooling flow problem in host halos ranging from the scale of groups to that of massive clusters abridged,http://arxiv.org/abs/astro-ph/0610907v1,0
one of the central features of the last 8 to 10 billion years of cosmic history has been the emergence of a wellpopulated red sequence of nonstarforming galaxies a number of models of galaxy formation and evolution have been devised to attempt to explain this behavior most current models require feedback from supermassive black holes agn feedback to quench star formation in galaxies in the centers of their dark matter halos central galaxies such models make the strong prediction that all quenched central galaxies must have a large supermassive black hole and by association a prominent bulge component i show using data from the sloan digital sky survey that the observations are consistent with this prediction over 995 of red sequence galaxies with stellar masses in excess of 1010 msun have a prominent bulge component as defined by having a sersic index n above 15 those very rare red sequence central galaxies with little or no bulge n15 usually have detectable star formation or agn activity the fraction of truly quenched bulgeless central galaxies is 01 of the total red sequence population i conclude that a bulge and by implication a supermassive black hole is an absolute requirement for full quenching of star formation in central galaxies this is in agreement with the most basic prediction of the agn feedback paradigm,http://arxiv.org/abs/0804.4001v1,0
we investigate the distribution of dark matter dm in gasrich lowmass galaxies confronting them with numerical cosmological simulations with cold dark matter lcdm we show that the derived rotation curves comply best with cored dm density profiles whereas the signatures of the central cusps invariably predicted by lcdm simulations are not seen,http://arxiv.org/abs/0811.4289v1,0
reasons supporting the idea that most of the dark matter in galaxies and clusters of galaxies is baryonic is discussed moreover it is argued that most of the dark matter in galactic halo should be in the form of machos and cold molecular clouds,http://arxiv.org/abs/astro-ph/9610031v1,0
we present constraints on the density of halo dark matter candidates within the solar circle based on the anisotropy in the high energy gammaray background the known galactic components of the gammaray background in particular the inverse compton component have been estimated more accurately we find the spectrum of the residual emission after subtracting the galactic component is inconsistent with emission from some of the proposed dark matter candidates we derive upper limits of 108 msun for the mass of diffuse gas and 3109 pc3 for the number density of primordial black holes contributing to the gammaray background,http://arxiv.org/abs/astro-ph/9811324v1,0
i demonstrate that precision timing of millisecond pulsars possess the capabilities of detecting the gravitational effects of intervening galactic substructure this analysis is applicable to all types of collapsed baryons including stars planets and machos as well as many types of dark matter including primordial black holes scalar miniclusters and sufficiently dense clumps of cold dark matter the physical signal is quantified and decomposed into observable and unobservable components templates for the observable signals are also presented additionally i calculate the expected changes in the observed period and period derivatives that will result from intervening matter i find that pulsar timing is potentially a very useful tool for probing the nature of dark matter and to learn more about the substructure present within our galaxy,http://arxiv.org/abs/0801.3458v1,0
this research paper studies about the role of social media use and increase the risk factor of mental health during covid 19 or lockdown although few studies have been conducted on the role about the effect of social media use on mental health during lockdown and impact on human reactive nature during lockdown as a rapidly spreading pandemic a biomedical disease has serious physical and tremendous mental health implications an occupational community of internal migrant workers is one of the most vulnerable but neglected and is likely to develop psychological illeffects due to covid19s double whammy impact mental health is a crucial aspect that needs to be addressed during this lockdown as all modes of communication revolve around the virus there are many difficulties with the unprecedented changes that have occurred so quickly due to the pandemic and stayat home confinement to achieve social distance and mitigate the risk of infection these include impaired health wellbeing and sleep as a result of daily routine disruption anxiety worry isolation greater stress on family and work and excessive screen time an essential part of our overall health and wellbeing is mental and emotional health an important skill is managing emotions and maintaining emotional balance it helps you face challenges and stress when you manage your emotional health lack of skills in emotional regulation may lead to poor mental health and relationship difficulties it is as important to look after our mental health as it is to look after our physical health for mental health professionals the pandemic has also brought many ethical challenges,http://arxiv.org/abs/2102.09369v1,1
mothers of infants have specific demands in fostering emotional bonds with their children characterized by dynamics that are different from adultadult interactions notably requiring heightened maternal emotional regulation in this study we analyzed maternal emotional state by modeling maternal emotion regulation reflected in smiles the dataset comprises n94 videos of approximately 3 plus or minus 1minutes capturing free play interactions between 6 and 12monthold infants and their mothers corresponding demographic details of selfreported maternal mental health provide variables for determining mothers relations to emotions measured during free play in this work we employ diverse methodological approaches to explore the temporal evolution of maternal smiles our findings reveal a correlation between the temporal dynamics of mothers smiles and their emotional state furthermore we identify specific smile features that correlate with maternal emotional state thereby enabling informed inferences with existing literature on general smile analysis this study offers insights into emotional labor defined as the management of ones own emotions for the benefit of others and emotion regulation entailed in motherinfant interactions,http://arxiv.org/abs/2408.01434v1,1
we describe a set of experiments for building a temporal mental health dynamics system we utilise a preexisting methodology for distantsupervision of mental health data mining from social media platforms and deploy the system during the global covid19 pandemic as a case study despite the challenging nature of the task we produce encouraging results both explicit to the global pandemic and implicit to a global phenomenon christmas depression supported by the literature we propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decisionmaking,http://arxiv.org/abs/2008.13121v3,1
mental health support in colleges is vital in educating students by offering counseling services and organizing supportive events however evaluating its effectiveness faces challenges like data collection difficulties and lack of standardized metrics limiting research scope student feedback is crucial for evaluation but often relies on qualitative analysis without systematic investigation using advanced machine learning methods this paper uses public student voice survey data to analyze student sentiments on mental health support with large language models llms we created a sentiment analysis dataset smilecollege with humanmachine collaboration the investigation of both traditional machine learning methods and stateoftheart llms showed the best performance of gpt35 and bert on this new dataset the analysis highlights challenges in accurately predicting response sentiments and offers practical insights on how llms can enhance mental healthrelated research and improve college mental health services this datadriven approach will facilitate efficient and informed mental health support evaluation management and decisionmaking,http://arxiv.org/abs/2412.04326v1,1
there is a growing interest in hci to envision design and evaluate technologyenabled interventions that support users emotion regulation this interest stems in part from increased recognition that the ability to regulate emotions is critical to mental health and that a lack of effective emotion regulation is a transdiagnostic factor for mental illness however the potential to combine innovative hci designs with the theoretical grounding and stateofart interventions from psychology has yet to be fully realised in this paper we synthesise hci work on emotion regulation interventions and propose a threepart framework to guide technology designers in making i theoryinformed decisions about intervention targets ii strategic decisions regarding the technologyenabled intervention mechanisms to be included in the system and iii practical decisions around previous implementations of the selected intervention components we show how this framework can both systematise hci work to date and suggest a research agenda for future work,http://arxiv.org/abs/2204.00118v2,1
failure and resilience are important aspects of gameplay this is especially important for serious and competitive games where players need to adapt and cope with failure frequently in such situations emotion regulation the active process of modulating ones emotions to cope and adapt to challenging situations becomes essential it is one of the prominent aspects of human intelligence and promotes mental health and wellbeing while there has been work on developing artificial emotional regulation assistants to help users cope with emotion regulation in the field of intelligent tutoring systems little is done to incorporate such systems or ideas into serious video games in this paper we introduce a datadriven 6phase approach to establish empathetic artificial intelligence eai which operates on raw chat log data to detect key affective states identify common sequences and emotion regulation strategies and generalizes these to make them applicable for intervention systems,http://arxiv.org/abs/2302.09070v1,1
in this paper we provide causal evidence on abortions and risky health behaviors as determinants of mental health development among young women using administrative in and outpatient records from sweden we apply a novel grouped fixedeffects estimator proposed by bonhomme and manresa 2015 to allow for timevarying unobserved heterogeneity we show that the positive association obtained from standard estimators shrinks to zero once we control for grouped timevarying unobserved heterogeneity we estimate the groupspecific profiles of unobserved heterogeneity which reflect differences in unobserved risk to be diagnosed with a mental health condition we then analyze mental health development and risky health behaviors other than unwanted pregnancies across groups our results suggest that these are determined by the same type of unobserved heterogeneity which we attribute to the same unobserved process of decisionmaking we develop and estimate a theoretical model of risky choices and mental health in which mental health disparity across groups is generated by different degrees of selfcontrol problems our findings imply that mental health concerns cannot be used to justify restrictive abortion policies moreover potential selfcontrol problems should be targeted as early as possible to combat future mental health consequences,http://arxiv.org/abs/2103.12159v4,1
emotion regulation is a crucial element in dealing with emotional events and has positive effects on mental health this paper aims to provide a more comprehensive understanding of emotional events by introducing a new french corpus of emotional narratives collected using a questionnaire for emotion regulation we follow the theoretical framework of the component process model which considers emotions as dynamic processes composed of four interrelated components behavior feeling thinking and territory each narrative is related to a discrete emotion and is structured based on all emotion components by the writers we study the interaction of components and their impact on emotion classification with machine learning methods and pretrained language models our results show that each component improves prediction performance and that the best results are achieved by jointly considering all components our results also show the effectiveness of pretrained language models in predicting discrete emotion from certain components which reveal differences in how emotion components are expressed,http://arxiv.org/abs/2305.10446v1,1
in cognitive psychology automatic and selfreinforcing irrational thought patterns are known as cognitive distortions left unchecked patients exhibiting these types of thoughts can become stuck in negative feedback loops of unhealthy thinking leading to inaccurate perceptions of reality commonly associated with anxiety and depression in this paper we present a machine learning framework for the automatic detection and classification of 15 common cognitive distortions in two novel mental health free text datasets collected from both crowdsourcing and a realworld online therapy program when differentiating between distorted and nondistorted passages our model achieved a weighted f1 score of 088 for classifying distorted passages into one of 15 distortion categories our model yielded weighted f1 scores of 068 in the larger crowdsourced dataset and 045 in the smaller online counseling dataset both of which outperformed random baseline metrics by a large margin for both tasks we also identified the most discriminative words and phrases between classes to highlight common thematic elements for improving targeted and therapistguided mental health treatment furthermore we performed an exploratory analysis using unsupervised contentbased clustering and topic modeling algorithms as first efforts towards a datadriven perspective on the thematic relationship between similar cognitive distortions traditionally deemed unique finally we highlight the difficulties in applying mental healthbased machine learning in a realworld setting and comment on the implications and benefits of our framework for improving automated delivery of therapeutic treatment in conjunction with traditional cognitivebehavioral therapy,http://arxiv.org/abs/1909.07502v2,1
cognitive psychology delves on understanding perception attention memory language problemsolving decisionmaking and reasoning large language models llms are emerging as potent tools increasingly capable of performing humanlevel tasks the recent development in the form of gpt4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the llms to become perfect instruments of intelligence although gpt4 report has shown performance on some cognitive psychology tasks a comprehensive assessment of gpt4 via the existing wellestablished datasets is required in this study we focus on the evaluation of gpt4s performance on a set of cognitive psychology datasets such as commonsenseqa superglue math and hans in doing so we understand how gpt4 processes and integrates cognitive psychology with contextual information providing insight into the underlying cognitive processes that enable its ability to generate the responses we show that gpt4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior stateoftheart models our results strengthen the already available assessments and confidence on gpt4s cognitive psychology abilities it has significant potential to revolutionize the field of ai by enabling machines to bridge the gap between human and machine reasoning,http://arxiv.org/abs/2303.11436v2,1
after the pandemic artificial intelligence ai powered support for mental health care has become increasingly important the breadth and complexity of significant challenges required to provide adequate care involve a personalized patient understanding b safetyconstrained and medically validated chatbot patient interactions and c support for continued feedbackbased refinements in design using chatbotpatient interactions we propose alleviate a chatbot designed to assist patients suffering from mental health challenges with personalized care and assist clinicians with understanding their patients better alleviate draws from an array of publicly available clinically valid mentalhealth texts and databases allowing alleviate to make medically sound and informed decisions in addition alleviates modular design and explainable decisionmaking lends itself to robust and continued feedbackbased refinements to its design in this paper we explain the different modules of alleviate and submit a short video demonstrating alleviates capabilities to help patients and clinicians understand each other better to facilitate optimal care strategies,http://arxiv.org/abs/2304.00025v1,1
emerging psychopathology studies are showing that patterns of changes in emotional state emotion dynamics are associated with overall wellbeing and mental health more recently there has been some work in tracking emotion dynamics through ones utterances allowing for data to be collected on a larger scale across time and people however several questions about how emotion dynamics change with age especially in children and when determined through childrens writing remain unanswered in this work we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages we show that both approaches point to similar trends consistent increasing intensities for some emotions eg anger fear joy sadness arousal and dominance with age and a consistent decreasing valence with age we also find increasing emotional variability rise rates ie emotional reactivity and recovery rates ie emotional regulation with age these results act as a useful baselines for further research in how patterns of emotions expressed by children change with age and their association with mental health,http://arxiv.org/abs/2306.05387v1,1
the ability to regulate and cope with strong emotions is essential for maintaining our mental health and wellbeing however learning how to emotionally regulate can be a bit of a mystery since it is largely an invisible process and it can be difficult to conjure up strong emotions to practice regulating them this is where virtual reality or vr comes in vr is a computergenerated 3d environment where the user experiences a simulated world through 360 visuals stereo audio and 3d interaction with tracking sensors vr is a very visceral experience that feels real even though you know it isnt if a virtual ball came flying at your head you would duck my past research shows that we can provide vr experiences that elicit strong emotional reactions so that people can practice coping and regulating their emotional responses for example i helped create a vr experience of being in nature and then going into space to see the earth it created the emotional reaction of awe and wonder that led to a deeper connection with our planet this shows that emotional vr experiences can impact our emotions and behaviour both in vr and beyond my research proposal is to investigate the feasibility of emotion regulation skills development in vr with teenagers the idea is to simulate emotional experiences like the 1st day of high school as a means to develop emotion regulation skills so that they will be able to better cope with their emotions i will lead the design development and evaluation of this vr experience and work directly with youth to meet their needs this proof of concept prototype is the first step in developing a vr platform that provides youth with an effective way to regulate their emotions and improve their mental health from their own homes which will lead to improvements in education socioemotional and economic outcomes for youth in canada and globally,http://arxiv.org/abs/2212.00002v1,1
recently foundational issues of applicability of the formalism of quantum mechanics qm to cognitive psychology decision making and psychophysics attracted a lot of interest in particular in citedkbb the possibility to use of the projection postulate and representation of mental observables by hermitian operators was discussed in very detail the main conclusion of the recent discussions on the foundations of quantumlike cognitive psychology is that one has to be careful in determination of conditions of applicability of the projection postulate as a mathematical tool for description of measurements of observables represented by hermitian operators to represent some statistical experimental data both physical and mental in the quantumlike way one has to use generalized quantum observables given by positive operatorvalued measures povms this paper contains a brief review on povms which can be useful for newcomers to the field of quantumlike studies especially interesting for cognitive psychology is a variant of the formula of total probability ftp with the interference term derived for incompatible observables given by povms we present an interpretation of the interference term from the psychological viewpoint as was shown before the appearance of such a term perturbing classical ftp plays the important role in cognitive psychology eg recognition of ambiguous figures and the disjunction effect the interference term for observables given by povms has much more complicated structure than the corresponding term for observables given by hermitian operators we elaborate cognitive interpretations of different components of the povmsinterference term and apply our analysis to a quantumlike model of decision making,http://arxiv.org/abs/1405.1269v1,1
automatic depression detection on twitter can help individuals privately and conveniently understand their mental health status in the early stages before seeing mental health professionals most existing blackboxlike deep learning methods for depression detection largely focused on improving classification performance however explaining model decisions is imperative in health research because decisionmaking can often be highstakes and lifeanddeath reliable automatic diagnosis of mental health problems including depression should be supported by credible explanations justifying models predictions in this work we propose a novel explainable model for depression detection on twitter it comprises a novel encoder combining hierarchical attention mechanisms and feedforward neural networks to support psycholinguistic studies our model leverages metaphorical concept mappings as input thus it not only detects depressed individuals but also identifies features of such users tweets and associated metaphor concept mappings,http://arxiv.org/abs/2209.07494v1,1
we are united in how emotions are central to shaping our experiences and yet individuals differ greatly in how we each identify categorize and express emotions in psychology variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity determined through selfreports of ones emotions high emotion granularity has been linked with better mental and physical health whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes in this work we propose computational measures of emotion granularity derived from temporallyordered speaker utterances in social media in lieu of selfreports that suffer from various biases we then investigate the effectiveness of such textderived measures of emotion granularity in functioning as markers of various mental health conditions mhcs we establish baseline measures of emotion granularity derived from textual utterances and show that at an aggregate level emotion granularities are significantly lower for people selfreporting as having an mhc than for the control population this paves the way towards a better understanding of the mhcs and specifically the role emotions play in our wellbeing,http://arxiv.org/abs/2403.02281v2,1
we study gpt3 a recent large language model using tools from cognitive psychology more specifically we assess gpt3s decisionmaking information search deliberation and causal reasoning abilities on a battery of canonical experiments from the literature we find that much of gpt3s behavior is impressive it solves vignettebased tasks similarly or better than human subjects is able to make decent decisions from descriptions outperforms humans in a multiarmed bandit task and shows signatures of modelbased reinforcement learning yet we also find that small perturbations to vignettebased tasks can lead gpt3 vastly astray that it shows no signatures of directed exploration and that it fails miserably in a causal reasoning task these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents,http://arxiv.org/abs/2206.14576v1,1
as the prevalence of mental health challenges social media has emerged as a key platform for individuals to express their emotionsdeep learning tends to be a promising solution for analyzing mental health on social media however black box models are often inflexible when switching between tasks and their results typically lack explanations with the rise of large language models llms their flexibility has introduced new approaches to the field also due to the generative nature they can be prompted to explain decisionmaking processes however their performance on complex psychological analysis still lags behind deep learning in this paper we introduce the first multitask chinese social media interpretable mental health instructions cimhi dataset consisting of 9k samples which has been qualitycontrolled and manually validated we also propose mentalglm series models the first opensource llms designed for explainable mental health analysis targeting chinese social media trained on a corpus of 50k instructions the proposed models were evaluated on three downstream tasks and achieved better or comparable performance compared to deep learning models generalized llms and task finetuned llms we validated a portion of the generated decision explanations with experts showing promising results we also evaluated the proposed models on a clinical dataset where they outperformed other llms indicating their potential applicability in the clinical field our models show strong performance validated across tasks and perspectives the decision explanations enhance usability and facilitate better understanding and practical application of the models both the constructed dataset and the models are publicly available via,http://arxiv.org/abs/2410.10323v1,1
sleep is known to be a key factor in emotional regulation and overall mental health in this study we explore the integration of sleep measures from the previous night into wearablebased mood recognition to this end we propose naptune a novel prompttuning framework that utilizes sleeprelated measures as additional inputs to a frozen pretrained wearable timeseries encoder by adding and training lightweight prompt parameters to each transformer layer through rigorous empirical evaluation we demonstrate that the inclusion of sleep data using naptune not only improves mood recognition performance across different wearable timeseries namely ecg ppg and eda but also makes it more sampleefficient our method demonstrates significant improvements over the best baselines and unimodal variants furthermore we analyze the impact of adding sleeprelated measures on recognizing different moods as well as the influence of individual sleeprelated measures,http://arxiv.org/abs/2409.04723v1,1
the premise of this working paper is based around agentbased simulation models and how to go about creating them from given incomplete information agentbased simulations are stochastic simulations that revolve around groups of agents that each have their own characteristics and can make decisions such simulations can be used to emulate real life situations and to create hypothetical situations without the need for realworld testing prior here we describe the development of an agentbased simulation model for studying future digital mental health scenarios an incomplete conceptual model has been used as the basis for this development to define differences in responses to stimuli we employed fuzzy decision making logic the model has been implemented but not been used for structured experimentation yet this is planned as our next step,http://arxiv.org/abs/1902.01642v1,1
phd students report a higher prevalence of mental illness symptoms than highly educated individuals in the general population this situation presents a serious problem for universities thus the knowledge about this phenomenon is of great importance in decisionmaking in this paper we use the nature phd survey 2019 and estimate several binomial logistic regression models to analyze the risk of interrupting doctoral studies this risk is measured through the desire of change in either the supervisor or the area of expertise or the wish of not pursue a phd among the explanatory factors we focus on the influence of anxietydepression discrimination and bullying as control variables we use demographic characteristics and others related with the doctoral program insufficient contact time with supervisors and exceeding time spent studying crossing the 50h week barrier are risk factors of phd studies interruption but the most decisive risk factor is poor mental health universities should therefore foster an environment of wellbeing which allows the development of autonomy and resilience of their phd students or when necessary which fosters the development of conflict resolution skills,http://arxiv.org/abs/2010.07039v1,1
depression is a common mental health issue that requires prompt diagnosis and treatment despite the promise of social media data for depression detection the opacity of employed deep learning models hinders interpretability and raises bias concerns we address this challenge by introducing protodep a novel explainable framework for twitterbased depression detection protodep leverages prototype learning and the generative power of large language models to provide transparent explanations at three levels i symptomlevel explanations for each tweet and user ii casebased explanations comparing the user to similar individuals and iii transparent decisionmaking through classification weights evaluated on five benchmark datasets protodep achieves near stateoftheart performance while learning meaningful prototypes this multifaceted approach offers significant potential to enhance the reliability and transparency of depression detection on social media ultimately aiding mental health professionals in delivering more informed care,http://arxiv.org/abs/2407.21041v1,1
exploration of the physical environment is an indispensable precursor to data acquisition and enables knowledge generation via analytical or direct trialing artificial intelligence lacks the exploratory capabilities of even the most underdeveloped organisms hindering its autonomy and adaptability supported by cognitive psychology this works links human behavior and artificial agents to endorse selfdevelopment in accordance with reported data paradigms of epistemic and achievement emotion are embedded to machinelearning methodology contingent on their impact when decision making a study is subsequently designed to mirror previous human trials which artificial agents are made to undergo repeatedly towards convergence results demonstrate causality learned by the vast majority of agents between their internal states and exploration to match those reported for human counterparts the ramifications of these findings are pondered for both research into human cognition and betterment of artificial intelligence,http://arxiv.org/abs/2302.06615v1,1
this review aims to contribute to the quest for artificial general intelligence by examining neuroscience and cognitive psychology methods for potential inspiration despite the impressive advancements achieved by deep learning models in various domains they still have shortcomings in abstract reasoning and causal understanding such capabilities should be ultimately integrated into artificial intelligence systems in order to surpass datadriven limitations and support decision making in a way more similar to human intelligence this work is a vertical review that attempts a wideranging exploration of brain function spanning from lowerlevel biological neurons spiking neural networks and neuronal ensembles to higherlevel concepts such as brain anatomy vector symbolic architectures cognitive and categorization models and cognitive architectures the hope is that these concepts may offer insights for solutions in artificial general intelligence,http://arxiv.org/abs/2401.10904v1,1
global rates of mental health concerns are rising and there is increasing realization that existing models of mental health care will not adequately expand to meet the demand with the emergence of large language models llms has come great optimism regarding their promise to create novel largescale solutions to support mental health despite their nascence llms have already been applied to mental health related tasks in this paper we summarize the extant literature on efforts to use llms to provide mental health education assessment and intervention and highlight key opportunities for positive impact in each area we then highlight risks associated with llms application to mental health and encourage the adoption of strategies to mitigate these risks the urgent need for mental health support must be balanced with responsible development testing and deployment of mental health llms it is especially critical to ensure that mental health llms are finetuned for mental health enhance mental health equity and adhere to ethical standards and that people including those with lived experience with mental health concerns are involved in all stages from development through deployment prioritizing these efforts will minimize potential harms to mental health and maximize the likelihood that llms will positively impact mental health globally,http://arxiv.org/abs/2403.14814v3,1
recently people started to understand that applications of the mathematical formalism of quantum theory are not reduced to physics nowadays this formalism is widely used outside of quantum physics in particular in cognition psychology decision making information processing especially information retrieval the latter is very promising the aim of this brief introductory review is to stimulate research in this exciting area of information science this paper is not aimed to present a complete review on the state of art in quantum information retrieval,http://arxiv.org/abs/2008.13541v1,1
cognitive psychology investigates perception attention memory language problemsolving decisionmaking and reasoning kahnemans dualsystem theory elucidates the human decisionmaking process distinguishing between the rapid intuitive system 1 and the deliberative rational system 2 recent advancements have positioned large language models llms as formidable tools nearing humanlevel proficiency in various cognitive tasks nonetheless the presence of a dualsystem framework analogous to human cognition in llms remains unexplored this study introduces the textbfcognidual framework for llms cfllms designed to assess whether llms can through selftraining evolve from deliberate deduction to intuitive responses thereby emulating the human process of acquiring and mastering new information our findings reveal the cognitive mechanisms behind llms response generation enhancing our understanding of their capabilities in cognitive psychology practically selftrained models can provide faster responses to certain queries reducing computational demands during inference,http://arxiv.org/abs/2409.03381v2,1
in recent years quantum mechanics has been actively used in areas outside of physics such as psychology sociology theory of decisionmaking game theory and others in particular quantum mechanics is used to explain the paradoxes arising in cognitive psychology and decision making wang and busemeyer invented a quantum model and approach as well as nonparametric equality socalled qqequality explaining the questions order effect the primary objective of this note is to test the possibility to expand the wangbusemeyer model by considering questions which are mathematically represented by positive operator valued measures we found that for such observables the qqequality can be violated but we also showed that in principle it is possible to reduce expanded model to the original wangbusemeyer model by expanding the context of the questions this version of preprint is aimed to point out to annoying miscalculation in version 1 this miscalculation might mislead a reader who is not experienced in operating with povms otherwise the main line of construction and reasoning presented in version 1 is right and it can be easily completed by the reader on the basis of version 1 and the correction remark in version 2,http://arxiv.org/abs/1811.00045v2,1
this research draws upon cognitive psychology and information systems studies to anticipate user engagement and decisionmaking on digital platforms by employing natural language processing nlp techniques and insights from cognitive bias research we delve into user interactions with synonyms within digital content our methodology synthesizes four cognitive biasesrepresentativeness easeofuse affect and distributioninto the read model through a comprehensive user survey we assess the models ability to predict user engagement discovering that synonyms that accurately represent core ideas are easy to understand elicit emotional responses and are commonly encountered promote greater user engagement crucially our work offers a fresh lens on humancomputer interaction digital behaviors and decisionmaking processes our results highlight the promise of cognitive biases as potent indicators of user engagement underscoring their significance in designing effective digital content across fields like education and marketing,http://arxiv.org/abs/2307.14511v1,1
depression has been the leading cause of mentalhealth illness worldwide major depressive disorder mdd is a common mental health disorder that affects both psychologically as well as physically which could lead to loss of lives due to the lack of diagnostic tests and subjectivity involved in detecting depression there is a growing interest in using behavioural cues to automate depression diagnosis and stage prediction the absence of labelled behavioural datasets for such problems and the huge amount of variations possible in behaviour makes the problem more challenging this paper presents a novel multilevel attention based network for multimodal depression prediction that fuses features from audio video and text modalities while learning the intra and inter modality relevance the multilevel attention reinforces overall learning by selecting the most influential features within each modality for the decision making we perform exhaustive experimentation to create different regression models for audio video and text modalities several fusions models with different configurations are constructed to understand the impact of each feature and modality we outperform the current baseline by 1752 in terms of root mean squared error,http://arxiv.org/abs/1909.01417v1,1
diverse cases regarding the impact with its related factors of the covid19 pandemic on mental health have been reported in previous studies college student groups have been frequently selected as the target population in previous studies because they are easily affected by pandemics in this study multivariable datasets were collected from 751 college students based on the complex relationships between various mental health factors we utilized quantum annealing qabased feature selection algorithms that were executed by commercial dwave quantum computers to determine the changes in the relative importance of the associated factors before and after the pandemic multivariable linear regression mlr and xgboost models were also applied to validate the qabased algorithms based on the experimental results we confirm that qabased algorithms have comparable capabilities in factor analysis research to the mlr models that have been widely used in previous studies furthermore the performance of the qabased algorithms was validated through the important factor results from the algorithms pandemicrelated factors eg confidence in the social system and psychological factors eg decisionmaking in uncertain situations were more important in postpandemic conditions we believe that our study will serve as a reference for researchers studying similar topics,http://arxiv.org/abs/2310.00018v1,1
passively collected behavioral health data from ubiquitous sensors holds significant promise to provide mental health professionals insights from patients daily lives however developing analysis tools to use this data in clinical practice requires addressing challenges of generalization across devices and weak or ambiguous correlations between the measured signals and an individuals mental health to address these challenges we take a novel approach that leverages large language models llms to synthesize clinically useful insights from multisensor data we develop chain of thought prompting methods that use llms to generate reasoning about how trends in data such as step count and sleep relate to conditions like depression and anxiety we first demonstrate binary depression classification with llms achieving accuracies of 611 which exceed the state of the art while it is not robust for clinical use this leads us to our key finding even more impactful and valued than classification is a new humanai collaboration approach in which clinician experts interactively query these tools and combine their domain expertise and context about the patient with ai generated reasoning to support clinical decisionmaking we find models like gpt4 correctly reference numerical data 75 of the time and clinician participants express strong interest in using this approach to interpret selftracking data,http://arxiv.org/abs/2311.13063v3,1
bayesian nonparametric estimates of australian mental health distributions are obtained to assess how the mental health status of the population has changed over time and to compare the mental health status of femalemale and indigenousnonindigenous population subgroups first and secondorder stochastic dominance are used to compare distributions with results presented in terms of the posterior probability of dominance and the posterior probability of no dominance our results suggest mental health has deteriorated in recent years that males mental health status is better than that of females and nonindigenous health status is better than that of the indigenous population,http://arxiv.org/abs/2106.08047v1,1
childhood sexual abuse csa is a menace to society and has longlasting effects on the mental health of the survivors from time to time csa survivors are haunted by various mental health issues in their lifetime proper care and attention towards csa survivors facing mental health issues can drastically improve the mental health conditions of csa survivors previous works leveraging online social media osm data for understanding mental health issues havent focused on mental health issues in individuals with csa background our work fills this gap by studying reddit posts related to csa to understand their mental health issues mental health issues such as depression anxiety and posttraumatic stress disorder ptsd are most commonly observed in posts with csa background observable differences exist between posts related to mental health issues with and without csa background keeping this difference in mind for identifying mental health issues in posts with csa exposure we develop a twostage framework the first stage involves classifying posts with and without csa background and the second stage involves recognizing mental health issues in posts that are classified as belonging to csa background the top model in the first stage is able to achieve accuracy and f1score macro of 9626 and 9624 and in the second stage the top model reports hamming score of 6709 content warning reader discretion is recommended as our study tackles topics such as child sexual abuse molestation etc,http://arxiv.org/abs/2306.10338v1,1
mental health stigma prevents many individuals from receiving the appropriate care and social psychology studies have shown that mental health tends to be overlooked in men in this work we investigate gendered mental health stigma in masked language models in doing so we operationalize mental health stigma by developing a framework grounded in psychology research we use clinical psychology literature to curate prompts then evaluate the models propensity to generate gendered words we find that masked language models capture societal stigma about gender in mental health models are consistently more likely to predict female subjects than male in sentences about having a mental health condition 32 vs 19 and this disparity is exacerbated for sentences that indicate treatmentseeking behavior furthermore we find that different models capture dimensions of stigma differently for men and women associating stereotypes like anger blame and pity more with women with mental health conditions than with men in showing the complex nuances of models gendered mental health stigma we demonstrate that context and overlapping dimensions of identity are important considerations when assessing computational models social biases,http://arxiv.org/abs/2210.15144v2,1
probabilistic graphical models such as bayesian networks are one of the most powerful structures known by the computer science community for deriving probabilistic inferences however modern cognitive psychology has revealed that human decisions could not follow the rules of classical probability theory because humans cannot process large amounts of data in order to make judgements consequently the inferences performed are based on limited data coupled with several heuristics leading to violations of the law of total probability this means that probabilistic graphical models based on classical probability theory are too limited to fully simulate and explain various aspects of human decision making quantum probability theory was developed in order to accommodate the paradoxical findings that the classical theory could not explain recent findings in cognitive psychology revealed that quantum probability can fully describe human decisions in an elegant framework their findings suggest that before taking a decision human thoughts are seen as superposed waves that can interfere with each other influencing the final decision in this work we propose a new bayesian network based on the psychological findings of cognitive scientists we made experiments with two very well known bayesian networks from the literature the results obtained revealed that the quantum like bayesian network can affect drastically the probabilistic inferences specially when the levels of uncertainty of the network are very high no pieces of evidence observed when the levels of uncertainty are very low then the proposed quantum like network collapses to its classical counterpart,http://arxiv.org/abs/1409.8470v1,1
software testing is a complex intellectual activity based at least on analysis reasoning decision making abstraction and collaboration performed in a highly demanding environment naturally it uses and allocates multiple cognitive resources in software testers however while a cognitive psychology perspective is increasingly used in the general software engineering literature it has yet to find its place in software testing to the best of our knowledge no theory of software testers cognitive processes exists here we take the first step towards such a theory by presenting a cognitive model of software testing based on how problem solving is conceptualized in cognitive psychology our approach is to instantiate a general problem solving process for the specific problem of creating test cases we then propose an experiment for testing our cognitive test design model the experiment makes use of verbal protocol analysis to understand the mechanisms by which human testers choose design implement and evaluate test cases an initial evaluation was then performed with five software engineering master students as subjects the results support a problem solvingbased model of test design for capturing testers cognitive processes,http://arxiv.org/abs/2007.08927v3,1
several examples of cyberphysical human systems cphs include realtime decisions from humans as a necessary building block for the successful performance of the overall system many of these decisionmaking problems necessitate an appropriate model of human behavior tools from utility theory have been used successfully in several problems in transportation for resource allocation and balance of supply and demand citepben1985discrete more recently prospect theory has been demonstrated as a useful tool in behavioral economics and cognitive psychology for deriving human behavioral models that characterize their subjective decisionmaking in the presence of stochastic uncertainties and risks as an alternative to conventional utility theory citepkahnemanprospect2012 these models will be described in this article theoretical implications of prospect theory are also discussed examples will be drawn from transportation use cases such as shared mobility to illustrate these models as well as the distinctions between utility theory and prospect theory,http://arxiv.org/abs/2210.07322v1,1
large language models llms have gradually become the gateway for people to acquire new knowledge however attackers can break the models security protection jail to access restricted information which is called jailbreaking previous studies have shown the weakness of current llms when confronted with such jailbreaking attacks nevertheless comprehension of the intrinsic decisionmaking mechanism within the llms upon receipt of jailbreak prompts is noticeably lacking our research provides a psychological explanation of the jailbreak prompts drawing on cognitive consistency theory we argue that the key to jailbreak is guiding the llm to achieve cognitive coordination in an erroneous direction further we propose an automatic blackbox jailbreaking method based on the footinthedoor fitd technique this method progressively induces the model to answer harmful questions via multistep incremental prompts we instantiated a prototype system to evaluate the jailbreaking effectiveness on 8 advanced llms yielding an average success rate of 839 this study builds a psychological perspective on the explanatory insights into the intrinsic decisionmaking logic of llms,http://arxiv.org/abs/2402.15690v1,1
more than 1 million students play high school american football annually but many health professionals have recently questioned its safety or called for its ban these concerns have been partially driven by reports of chronic traumatic encephalopathy cte increased risks of neurodegenerative disease and associations between concussion history and laterlife cognitive impairment and depression among retired professional football players a recent observational study of a cohort of men who graduated from a wisconsin high school in 1957 found no statistically significant harmful effects of playing high school football on a range of cognitive psychological and socioeconomic outcomes measured at ages 35 54 65 and 72 unfortunately these findings may not generalize to younger populations thanks to changes and improvements in football helmet technology and training techniques in particular these changes may have led to increased perceptions of safety but ultimately more dangerous styles of play characterized by the frequent subconcussive impacts thought to be associated with laterlife neurological decline in this work we replicate the methodology of that earlier matched observational study using data from the national longitudinal study of adolescent to adult health add health these include adolescent and family comorbidities academic experience selfreported levels of general health and physical activity and the score on the add health picture vocabulary test our primary outcome is the cesd score measured in 2008 when subjects were aged 24 34 and settling into early adulthood we also examine several secondary outcomes related to physical and psychological health including suicidality our results can provide insight into the natural history of potential footballrelated decline and dysfunction,http://arxiv.org/abs/1808.03934v2,1
mental health challenges are thought to afflict around 10 of the global population each year with many going untreated due to stigma and limited access to services here we explore trends in words and phrases related to mental health through a collection of 1 2 and 3grams parsed from a data stream of roughly 10 of all english tweets since 2012 we examine temporal dynamics of mental health language finding that the popularity of the phrase mental health increased by nearly two orders of magnitude between 2012 and 2018 we observe that mentions of mental health spike annually and reliably due to mental health awareness campaigns as well as unpredictably in response to mass shootings celebrities dying by suicide and popular fictional stories portraying suicide we find that the level of positivity of messages containing mental health while stable through the growth period has declined recently finally we use the ratio of original tweets to retweets to quantify the fraction of appearances of mental health language due to social amplification since 2015 mentions of mental health have become increasingly due to retweets suggesting that stigma associated with discussion of mental health on twitter has diminished with time,http://arxiv.org/abs/2106.01481v1,1
mental health remains a significant challenge of public health worldwide with increasing popularity of online platforms many use the platforms to share their mental health conditions express their feelings and seek help from the community and counselors some of these platforms such as reachout are dedicated forums where the users register to seek help others such as reddit provide subreddits where the users publicly but anonymously post their mental health distress although posts are of varying length it is beneficial to provide a short but informative summary for fast processing by the counselors to facilitate research in summarization of mental health online posts we introduce mental health summarization dataset mentsum containing over 24k carefully selected user posts from reddit along with their short userwritten summary called tldr in english from 43 mental health subreddits this domainspecific dataset could be of interest not only for generating short summaries on reddit but also for generating summaries of posts on the dedicated mental health forums such as reachout we further evaluate both extractive and abstractive stateoftheart summarization baselines in terms of rouge scores and finally conduct an indepth human evaluation study of both userwritten and systemgenerated summaries highlighting challenges in this research,http://arxiv.org/abs/2206.00856v1,1
despite the everstrong demand for mental health care globally access to traditional mental health services remains severely limited expensive and stifled by stigma and systemic barriers thus over the last few years young people are increasingly turning to content on videosharing platforms vsps like tiktok and youtube to help them navigate their mental health journey however navigating towards trustworthy information relating to mental health on these platforms is challenging given the uncontrollable and unregulated growth of dedicated mental health content and content creators catering to a wide array of mental health conditions on these platforms in this paper we attempt to define what constitutes as mental health misinformation through examples in addition we also suggest some open questions to answer and challenges to tackle regarding this important and timely research topic,http://arxiv.org/abs/2304.07417v1,1
this chapter discusses the existing and future use of robotics and intelligent sensing technology in mental health care while the use of this technology is nascent in mental health care it represents a potentially useful tool in the practitioners toolbox the goal of this chapter is to provide a brief overview of the field discuss the recent use of robotics technology in mental health care practice explore some of the design issues and ethical issues of using robots in this space and finally to explore the potential of emerging technology,http://arxiv.org/abs/1511.02281v1,1
this research paper presents a metaanalysis of the multifaceted role of technology in mental health the pervasive influence of technology on daily lives necessitates a deep understanding of its impact on mental health services this study synthesizes literature covering behavioral intervention technologies bits digital mental health interventions during covid19 young mens attitudes toward mental health technologies technologybased interventions for university students and the applicability of mobile health technologies for individuals with serious mental illnesses bits are recognized for their potential to provide evidencebased interventions for mental health conditions especially anxiety disorders the covid19 pandemic acted as a catalyst for the adoption of digital mental health services underscoring their crucial role in providing accessible and quality care however their efficacy needs to be reinforced by workforce training highquality evidence and digital equity a nuanced understanding of young mens attitudes toward mental health is imperative for devising effective online services technologybased interventions for university students are promising although variable in effectiveness their deployment must be evidencebased and tailored to individual needs mobile health technologies particularly activity tracking hold promise for individuals with serious mental illnesses collectively technology has immense potential to revolutionize mental health care however the implementation must be evidencebased ethical and equitable with continued research focusing on experiences across diverse populations ensuring accessibility and efficacy for all,http://arxiv.org/abs/2307.10513v2,1
effective surveillance on the longterm public health impact due to war and terrorist attacks remain limited such health issues are commonly underreported specifically for a large group of individuals for this purpose efficient estimation of the size of the population under the risk of physical and mental health hazards is of utmost necessity in this context multiple system estimation is a potential strategy that has recently been applied to quantify underreported events allowing heterogeneity among the individuals and dependence between the sources of information to model such complex phenomena a novel trivariate bernoulli model is developed and an estimation methodology using monte carlo based em algorithm is proposed simulation results show superiority of the performance of the proposed method over existing competitors and robustness under model misspecifications the method is applied to analyze real case studies on the gulf war and 911 terrorist attack at world trade center us the results provide interesting insights that can assist in effective decision making and policy formulation for monitoring the health status of postwar survivors,http://arxiv.org/abs/2208.11992v4,1
mental manipulation severely undermines mental wellness by covertly and negatively distorting decisionmaking while there is an increasing interest in mental health care within the natural language processing community progress in tackling manipulation remains limited due to the complexity of detecting subtle covert tactics in conversations in this paper we propose intentaware prompting iap a novel approach for detecting mental manipulations using large language models llms providing a deeper understanding of manipulative tactics by capturing the underlying intents of participants experimental results on the mentalmanip dataset demonstrate superior effectiveness of iap against other advanced prompting strategies notably our approach substantially reduces false negatives helping detect more instances of mental manipulation with minimal misjudgment of positive cases the code of this paper is available at,http://arxiv.org/abs/2412.08414v1,1
predictive machine learning ml models are computational innovations that can enhance medical decisionmaking including aiding in determining optimal timing for discharging patients however societal biases can be encoded into such models raising concerns about inadvertently affecting health outcomes for disadvantaged groups this issue is particularly pressing in the context of substance use disorder sud treatment where biases in predictive models could significantly impact the recovery of highly vulnerable patients in this study we focus on the development and assessment of ml models designed to predict the length of stay los for both inpatients ie residential and outpatients undergoing sud treatment we utilize the treatment episode data set for discharges tedsd from the substance abuse and mental health services administration samhsa through the lenses of distributive justice and sociorelational fairness we assess our models for bias across variables related to demographics eg race as well as medical eg diagnosis and financial conditions eg insurance we find that race us geographic region type of substance used diagnosis and payment source for treatment are primary indicators of unfairness from a policy perspective we provide bias mitigation strategies to achieve fair outcomes we discuss the implications of these findings for medical decisionmaking and health equity we ultimately seek to contribute to the innovation and policymaking literature by seeking to advance the broader objectives of social justice when applying computational innovations in health care,http://arxiv.org/abs/2412.05832v1,1
mental health is a significant and growing public health concern as language usage can be leveraged to obtain crucial insights into mental health conditions there is a need for largescale labeled mental healthrelated datasets of users who have been diagnosed with one or more of such conditions in this paper we investigate the creation of highprecision patterns to identify selfreported diagnoses of nine different mental health conditions and obtain highquality labeled data without the need for manual labelling we introduce the smhd selfreported mental health diagnoses dataset and make it available smhd is a novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users we examine distinctions in users language as measured by linguistic and psychological variables we further explore text classification methods to identify individuals with mental conditions through their language,http://arxiv.org/abs/1806.05258v2,1
background mental health problems are prevalent in college students the covid19 pandemic exacerbated the problems and created a surge in the popularity of telehealth and mobile health solutions despite that mobile health is a promising approach to help students with mental health needs few studies exist in investigating key features students need in a mental health selfmanagement tool objective the objective of our study was to identified key requirements and features for the design of a studentcentered mental health selfmanagement tool methods an interview study was first conducted to understand college students needs and preferences on a mental health selfmanagement tool functional information requirement analysis was then conducted to translate the needs into design implications results a total of 153 university students were recruited for the semistructured interview the participants mentioned several features including coping techniques artificial intelligence time management tracking and communication with others participants preferences on usability and privacy settings were also collected the desired functions were analyzed and turned into designagnostic information requirements conclusions this study documents findings from interviews with university students to understand their needs and preferences for a tool to help with selfmanagement of mental health,http://arxiv.org/abs/2206.02960v1,1
selfdisclosed mental health diagnoses which serve as ground truth annotations of mental health status in the absence of clinical measures underpin the conclusions behind most computational studies of mental health language from the last decade however psychiatric conditions are dynamic a prior depression diagnosis may no longer be indicative of an individuals mental health either due to treatment or other mitigating factors we ask to what extent are selfdisclosures of mental health diagnoses actually relevant over time we analyze recent activity from individuals who disclosed a depression diagnosis on social media over five years ago and in turn acquire a new understanding of how presentations of mental health status on social media manifest longitudinally we also provide expanded evidence for the presence of personalityrelated biases in datasets curated using selfdisclosed diagnoses our findings motivate three practical recommendations for improving mental health datasets curated using selfdisclosed diagnoses 1 annotate diagnosis dates and psychiatric comorbidities 2 sample control groups using propensity score matching 3 identify and remove spurious correlations introduced by selection bias,http://arxiv.org/abs/2206.11155v1,1
mental health disorders are particularly prevalent among those in the criminal justice system and may be a contributing factor in recidivism using north carolina court cases from 1994 to 2009 this paper evaluates how mandated mental health treatment as a term of probation impacts the likelihood that individuals return to the criminal justice system i use random variation in judge assignment to compare those who were required to seek weekly mental health counseling to those who were not the main findings are that being assigned to seek mental health treatment decreases the likelihood of threeyear recidivism by about 12 percentage points or 36 percent this effect persists over time and is similar among various types of individuals on probation in addition i show that mental health treatment operates distinctly from drug addiction interventions in a multipletreatment framework i provide evidence that mental health treatments longerterm effectiveness is strongest among more financiallyadvantaged probationers consistent with this setting in which the cost of mandated treatment is shouldered by offenders finally conservative calculations result in a 51 benefittocost ratio which suggests that the treatmentinduced decrease in future crime would be more than sufficient to offset the costs of treatment,http://arxiv.org/abs/2212.06736v2,1
long counseling text generation for mental health support ltgm an innovative and challenging task aims to provide helpseekers with mental health support through a comprehensive and more acceptable response the combination of chainofthought cot prompting and large language models llms is employed and get the sota performance on various nlp tasks especially on text generation tasks zeroshot cot prompting is one of the most common methods in cot prompting however in the ltgm task zeroshot cot prompting can not simulate a counselor or provide personalized strategies without effective mental health counseling strategy prompts to tackle this challenge we propose a zeroshot dynamic strategy chain dsc prompting method firstly we utilize gpt2 to learn the responses written by mental health counselors and dynamically generate mental health counseling strategies tailored to the helpseekers needs secondly the zeroshot dsc prompting is constructed according to mental health counseling strategies and the helpseekers post finally the zeroshot dsc prompting is employed to guide llms in generating more humanlike responses for the helpseekers both automatic and manual evaluations demonstrate that zeroshot dsc prompting can deliver more humanlike responses than cot prompting methods on ltgm tasks,http://arxiv.org/abs/2308.10444v1,1
housing instability is a widespread phenomenon in the united states in combination with other social determinants of health housing instability affects childrens overall health and development drawing on data from the 2022 national survey of childrens health we employed multiple logistic regression models to understand how sociodemographic factors especially housing instability affect mental health outcomes and treatment access for youth aged 617 years our results show that youth facing housing instability have a higher likelihood of experiencing anxiety or 142 p0001 and depression or 157 p0001 furthermore youth experiencing both mental health conditions and housing instability are significantly less likely to receive mental health services in the past year indicating the substantial barriers they face in accessing mental health care based on our findings we highlight opportunities for digital mental health interventions to provide children experiencing housing instability with more accessible and consistent mental health services,http://arxiv.org/abs/2409.06011v2,1
large language models llms are increasingly integrated into various medical fields including mental health support systems however there is a gap in research regarding the effectiveness of llms in nonenglish mental health support applications to address this problem we present a novel multilingual adaptation of widelyused mental health datasets translated from english into six languages greek turkish french portuguese german and finnish this dataset enables a comprehensive evaluation of llm performance in detecting mental health conditions and assessing their severity across multiple languages by experimenting with gpt and llama we observe considerable variability in performance across languages despite being evaluated on the same translated dataset this inconsistency underscores the complexities inherent in multilingual mental health support where languagespecific nuances and mental health data coverage can affect the accuracy of the models through comprehensive error analysis we emphasize the risks of relying exclusively on large language models llms in medical settings eg their potential to contribute to misdiagnoses moreover our proposed approach offers significant cost savings for multilingual tasks presenting a major advantage for broadscale implementation,http://arxiv.org/abs/2409.17397v1,1
large language models llms are increasingly used in medical fields in mental health support the early identification of linguistic markers associated with mental health conditions can provide valuable support to mental health professionals and reduce long waiting times for patients despite the benefits of llms for mental health support there is limited research on their application in mental health systems for languages other than english our study addresses this gap by focusing on the detection of depression severity in greek through usergenerated posts which are automatically translated from english our results show that gpt35turbo is not very successful in identifying the severity of depression in english and it has a varying performance in greek as well our study underscores the necessity for further research especially in languages with less resources also careful implementation is necessary to ensure that llms are used effectively in mental health platforms and human supervision remains crucial to avoid misdiagnosis,http://arxiv.org/abs/2410.12985v1,1
behavioral research can provide important insights for se practices but in performing it many studies of se are committing a normative fallacy they misappropriate normative and prescriptive theories for descriptive purposes the evidence from reviews of empirical studies of decision making in se suggests that the normative fallacy may is common this article draws on cognitive psychology and behavioral economics to explains this fallacy because data collection is framed by narrow and empirically invalid theories flawed assumptions baked into those theories lead to misleading interpretations of observed behaviors and ultimately to invalid conclusions and flawed recommendations researchers should be careful not to rely solely on engineering methods to explain what people do when they do engineering instead insist that descriptive research be based on validated descriptive theories listen carefully to skilled practitioners and only rely on validated findings to prescribe what they should do,http://arxiv.org/abs/2005.03084v1,1
with advancement in computer science research on artificial intelligence and in cognitive psychology research on human learning and performance the next generation of computerbased tutoring systems moved beyond the simple presentation of pages of text or graphics these new intelligent tutoring systems itss called cognitive tutors incorporated modeltracing technology which is a cognitive model of student problem solving that captures students multiple strategies and common misconceptions such intelligent tutoring systems or knowledge based tutoring systems can guide learners to progress in the learning process at their best this paper deals with the review of various intelligent tutoring systems using bayesian networks and how bayesian networks can be used for efficient decision making,http://arxiv.org/abs/1302.7081v1,1
large language models are powerful systems that excel at many tasks ranging from translation to mathematical reasoning yet at the same time these models often show unhumanlike characteristics in the present paper we address this gap and ask whether large language models can be turned into cognitive models we find that after finetuning them on data from psychological experiments these models offer accurate representations of human behavior even outperforming traditional cognitive models in two decisionmaking domains in addition we show that their representations contain the information necessary to model behavior on the level of individual subjects finally we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task taken together these results suggest that large pretrained models can be adapted to become generalist cognitive models thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole,http://arxiv.org/abs/2306.03917v1,1
concept learning is a fundamental aspect of human cognition and plays a critical role in mental processes such as categorization reasoning memory and decisionmaking researchers across various disciplines have shown consistent interest in the process of concept acquisition in individuals to elucidate the mechanisms involved in human concept learning this study examines the findings from computational neuroscience and cognitive psychology these findings indicate that the brains representation of concepts relies on two essential components multisensory representation and textderived representation these two types of representations are coordinated by a semantic control system ultimately leading to the acquisition of concepts drawing inspiration from this mechanism the study develops a humanlike computational model for concept learning based on spiking neural networks by effectively addressing the challenges posed by diverse sources and imbalanced dimensionality of the two forms of concept representations the study successfully attains humanlike concept representations tests involving similar concepts demonstrate that our model which mimics the way humans learn concepts yields representations that closely align with human cognition,http://arxiv.org/abs/2401.06471v1,1
mental health disorders affect a large number of people leading to many lives being lost every year these disorders affect struggling individuals and businesses whose productivity decreases due to days of lost work or lower employee performance recent studies provide alarming numbers of individuals who suffer from mental health disorders eg depression and anxiety in particular contexts such as academia in the context of the software industry there are limited studies that aim to understand the presence of mental health disorders and the characteristics of jobs in this context that can be triggers for the deterioration of the mental health of software professionals in this paper we present the results of a survey with 500 software professionals we investigate different aspects of their mental health and the characteristics of their work to identify possible triggers of mental health deterioration our results provide the first evidence that mental health is a critical issue to be addressed in the software industry as well as raise the direction of changes that can be done in this context to improve the mental health of software professionals,http://arxiv.org/abs/2309.17140v1,1
cumulative prospect theory cpt is a modeling tool widely used in behavioral economics and cognitive psychology that captures subjective decision making of individuals under risk or uncertainty in this paper we propose a dynamic pricing strategy for shared mobility on demand services smodss using a passenger behavioral model based on cpt this dynamic pricing strategy together with dynamic routing via a constrained optimization algorithm that we have developed earlier provide a complete solution customized for smods of multipassenger transportation the basic principles of cpt and the derivation of the passenger behavioral model in the smods context are described in detail the implications of cpt on dynamic pricing of the smods are delineated using computational experiments involving passenger preferences these implications include interpretation of the classic fourfold pattern of risk attitudes strong risk aversion over mixed prospects and behavioral preferences of self reference overall it is argued that the use of the cpt framework corresponds to a crucial building block in designing sociotechnical systems by allowing quantification of subjective decision making under risk or uncertainty that is perceived to be otherwise qualitative,http://arxiv.org/abs/1904.04824v2,1
machine learning ml algorithms are gaining increased importance in many academic and industrial applications and such algorithms are accordingly becoming common components in computer science curricula learning ml is challenging not only due to its complex mathematical and algorithmic aspects but also due to a the complexity of using correctly these algorithms in the context of reallife situations and b the understanding of related social and ethical issues cognitive biases are phenomena of the human brain that may cause erroneous perceptions and irrational decisionmaking processes as such they have been researched thoroughly in the context of cognitive psychology and decision making they do however have important implications for computer science education as well one wellknown cognitive bias first described by kahneman and tversky is the base rate neglect bias according to which humans fail to consider the base rate of the underlying phenomena when evaluating conditional probabilities in this paper we explore the expression of the base rate neglect bias in ml education specifically we show that about one third of students in an introduction to ml course from varied backgrounds computer science students and teachers data science engineering social science and digital humanities fail to correctly evaluate ml algorithm performance due to the base rate neglect bias this failure rate should alert educators and promote the development of new pedagogical methods for teaching ml algorithm performance,http://arxiv.org/abs/2209.08312v2,1
anonymity in social media platforms keeps users hidden behind a keyboard this absolves users of responsibility allowing them to engage in online rage hate speech and other textbased toxicity that harms online wellbeing recent research in the field of digital emotion regulation der has revealed that indulgence in online toxicity can be a result of ineffective emotional regulation er this we believe can be reduced by educating users about the consequences of their actions prior der research has primarily focused on exploring digital emotion regulation practises identifying emotion regulation using multimodal sensors and encouraging users to act responsibly in online conversations while these studies provide valuable insights into how users consciously utilise digital media for emotion regulation they do not capture the contextual dynamics of emotion regulation online through interaction design this work provides an intervention for the delivery of er support it introduces a novel technique for identifying the need for emotional regulation in online conversations and delivering information to users in a way that integrates didactic learning into their daily life by fostering selfreflection in periods of intensified emotional expression we present a graphbased framework for onthespot emotion regulation support in online conversations our findings suggest that using this model in a conversation can help identify its influential threadsnodes to locate where toxicity is concentrated and help reduce it by up to 12 this is the first study in the field of der that focuses on learning transfer by inducing selfreflection and implicit emotion regulation,http://arxiv.org/abs/2303.00884v1,1
this paper investigates the mental health penalty for women after childbirth in switzerland leveraging insurance data we employ a staggered differenceindifference research design the findings reveal a substantial mental health penalty for women following the birth of their first child approximately four years after childbirth there is a one percentage point pp increase in antidepressant prescriptions representing a 50 increase compared to prebirth levels this increase rises to 17 pp a 70 increase six years postpartum the mental health penalty is likely not only a direct consequence of giving birth but also a consequence of the changed life circumstances and time constraints that accompany it as the penalty is rising over time and is higher for women who are employed before childbirth,http://arxiv.org/abs/2410.20861v1,1
many people struggling with mental health issues are unable to access adequate care due to high costs and a shortage of mental health professionals leading to a global mental health crisis online mental health communities can help mitigate this crisis by offering a scalable easily accessible alternative to inperson sessions with therapists or support groups however people seeking emotional or psychological support online may be especially vulnerable to the kinds of antisocial behavior that sometimes occur in online discussions moderation can improve online discourse quality but we lack an understanding of its effects on online mental health conversations in this work we leveraged a natural experiment occurring across 200000 messages from 7000 online mental health conversations to evaluate the effects of moderation on online mental health discussions we found that participation in group mental health discussions led to improvements in psychological perspective and that these improvements were larger in moderated conversations the presence of a moderator increased user engagement encouraged users to discuss negative emotions more candidly and dramatically reduced bad behavior among chat participants moderation also encouraged stronger linguistic coordination which is indicative of trust building in addition moderators who remained active in conversations were especially successful in keeping conversations on topic our findings suggest that moderation can serve as a valuable tool to improve the efficacy and safety of online mental health conversations based on these findings we discuss implications and tradeoffs involved in designing effective online spaces for mental health support,http://arxiv.org/abs/2005.09225v7,1
mental illnesses adversely affect a significant proportion of the population worldwide however the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are timeconsuming and expensive consequently bestavailable estimates concerning the prevalence of mental health conditions are often years out of date automated approaches to supplement these survey methods with broad aggregated information derived from social media content provides a potential means for near realtime estimates at scale these may in turn provide grist for supporting evaluating and iteratively improving upon public health programs and interventions we propose a novel model for automated mental health status quantification that incorporates user embeddings this builds upon recent work exploring representation learning methods that induce embeddings by leveraging social media post histories such embeddings capture latent characteristics of individuals eg political leanings and encode a soft notion of homophily in this paper we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses to this end we estimated user embeddings for a set of users known to be affected by depression and posttraumatic stress disorder ptsd and for a set of demographically matched control users we then evaluated these embeddings with respect to i their ability to capture homophilic relations with respect to mental health status and ii the performance of downstream mental health prediction models based on these features our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions and are predictive of mental health,http://arxiv.org/abs/1705.00335v1,1
the rapid evolution of large language models llms offers promising potential to alleviate the global scarcity of mental health professionals however llms alignment with essential mental health counseling competencies remains understudied we introduce counselingbench a novel ncmhcebased benchmark evaluating llms across five key mental health counseling competencies testing 22 generalpurpose and medicalfinetuned llms we find frontier models exceed minimum thresholds but fall short of expertlevel performance with significant variations they excel in intake assessment diagnosis yet struggle with core counseling attributes and professional practice ethics medical llms surprisingly underperform generalist models accuracywise while at the same time producing slightly higherquality justifications but making more contextrelated errors our findings highlight the complexities of developing ai systems for mental health counseling particularly for competencies requiring empathy and contextual understanding we found that frontier llms perform at a level exceeding the minimal required level of aptitude for all key mental health counseling competencies but fall short of expertlevel performance and that current medical llms do not significantly improve upon generalist models in mental health counseling competencies this underscores the critical need for specialized mental health counselingspecific finetuned llms that rigorously aligns with core competencies combined with appropriate human supervision before any responsible realworld deployment can be considered,http://arxiv.org/abs/2410.22446v1,1
selfreported diagnosis statements have been widely employed in studying language related to mental health in social media however existing research has largely ignored the temporality of mental health diagnoses in this work we introduce rsddtime a new dataset of 598 manually annotated selfreported depression diagnosis posts from reddit that include temporal information about the diagnosis annotations include whether a mental health condition is present and how recently the diagnosis happened furthermore we include exact temporal spans that relate to the date of diagnosis this information is valuable for various computational methods to examine mental health through social media because ones mental health state is not static we also test several baseline classification and extraction approaches which suggest that extracting temporal information from selfreported diagnosis statements is challenging,http://arxiv.org/abs/1806.07916v1,1
the spread of the novel coronavirus disease caused schools in japan to close to cope with the pandemic in response to this parents of students were obliged to care for their children during the daytime when they were usually at school does the increase in burden of childcare influence parents mental health based on short panel data from midmarch to midapril 2020 we explored how school closures influenced the mental health of parents with schoolaged children using the fixed effects model we found that school closures lead to students mothers suffering from worse mental health than other females while the fathers mental health did not differ from other males this tendency was only observed for less educated mothers who had children attending primary school but not those attending junior high school the contribution of this paper is to show that school closures increased the inequality of mental health between genders and the educational background of parents,http://arxiv.org/abs/2101.08476v1,1
previous researches on dialogue system assessment usually focus on the quality evaluation eg fluency relevance etc of responses generated by the chatbots which are local and technical metrics for a chatbot which responds to millions of online users including minors we argue that it should have a healthy mental tendency in order to avoid the negative psychological impact on them in this paper we establish several mental health assessment dimensions for chatbots depression anxiety alcohol addiction empathy and introduce the questionnairebased mental health assessment methods we conduct assessments on some wellknown opendomain chatbots and find that there are severe mental health issues for all these chatbots we consider that it is due to the neglect of the mental health risks during the dataset building and the model training procedures we expect to attract researchers attention to the serious mental health problems of chatbots and improve the chatbots ability in positive emotional interaction,http://arxiv.org/abs/2201.05382v1,1
mobile mental health applications are seen as a promising way to fulfill the growing need for mental health care although there are more than ten thousand mental health apps available on app marketplaces such as google play and apple app store many of them are not evidencebased or have been minimally evaluated or regulated the reallife experience and concerns of the app users are largely unknown to address this knowledge gap we analyzed 2159 user reviews from 117 android apps and 2764 user reviews from 76 ios apps our findings include the critiques around inconsistent moderation standards and lack of transparency appembedded social features and chatbots were criticized for providing little support during crises we provide research and design implications for future mental health app developers discuss the necessity of developing a comprehensive and centralized app development guideline and the opportunities of incorporating existing ai technology in mental health chatbots,http://arxiv.org/abs/2209.07796v1,1
pretrained language models have been used in various natural language processing applications in the mental health domain domainspecific language models are pretrained and released which facilitates the early detection of mental health conditions social posts eg on reddit are usually long documents however there are no domainspecific pretrained models for longsequence modeling in the mental health domain this paper conducts domainspecific continued pretraining to capture the long context for mental health specifically we train and release mentalxlnet and mentallongformer based on xlnet and longformer we evaluate the mental health classification performance and the longrange ability of these two domainspecific pretrained models our models are released in huggingface,http://arxiv.org/abs/2304.10447v1,1
evaluating large language models llms in the mental health domain poses distinct challenged from other domains given the subtle and highly subjective nature of symptoms that exhibit significant variability among individuals this paper presents psyeval the first comprehensive suite of mental healthrelated tasks for evaluating llms psyeval encompasses five subtasks that evaluate three critical dimensions of mental health this comprehensive framework is designed to thoroughly assess the unique challenges and intricacies of mental healthrelated tasks making psyeval a highly specialized and valuable tool for evaluating llm performance in this domain we evaluate twelve advanced llms using psyeval experiment results not only demonstrate significant room for improvement in current llms concerning mental health but also unveil potential directions for future model optimization,http://arxiv.org/abs/2311.09189v2,1
in order to uncover users attitudes towards chatgpt in mental health this study examines public opinions about chatgpt in mental health discussions on reddit researchers used the bertbasemultilingualuncasedsentiment techniques for sentiment analysis and the bertopic model for topic modeling it was found that overall negative sentiments prevail followed by positive ones with neutral sentiments being the least common the prevalence of negative emotions has increased over time negative emotions encompass discussions on chatgpt providing bad mental health advice debates on machine vs human value the fear of ai and concerns about universal basic income ubi in contrast positive emotions highlight chatgpts effectiveness in counseling with mentions of keywords like time and wallet neutral discussions center around private data concerns these findings shed light on public attitudes toward chatgpt in mental health potentially contributing to the development of trustworthy ai in mental health from the public perspective,http://arxiv.org/abs/2311.15800v1,1
people experiencing severe distress increasingly use large language model llm chatbots as mental health support tools discussions on social media have described how engagements were lifesaving for some but evidence suggests that generalpurpose llm chatbots also have notable risks that could endanger the welfare of users if not designed responsibly in this study we investigate the lived experiences of people who have used llm chatbots for mental health support we build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots fill in gaps in everyday care and navigate associated cultural limitations when seeking support from chatbots we ground our analysis in psychotherapy literature around effective support and introduce the concept of therapeutic alignment or aligning ai with therapeutic values for mental health contexts our study offers recommendations for how designers can approach the ethical and effective use of llm chatbots and other ai mental health support tools in mental health care,http://arxiv.org/abs/2401.14362v2,1
we introduce a multilayer perceptron mlp called the covid19 depression and anxiety predictor codap to predict mental health trends particularly anxiety and depression during the covid19 pandemic our method utilizes a comprehensive dataset which tracked mental health symptoms weekly over ten weeks during the initial covid19 wave april to june 2020 in a diverse cohort of us adults this period characterized by a surge in mental health symptoms and conditions offers a critical context for our analysis our focus was to extract and analyze patterns of anxiety and depression through a unique lens of qualitative individual attributes using codap this model not only predicts patterns of anxiety and depression during the pandemic but also unveils key insights into the interplay of demographic factors behavioral changes and social determinants of mental health these findings contribute to a more nuanced understanding of the complexity of mental health issues in times of global health crises potentially guiding future early interventions,http://arxiv.org/abs/2403.06033v1,1
lgbtq community face disproportionate mental health challenges including higher rates of depression anxiety and suicidal ideation research has shown that lgbtq people have been using large language modelbased chatbots such as chatgpt for their mental health needs despite the potential for immediate support and anonymity these chatbots offer concerns regarding their capacity to provide empathetic accurate and affirming responses remain in response to these challenges we propose a framework for evaluating the affirmativeness of llms based on principles of affirmative therapy emphasizing the need for attitudes knowledge and actions that support and validate lgbtq experiences we propose a combination of qualitative and quantitative analyses hoping to establish benchmarks for affirmative ai ensuring that llmbased chatbots can provide safe supportive and effective mental health support to lgbtq individuals we benchmark llm affirmativeness not as a mental health solution for lgbtq individuals or to claim it resolves their mental health issues as we highlight the need to consider complex discrimination in the lgbtq community when designing technological aids our goal is to evaluate llms for lgbtq mental health support since many in the community already use them aiming to identify potential harms of using generalpurpose llms in this context,http://arxiv.org/abs/2405.04652v1,1
the application of machine learning ml in detecting diagnosing and treating mental health disorders is garnering increasing attention traditionally research has focused on single modalities such as text from clinical notes audio from speech samples or video of interaction patterns recently multimodal ml which combines information from multiple modalities has demonstrated significant promise in offering novel insights into human behavior patterns and recognizing mental health symptoms and risk factors despite its potential multimodal ml in mental health remains an emerging field facing several complex challenges before practical applications can be effectively developed this survey provides a comprehensive overview of the data availability and current stateoftheart multimodal ml applications for mental health it discusses key challenges that must be addressed to advance the field the insights from this survey aim to deepen the understanding of the potential and limitations of multimodal ml in mental health guiding future research and development in this evolving domain,http://arxiv.org/abs/2407.16804v1,1
noninvasive methods for diagnosing mental health conditions such as speech analysis offer promising potential in modern medicine recent advancements in machine learning particularly speech foundation models have shown significant promise in detecting mental health states by capturing diverse features this study investigates which pretext tasks in these models best transfer to mental health detection and examines how different model layers encode features relevant to mental health conditions we also probed the optimal length of audio segments and the best pooling strategies to improve detection accuracy using the callyopegp and androids datasets we evaluated the models effectiveness across different languages and speech tasks aiming to enhance the generalizability of speechbased mental health diagnostics our approach achieved sota scores in depression detection on the androids dataset,http://arxiv.org/abs/2409.19042v1,1
access to mental health support remains limited particularly in marginalized communities where structural and cultural barriers hinder timely care this paper explores the potential of aienabled chatbots as a scalable solution focusing on advanced large language models llmsgpt v4 mistral large and llama v31and assessing their ability to deliver empathetic meaningful responses in mental health contexts while these models show promise in generating structured responses they fall short in replicating the emotional depth and adaptability of human therapists additionally trustworthiness bias and privacy challenges persist due to unreliable datasets and limited collaboration with mental health professionals to address these limitations we propose a federated learning framework that ensures data privacy reduces bias and integrates continuous validation from clinicians to enhance response quality this approach aims to develop a secure evidencebased ai chatbot capable of offering trustworthy empathetic and biasreduced mental health support advancing ais role in digital mental health care,http://arxiv.org/abs/2410.02783v1,1
mental health issues significantly impact individuals daily lives yet many do not receive the help they need even with available online resources this study aims to provide accessible stigmafree personalized and realtime mental health support through cuttingedge ai technologies it makes the following contributions 1 conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs 2 introducing soullmate an adaptive llmdriven system that integrates llm technologies chain retrievalaugmented generation rag prompt engineering and domain knowledge this system offers advanced features such as suicide risk detection and proactive guidance dialogue and utilizes rag for personalized profile uploads and conversational information extraction 3 developing novel evaluation approaches to assess preliminary assessments and suicide risk detection utilizing annotated reallife interview data and professionally labeled datasets indicating suicide tendencies 4 proposing key indicator summarization kis and proactive questioning strategy pqs methods to enhance model performance and usability through contextsensitive response adjustments and semantic coherence evaluations this study contributes to advancing mental health support technologies potentially improving the accessibility and effectiveness of mental health care globally,http://arxiv.org/abs/2410.11859v1,1
student mental health is a sensitive issue that necessitates special attention a primary concern is the studenttocounselor ratio which surpasses the recommended standard of 2501 in most universities this imbalance results in extended waiting periods for inperson consultations which cause suboptimal treatment significant efforts have been directed toward developing mental health dialogue systems utilizing the existing opensource mental healthrelated datasets however currently available datasets either discuss general topics or various strategies that may not be viable for direct application due to numerous ethical constraints inherent in this research domain to address this issue this paper introduces a specialized mental health dataset that emphasizes the active listening strategy employed in conversation for counseling also named as convcounsel this dataset comprises both speech and text data which can facilitate the development of a reliable pipeline for mental health dialogue systems to demonstrate the utility of the proposed dataset this paper also presents the nycuka a spoken mental health dialogue system that is designed by using the convcounsel dataset the results show the merit of using this dataset,http://arxiv.org/abs/2411.00604v1,1
ensuring accurate call prioritisation is essential for optimising the efficiency and responsiveness of mental health helplines currently call operators rely entirely on the callers statements to determine the priority of the calls it has been shown that entirely subjective assessment can lead to errors furthermore it is a missed opportunity not to utilise the voice properties readily available during the call to aid in the evaluation incorrect prioritisation can result in delayed assistance for highrisk individuals resource misallocation increased mental health deterioration loss of trust and potential legal consequences it is vital to address these risks to guarantee the reliability and effectiveness of mental health services this study delves into the potential of using machine learning a branch of artificial intelligence to estimate call priority from the callers voices for users of mental health phone helplines after analysing 459 call records from a mental health helpline we achieved a balanced accuracy of 92 showing promise in aiding the call operators efficiency in call handling processes and improving customer satisfaction,http://arxiv.org/abs/2412.00057v1,1
given the current social distance restrictions across the world most individuals now use social media as their major medium of communication millions of people suffering from mental diseases have been isolated due to this and they are unable to get help in person they have become more reliant on online venues to express themselves and seek advice on dealing with their mental disorders according to the world health organization who approximately 450 million people are affected mental illnesses such as depression anxiety etc are immensely common and have affected an individuals physical health recently artificial intelligence ai methods have been presented to help mental health providers including psychiatrists and psychologists in decision making based on patients authentic information eg medical records behavioral data social media utilization etc ai innovations have demonstrated predominant execution in numerous realworld applications broadening from computer vision to healthcare this study analyzes unstructured user data on the reddit platform and classifies five common mental illnesses depression anxiety bipolar disorder adhd and ptsd we trained traditional machine learning deep learning and transfer learning multiclass models to detect mental disorders of individuals this effort will benefit the public health system by automating the detection process and informing appropriate authorities about people who require emergency assistance,http://arxiv.org/abs/2207.01012v1,1
the empirical risk minimization approach to datadriven decision making requires access to training data drawn under the same conditions as those that will be faced when the decision rule is deployed however in a number of settings we may be concerned that our training sample is biased in the sense that some groups characterized by either observable or unobservable attributes may be under or overrepresented relative to the general population and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment we propose a model of sampling bias called conditional gammabiased sampling where observed covariates can affect the probability of sample selection arbitrarily much but the amount of unexplained variation in the probability of sample selection is bounded by a constant factor applying the distributionally robust optimization framework we propose a method for learning a decision rule that minimizes the worstcase risk incurred under a family of test distributions that can generate the training distribution under gammabiased sampling we apply a result of rockafellar and uryasev to show that this problem is equivalent to an augmented convex risk minimization problem we give statistical guarantees for learning a model that is robust to sampling bias via the method of sieves and propose a deep learning algorithm whose loss function captures our robust learning target we empirically validate our proposed method in a case study on prediction of mental health scores from health survey data and a case study on icu length of stay prediction,http://arxiv.org/abs/2209.01754v3,1
digital biomarkers and remote patient monitoring can provide valuable and timely insights into how a patient is coping with their condition disease progression treatment response etc complementing treatment in traditional healthcare settingssmartphones with embedded and connected sensors have immense potential for improving healthcare through various apps and mhealth mobile health platforms this capability could enable the development of reliable digital biomarkers from longterm longitudinal data collected remotely from patients we built an opensource platform radarbase to support largescale data collection in remote monitoring studies radarbase is a modern remote data collection platform built around confluents apache kafka to support scalability extensibility security privacy and quality of data it provides support for study design and setup active eg proms and passive eg phone sensors wearable devices and iot remote data collection capabilities with feature generation eg behavioural environmental and physiological markers the backend enables secure data transmission and scalable solutions for data storage management and data access the platform has successfully collected longitudinal data for various cohorts in a number of disease areas including multiple sclerosis depression epilepsy adhd alzheimer autism and lung diseases digital biomarkers developed through collected data are providing useful insights into different diseases radarbase provides a modern opensource communitydriven solution for remote monitoring data collection and digital phenotyping of physical and mental health diseases clinicians can use digital biomarkers to augment their decision making for the prevention personalisation and early intervention of disease,http://arxiv.org/abs/2308.02043v1,1
this paper explores the intricate relationship between capitalism racial injustice and artificial intelligence ai arguing that ai acts as a contemporary vehicle for ageold forms of exploitation by linking historical patterns of racial and economic oppression with current ai practices this study illustrates how modern technology perpetuates and deepens societal inequalities it specifically examines how ai is implicated in the exploitation of marginalized communities through underpaid labor in the gig economy the perpetuation of biases in algorithmic decisionmaking and the reinforcement of systemic barriers that prevent these groups from benefiting equitably from technological advances furthermore the paper discusses the role of ai in extending and intensifying the social economic and psychological burdens faced by these communities highlighting the problematic use of ai in surveillance law enforcement and mental health contexts the analysis concludes with a call for transformative changes in how ai is developed and deployed advocating for a reevaluation of the values driving ai innovation the paper promotes an approach that integrates social justice and equity into the core of technological design and policy this shift is crucial for ensuring that ai serves as a tool for societal improvement fostering empowerment and healing rather than deepening existing divides,http://arxiv.org/abs/2403.06332v2,1
this manuscript presents a methodical examination of the utilization of artificial intelligence in the assessment of emotions in texts related to healthcare with a particular focus on the incorporation of natural language processing and deep learning technologies we scrutinize numerous research studies that employ ai to augment sentiment analysis categorize emotions and forecast patient outcomes based on textual information derived from clinical narratives patient feedback on medications and online health discussions the review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification the prognostic capabilities of ai models for neurodegenerative diseases and the creation of aipowered systems that offer support in clinical decisionmaking remarkably the utilization of ai applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contributing to the early identification of mental health disorders there persist challenges which encompass ensuring the ethical application of ai safeguarding patient confidentiality and addressing potential biases in algorithmic procedures nevertheless the potential of ai to revolutionize healthcare practices is unmistakable offering a future where healthcare is not only more knowledgeable and efficient but also more empathetic and centered around the needs of patients this investigation underscores the transformative influence of ai on healthcare delivering a comprehensive comprehension of its role in examining emotional content in healthcare texts and highlighting the trajectory towards a more compassionate approach to patient care the findings advocate for a harmonious synergy between ais analytical capabilities and the human aspects of healthcare,http://arxiv.org/abs/2403.09762v1,1
correctly assessing the malignancy of breast lesions identified during ultrasound examinations is crucial for effective clinical decisionmaking however the current golden standard relies on manual birads scoring by clinicians often leading to unnecessary biopsies and a significant mental health burden on patients and their families in this paper we introduce personalizedus an interpretable machine learning system that leverages recent advances in conformal prediction to provide precise and personalized risk estimates with local coverage guarantees and sensitivity specificity and predictive values above 09 across various threshold levels in particular we identify meaningful lesion subgroups where distributionfree modelagnostic conditional coverage holds with approximately 90 of our prediction sets containing only the ground truth in most lesion subgroups thus explicitly characterizing for which patients the model is most suitably applied moreover we make available a curated tabular dataset of 1936 biopsied breast lesions from a recent observational multicenter study and benchmark the performance of several stateoftheart learning algorithms we also report a successful case study of the deployed system in the same multicenter context concrete clinical benefits include up to a 65 reduction in requested biopsies among birads 4a and 4b lesions with minimal to no missed cancer cases,http://arxiv.org/abs/2408.15458v1,1
widely distributed misinformation shared across social media channels is a pressing issue that poses a significant threat to many aspects of societys wellbeing inaccurate shared information causes confusion can adversely affect mental health and can lead to misinformed decisionmaking therefore it is important to implement proactive measures to intervene and curb the spread of misinformation where possible this has prompted scholars to investigate a variety of intervention strategies for misinformation sharing on social media this study explores the typology of intervention strategies for addressing misinformation sharing on social media identifying 4 important clusters cognitionbased automatedbased informationbased and hybridbased the literature selection process utilized the prisma method to ensure a systematic and comprehensive analysis of relevant literature while maintaining transparency and reproducibility a total of 139 articles published from 20132023 were then analyzed meanwhile bibliometric analyses were conducted using performance analysis and science mapping techniques for the typology development a comparative analysis of the typology was conducted to reveal patterns and evolution in the field this provides valuable insights for both theory and practical applications overall the study concludes that scholarly contributions to scientific research and publication help to address research gaps and expand knowledge in this field understanding the evolution of intervention strategies for misinformation sharing on social media can support future research that contributes to the development of more effective and sustainable solutions to this persistent problem,http://arxiv.org/abs/2409.17637v1,1
emotion regulation is the process of consciously altering ones affective state that is the underlying emotional state such as happiness confidence guilt anger etc the ability to effectively regulate emotions is necessary for functioning efficiently in everyday life today the pervasiveness of digital technology is being purposefully employed to modify our affective states a process known as digital emotion regulation understanding digital emotion regulation can help support the rise of ethical technology design development and deployment this article presents an overview of digital emotion regulation in social media applications as well as a synthesis of recent research on emotion regulation interventions for social media we share our findings from analysing stateoftheart literature on how different social media applications are utilised at different stages in the process of emotion regulation,http://arxiv.org/abs/2307.13187v1,1
while the applications and demands of machine learning ml systems in mental health are growing there is little discussion nor consensus regarding a uniquely challenging aspect building security methods and requirements into these ml systems and keep the ml system usable for endusers this question of usable security is very important because the lack of consideration in either security or usability would hinder largescale user adoption and active usage of ml systems in mental health applications in this short paper we introduce a framework of four pillars and a set of desired properties which can be used to systematically guide and evaluate securityrelated designs implementations and deployments of ml systems for mental health we aim to weave together threads from different domains incorporate existing views and propose new principles and requirements in an effort to lay out a clear framework where criteria and expectations are established and are used to make security mechanisms usable for endusers of those ml systems in mental health together with this framework we present several concrete scenarios where different usable security cases and profiles in mlsystems in mental health applications are examined and evaluated,http://arxiv.org/abs/2008.07738v1,1
mental health is an extremely important subject especially in these unprecedented times of the covid19 pandemic ubiquitous mobile phones can equip users to supplement psychiatric treatment and manage their mental health mobile mental health mmh apps emerge as an effective alternative to assist with a broad range of psychological disorders filling the muchneeded patientprovider accessibility gap however it also raises significant concerns with sensitive information leakagethe absence of a transparent privacy policy and lack of user awareness may pose a significant threat to undermining the applicability of such tools we conducted a multifold study of 1 privacy policies manually and with polisis an automated framework to evaluate privacy policies 2 app permissions 3 static analysis for inherent security issues 4 dynamic analysis for threat surface and vulnerabilities detection and 5 traffic analysis our results indicate that apps exploitable flaws dangerous permissions and insecure data handling pose a potential threat to the users privacy and security the dynamic analysis identified 145 vulnerabilities in 20 toprated mmh apps where attackers and malicious apps can access sensitive information 45 of mmh apps use a unique identifier hardware id which can link a unique id to a particular user and probe users mental health traffic analysis shows that sensitive mental health data can be leaked through insecure data transmission mmh apps need better scrutiny and regulation for more widespread usage to meet the increasing need for mental health care without being intrusive to the already vulnerable population,http://arxiv.org/abs/2206.10728v2,1
in this study we leveraged machine learning techniques to identify risk factors associated with postcovid19 mental health disorders our analysis based on data collected from 669 patients across various provinces in iraq yielded valuable insights we found that age gender and geographical region of residence were significant demographic factors influencing the likelihood of developing mental health disorders in postcovid19 patients additionally comorbidities and the severity of covid19 illness were important clinical predictors psychosocial factors such as social support coping strategies and perceived stress levels also played a substantial role our findings emphasize the complex interplay of multiple factors in the development of mental health disorders following covid19 recovery healthcare providers and policymakers should consider these risk factors when designing targeted interventions and support systems for individuals at risk machine learningbased approaches can provide a valuable tool for predicting and preventing adverse mental health outcomes in postcovid19 patients further research and prospective studies are needed to validate these findings and enhance our understanding of the longterm psychological impact of the covid19 pandemic this study contributes to the growing body of knowledge regarding the mental health consequences of the covid19 pandemic and underscores the importance of a multidisciplinary approach to address the diverse needs of individuals on the path to recovery,http://arxiv.org/abs/2309.16055v1,1
culture moderates the way individuals perceive and express mental distress current understandings of mental health expressions on social media however are predominantly derived from weird western educated industrialized rich and democratic contexts to address this gap we examine mental health posts on reddit made by individuals geolocated in india to identify variations in social media language specific to the indian context compared to users from the rest of the world row our experiments reveal significant psychosocial variations in emotions sadness in india vs anxiety in row temporal orientation presentfocused in india vs pastfocused in the west and sociocultural aspects substance use vs workachievement clinical psychologists practicing in india validated the findings and underlined significant overlap in mental healthrelated concerns observed in social media posts and inperson sessions this study demonstrates the potential of social media platforms for identifying crosscultural differences in mental health struggles eg seeking help in india vs seeking peer support in row future research should investigate how mental health assessment can be culturally adapted to personalize interventions ensuring equitable mental health care for individuals from all cultural backgrounds,http://arxiv.org/abs/2402.11477v3,1
the growing prevalence and complexity of mental health disorders present significant challenges for accurate diagnosis and treatment particularly in understanding the interplay between cooccurring conditions mental health disorders such as depression and anxiety often cooccur yet current datasets derived from social media posts typically focus on singledisorder labels limiting their utility in comprehensive diagnostic analyses this paper addresses this critical gap by proposing a novel methodology for cleaning sampling labeling and combining data to create versatile multilabel datasets our approach introduces a synthetic labeling technique to transform singlelabel datasets into multilabel annotations capturing the complexity of overlapping mental health conditions to achieve this two singlelabel datasets are first merged into a foundational multilabel dataset enabling realistic analyses of cooccurring diagnoses we then design and evaluate various prompting strategies for large language models llms ranging from singlelabel predictions to unrestricted prompts capable of detecting any present disorders after rigorously assessing multiple llms and prompt configurations the optimal combinations are identified and applied to label six additional singledisorder datasets from rmhd the result is spaadedr a robust multilabel dataset encompassing diverse mental health conditions this research demonstrates the transformative potential of llmdriven synthetic labeling in advancing mental health diagnostics from social media data paving the way for more nuanced datadriven insights into mental health care,http://arxiv.org/abs/2412.03796v1,1
recent advancements in nlp have spurred significant interest in analyzing social media text data for identifying linguistic features indicative of mental health issues however the domain of expressive narrative stories ensdeeply personal and emotionally charged narratives that offer rich psychological insightsremains underexplored this study bridges this gap by utilizing a dataset sourced from reddit focusing on ens from individuals with and without selfdeclared depression our research evaluates the utility of advanced language models bert and mentalbert against traditional models we find that traditional models are sensitive to the absence of explicit topicrelated words which could risk their potential to extend applications to ens that lack clear mental health terminology despite mentalbert is design to better handle psychiatric contexts it demonstrated a dependency on specific topic words for classification accuracy raising concerns about its application when explicit mental health terms are sparse pvalue005 in contrast bert exhibited minimal sensitivity to the absence of topic words in ens suggesting its superior capability to understand deeper linguistic features making it more effective for realworld applications both bert and mentalbert excel at recognizing linguistic nuances and maintaining classification accuracy even when narrative order is disrupted this resilience is statistically significant with sentence shuffling showing substantial impacts on model performance pvalue005 especially evident in ens comparisons between individuals with and without mental health declarations these findings underscore the importance of exploring ens for deeper insights into mental healthrelated narratives advocating for a nuanced approach to mental health text analysis that moves beyond mere keyword detection,http://arxiv.org/abs/2412.16302v1,1
our aim is to compare the fundamental notions of quantum physics contextuality vs incompatibility one has to distinguish two different notions of contextuality it bohrcontextuality and it bellcontextuality the latter is defined operationally via violation of noncontextuality bell type inequalities this sort of contextuality will be compared with incompatibility it is easy to show that for quantum observables there is it no contextuality without incompatibility the natural question arises what is contextuality without incompatibility what is dryresidue generally this is the very complex question we concentrated on contextuality for four quantum observables we shown that in the chshscenarios for natural quantum observables it contextuality is reduced to incompatibility however generally contextuality without incompatibility may have some physical content we found a mathematical constraint extracting the contextuality component from incompatibility however the physical meaning of this constraint is not clear in appendix 1 we briefly discuss another sort of contextuality based on the bohrs complementarity principle which is treated as the it contextualityincompatibility principle bohrcontextuality plays the crucial role in quantum foundations incompatibility is in fact a consequence of bohrcontextuality finally we remark that outside of physics eg in cognitive psychology and decision making bellcontextuality cleaned of incompatibility can play the important role,http://arxiv.org/abs/2005.05124v3,1
untile recently crowdsourcing has been primarily conceived as an online activity to harness resources for problem solving however the emergence of opportunistic networking on has opened up crowdsourcing to the spatial domain in this paper we bring the on model for potential crowdsourcing in the smart city environment we introduce cognitive features to the on that allow users mobile devices to become aware of the surrounding physical environment specifically we exploit cognitive psychology studies on dynamic memory structures and cognitive heuristics ie mental models that describe how the human brain handle decisionmaking amongst complex and realtime stimuli combined with on these cognitive features allow devices to act as proxies in the cyberworld of their users and exchange knowledge to deliver awareness of places in an urban environment this is done through tags associated with locations they represent features that are perceived by humans about a place we consider the extent to which this knowledge becomes available to participants using interactions with locations and other nodes this is assessed taking into account a wide range of cognitive parameters outcomes are important because this functionality could support a new type of recommendation system that is independent of the traditional forms of networking,http://arxiv.org/abs/2109.14946v1,1
there is a clear need to involve patients in medical decisions however cognitive psychological research has highlighted the cognitive limitations of humans with respect to 1 probabilistic assessment of the patient state and of potential outcomes of various decisions 2 elicitation of the patient utility function and 3 integration of the probabilistic knowledge and of patient preferences to determine the optimal strategy therefore without adequate computational support current shared decision models have severe ethical deficiencies an informed consent model unfairly transfers the responsibility to a patient who does not have the necessary knowledge nor the integration capability a paternalistic model endows with exaggerated power a physician who might not be aware of the patient preferences is prone to multiple cognitive biases and whose computational integration capability is bounded recent progress in artificial intelligence suggests adding a third agent a computer in all deliberative medical decisions non emergency medical decisions in which more than one alternative exists the patient preferences can be elicited the therapeutic alternatives might be influenced by these preferences medical knowledge exists regarding the likelihood of the decision outcomes and there is sufficient decision time ethical physicians should exploit computational decision support technologies neither making the decisions solely on their own nor shirking their duty and shifting the responsibility to patients in the name of informed consent the resulting three way patient care provider computer human machine model that we suggest emphasizes the patient preferences the physician knowledge and the computational integration of both aspects does not diminish the physician role but rather brings out the best in human and machine,http://arxiv.org/abs/2102.01811v1,1
working memory wm a fundamental cognitive process facilitating the temporary storage integration manipulation and retrieval of information plays a vital role in reasoning and decisionmaking tasks robust benchmark datasets that capture the multifaceted nature of wm are crucial for the effective development and evaluation of ai wm models here we introduce a comprehensive working memory worm benchmark dataset for this purpose worm comprises 10 tasks and a total of 1 million trials assessing 4 functionalities 3 domains and 11 behavioral and neural characteristics of wm we jointly trained and tested stateoftheart recurrent neural networks and transformers on all these tasks we also include human behavioral benchmarks as an upper bound for comparison our results suggest that ai models replicate some characteristics of wm in the brain most notably primacy and recency effects and neural clusters and correlates specialized for different domains and functionalities of wm in the experiments we also reveal some limitations in existing models to approximate human behavior this dataset serves as a valuable resource for communities in cognitive psychology neuroscience and ai offering a standardized framework to compare and enhance wm models investigate wms neural underpinnings and develop wm models with humanlike capabilities our source code and data are available at,http://arxiv.org/abs/2307.10768v2,1
the past few years have seen a surge in the application of quantum theory methodologies and quantumlike modeling in fields such as cognition psychology and decisionmaking despite the success of this approach in explaining various psychological phenomena such as order conjunction disjunction and response replicability effects there remains a potential dissatisfaction due to its lack of clear connection to neurophysiological processes in the brain currently it remains a phenomenological approach in this paper we develop a quantumlike representation of networks of communicating neurons this representation is not based on standard quantum theory but on generalized probability theory gpt with a focus on the operational measurement framework specifically we use a version of gpt that relies on ordered linear state spaces rather than the traditional complex hilbert spaces a network of communicating neurons is modeled as a weighted directed graph which is encoded by its weight matrix the state space of these weight matrices is embedded within the gpt framework incorporating effect observables and state updates within the theory of measurement instruments a critical aspect of this model this gpt based approach successfully reproduces key quantumlike effects such as order nonrepeatability and disjunction effects commonly associated with decision interference moreover this framework supports quantumlike modeling in medical diagnostics for neurological conditions such as depression and epilepsy while this paper focuses primarily on cognition and neuronal networks the proposed formalism and methodology can be directly applied to a wide range of biological and social networks,http://arxiv.org/abs/2411.00036v2,1
compared to physical health population mental health measurement in the us is very coarsegrained currently in the largest population surveys such as those carried out by the centers for disease control or gallup mental health is only broadly captured through mentally unhealthy days or sadness and limited to relatively infrequent state or metropolitan estimates through the large scale analysis of social media data robust estimation of population mental health is feasible at much higher resolutions up to weekly estimates for counties in the present work we validate a pipeline that uses a sample of 12 billion tweets from 2 million geolocated users to estimate mental health changes for the two leading mental health conditions depression and anxiety we find moderate to large associations between the languagebased mental health assessments and survey scores from gallup for multiple levels of granularity down to the countyweek fixed effects beta 25 to 158 p001 languagebased assessment allows for the costeffective and scalable monitoring of population mental health at weekly time scales such spatially finegrained time series are well suited to monitor effects of societal events and policies as well as enable quasiexperimental study designs in population health and other disciplines beyond mental health in the us this method generalizes to a broad set of psychological outcomes and allows for community measurement in underresourced settings where no traditional survey measures but social media data are available,http://arxiv.org/abs/2302.12952v1,1
amidst the growing interest in developing taskautonomous ai for automated mental health care this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy outlines ethical requirements and defines beneficial default behaviors for ai agents in the context of mental health support we also evaluate fourteen stateoftheart language models ten offtheshelf four finetuned using 16 mental healthrelated questionnaires designed to reflect various mental health conditions such as psychosis mania depression suicidal thoughts and homicidal tendencies the questionnaire design and response evaluations were conducted by mental health clinicians mds we find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context this is due to a range of issues including overly cautious or sycophantic responses and the absence of necessary safeguards alarmingly we find that most of the tested models could cause harm if accessed in mental health emergencies failing to protect users and potentially exacerbating existing symptoms we explore solutions to enhance the safety of current models before the release of increasingly taskautonomous ai systems in mental health it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users this involves aligning with the ethical framework and default behaviors outlined in our study we contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current ai technologies to user mental health and safety trigger warning contains and discusses examples of sensitive mental health topics including suicide and selfharm,http://arxiv.org/abs/2406.11852v2,1
predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest with very promising results being reported across many studies such approaches have the potential to revolutionise mental health assessment if their development and evaluation follows a real world deployment setting in this work we take a closer look at stateoftheart approaches using different mental health datasets and indicators different feature sources and multiple simulations in order to assess their ability to generalise we demonstrate that under a pragmatic evaluation framework none of the approaches deliver or even approach the reported performances in fact we show that current stateoftheart approaches can barely outperform the most naive baselines in the realworld setting posing serious questions not only about their deployment ability but also about the contribution of the derived features for the mental health assessment task and how to make better use of such data in the future,http://arxiv.org/abs/1807.07351v1,1
as the popularity of social media platforms continues to rise an everincreasing amount of human communication and self expression takes place online most recent research has focused on mining social media for public user opinion about external entities such as product reviews or sentiment towards political news however less attention has been paid to analyzing users internalized thoughts and emotions from a mental health perspective in this paper we quantify the semantic difference between public tweets and private mental health journals used in online cognitive behavioral therapy we will use deep transfer learning techniques for analyzing the semantic gap between the two domains we show that for the task of emotional valence prediction social media can be successfully harnessed to create more accurate robust and personalized mental health models our results suggest that the semantic gap between public and private selfexpression is small and that utilizing the abundance of available social media is one way to overcome the small sample sizes of mental health data which are commonly limited by availability and privacy concerns,http://arxiv.org/abs/1708.01372v1,1
the covid19 pandemic like many of the disease outbreaks that have preceded it is likely to have a profound effect on mental health understanding its impact can inform strategies for mitigating negative consequences in this work we seek to better understand the effects of covid19 on mental health by examining discussions within mental health support communities on reddit first we quantify the rate at which covid19 is discussed in each community or subreddit in order to understand levels of preoccupation with the pandemic next we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen finally we analyze how covid19 has influenced language use and topics of discussion within each subreddit,http://arxiv.org/abs/2009.04008v1,1
empathy is critical to successful mental health support empathy measurement has predominantly occurred in synchronous facetoface settings and may not translate to asynchronous textbased contexts because millions of people use textbased platforms for mental health support understanding empathy in these contexts is crucial in this work we present a computational approach to understanding how empathy is expressed in online mental health platforms we develop a novel unifying theoreticallygrounded framework for characterizing the communication of empathy in textbased conversations we collect and share a corpus of 10k post response pairs annotated using this empathy framework with supporting evidence for annotations rationales we develop a multitask robertabased biencoder model for identifying empathy in conversations and extracting rationales underlying its predictions experiments demonstrate that our approach can effectively identify empathic conversations we further apply this model to analyze 235k mental health interactions and show that users do not selflearn empathy over time revealing opportunities for empathy training and feedback,http://arxiv.org/abs/2009.08441v1,1
mental health is a global epidemic affecting close to half a billion people worldwide chronic shortage of resources hamper detection and recovery of affected people effective sensing technologies can help fight the epidemic through early detection prediction and resulting proper treatment existing and novel technologies for sensing mental health state could address the aforementioned concerns by activating granular tracking of physiological behavioral and social signals pertaining to problems in mental health our paper focuses on the available methods of sensing mental health problems through direct and indirect measures we see how active and passive sensing by technologies as well as reporting from relevant sources can contribute toward these detection methods we also see available methods of therapeutic treatment available through digital means we highlight a few key intervention technologies that are being developed by researchers to fight against mental illness issues,http://arxiv.org/abs/2009.12488v1,1
while there is an emergence of research investigating the educational impacts of the covid19 pandemic empirical studies assessing teacher mental health throughout the pandemic have been scarce using a large national dataset the current study first compared mental health outcomes during the pandemic between pk12 teachers and professionals in other occupations further we compared the prevalence of mental health outcomes between inperson and remote teachers n 131154 findings indicated teachers reported greater mental health concerns than those in other professions and that remote teachers reported significantly higher levels of distress than those teaching inperson policy implications are discussed with a focus on providing support to meet the evolving needs of teachers,http://arxiv.org/abs/2109.01547v1,1
interpersonal relationships are necessary for successful daily functioning and wellbeing numerous studies have demonstrated the importance of social connectivity for mental health both through direct peertopeer influence and by the location of individuals within their social network passive monitoring using smartphones provides an advanced tool to map social networks based on the proximity between individuals this study investigates the feasibility of using a smartphone app to measure and assess the relationship between social network metrics and mental health the app collected bluetooth and mental health data in 63 participants social networks of proximity were estimated from bluetooth data and 95 of the edges were scanned at least every 30 minutes the majority of participants found this method of data collection acceptable and reported that they would be likely to participate in future studies using this app these findings demonstrate the feasibility of using a smartphone app that participants can install on their own phone to investigate the relationship between social connectivity and mental health,http://arxiv.org/abs/1702.02644v2,1
we introduce initial groundwork for estimating suicide risk and mental health in a deep learning framework by modeling multiple conditions the system learns to make predictions about suicide risk and mental health at a low false positive rate conditions are modeled as tasks in a multitask learning mtl framework with gender prediction as an additional auxiliary task we demonstrate the effectiveness of multitask learning by comparison to a welltuned singletask baseline with the same number of parameters our best mtl model predicts potential suicide attempt as well as the presence of atypical mental health with auc 08 we also find additional large improvements using multitask learning on mental health tasks with limited training data,http://arxiv.org/abs/1712.03538v1,1
datadriven methods for mental health treatment and surveillance have become a major focus in computational science research in the last decade however progress in the domain in terms of both medical understanding and system performance remains bounded by the availability of adequate data prior systematic reviews have not necessarily made it possible to measure the degree to which datarelated challenges have affected research progress in this paper we offer an analysis specifically on the state of social media data that exists for conducting mental health research we do so by introducing an opensource directory of mental health datasets annotated using a standardized schema to facilitate metaanalysis,http://arxiv.org/abs/2011.05233v2,1
housing expenditure tends to be sticky and costly to adjust and makes up a large proportion of household expenditure additionally the loss of housing can have catastrophic consequences these specific features of housing expenditure imply that housing stress could cause negative mental health impacts this research investigates the effects of housing stress on mental health contributing to the literature by nesting housing stress within a measure of financial hardship thus improving robustness to omitted variables and creating a natural comparison group for matching fixed effects fe regressions and a differenceindifferences did methodology are estimated utilising data from the household income and labour dynamics in australia hilda survey the results show that renters who are in housing stress have a significant decline in selfreported mental health with those in prior financial hardship being more severely affected in contrast there is little to no evidence of housing stress impacting on owners with a mortgage the results also suggest that the mental health impact of housing stress is more important than some but not all aspects of financial hardship,http://arxiv.org/abs/2205.01255v1,1
understanding human behavior and monitoring mental health are essential to maintaining the community and societys safety as there has been an increase in mental health problems during the covid19 pandemic due to uncontrolled mental health early detection of mental issues is crucial nowadays the usage of intelligent virtual personal assistants iva has increased worldwide individuals use their voices to control these devices to fulfill requests and acquire different services this paper proposes a novel deep learning model based on the gated recurrent neural network and convolution neural network to understand human emotion from speech to improve their iva services and monitor their mental health,http://arxiv.org/abs/2208.12812v3,1
pain is a common reason for accessing healthcare resources and is a growing area of research especially in its overlap with mental health mental health electronic health records are a good data source to study this overlap however much information on pain is held in the free text of these records where mentions of pain present a unique natural language processing problem due to its ambiguous nature this project uses data from an anonymised mental health electronic health records database the data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not this will facilitate the extraction of relevant pain information from large databases and the use of such outputs for further studies on pain and mental health 1985 documents were manually tripleannotated for creation of gold standard training data which was used to train three commonly used classification algorithms the best performing model achieved an f1score of 098 95 ci 098099,http://arxiv.org/abs/2304.01240v2,1
social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope however many users fail to receive genuine clinical support thus exacerbating their symptoms screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives pretrained language models lms can assess users social media data and classify them in terms of their mental health risk we propose a questionanswering qa approach to assess mental health risk using the unifiedqa model on two large mental health datasets to protect user data we extend unifiedqa by anonymizing the model training process using differential privacy our results demonstrate the effectiveness of modeling risk assessment as a qa task specifically for mental health use cases furthermore the models performance decreases by less than 1 with the inclusion of differential privacy the proposed systems performance is indicative of a promising research direction that will lead to the development of privacyaware diagnostic systems,http://arxiv.org/abs/2306.05652v1,1
mental health disorders remain a significant challenge in modern healthcare with diagnosis and treatment often relying on subjective patient descriptions and past medical history to address this issue we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts allowing data to remain on users devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacyaware and accountable manner we evaluate our model using a popular mental health dataset that demonstrates promising results by utilizing connected health systems and machine learning models our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients mental health outside of traditional office visits,http://arxiv.org/abs/2307.04777v1,1
regarding the rising number of people suffering from mental health illnesses in todays society the importance of mental health cannot be overstated wearable sensors which are increasingly widely available provide a potential way to track and comprehend mental health issues these gadgets not only monitor everyday activities but also continuously record vital signs like heart rate perhaps providing information on a persons mental state recent research has used these sensors in conjunction with machine learning methods to identify patterns relating to different mental health conditions highlighting the immense potential of this data beyond simple activity monitoring in this research we present a novel algorithm called the hybrid random forest neural network that has been tailored to evaluate sensor data from depressed patients our method has a noteworthy accuracy of 80 when evaluated on a special dataset that included both unipolar and bipolar depressive patients as well as healthy controls the findings highlight the algorithms potential for reliably determining a persons depression condition using sensor data making a substantial contribution to the area of mental health diagnostics,http://arxiv.org/abs/2310.09277v1,1
selfguided mental health interventions such as doityourself tools to learn and practice coping strategies show great promise to improve access to mental health care however these interventions are often cognitively demanding and emotionally triggering creating accessibility barriers that limit their widescale implementation and adoption in this paper we study how humanlanguage model interaction can support selfguided mental health interventions we take cognitive restructuring an evidencebased therapeutic technique to overcome negative thinking as a case study in an irbapproved randomized field study on a large mental health website with 15531 participants we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring our findings reveal that our system positively impacts emotional intensity for 67 of participants and helps 65 overcome negative thoughts although adolescents report relatively worse outcomes we find that tailored interventions that simplify language model generations improve overall effectiveness and equity,http://arxiv.org/abs/2310.15461v2,1
mental health conversational agents aka chatbots are widely studied for their potential to offer accessible support to those experiencing mental health challenges previous surveys on the topic primarily consider papers published in either computer science or medicine leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains to bridge this gap we conduct a comprehensive literature review using the prisma framework reviewing 534 papers published in both computer science and medicine our systematic review reveals 136 key papers on building mental healthrelated conversational agents with diverse characteristics of modeling and experimental design techniques we find that computer science papers focus on llm techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rulebased conversational agents and outcome metrics to measure the health outcomes of participants based on our findings on transparency ethics and cultural heterogeneity in this review we provide a few recommendations to help bridge the disciplinary divide and enable the crossdisciplinary development of mental health conversational agents,http://arxiv.org/abs/2310.17017v1,1
the global mental health crisis is looming with a rapid increase in mental disorders limited resources and the social stigma of seeking treatment as the field of artificial intelligence ai has witnessed significant advancements in recent years large language models llms capable of understanding and generating humanlike text may be used in supporting or providing psychological counseling however the application of llms in the mental health domain raises concerns regarding the accuracy effectiveness and reliability of the information provided this paper investigates the major challenges associated with the development of llms for psychological counseling including model hallucination interpretability bias privacy and clinical effectiveness we explore potential solutions to these challenges that are practical and applicable to the current paradigm of ai from our experience in developing and deploying llms for mental health ai holds a great promise for improving mental health care if we can carefully navigate and overcome pitfalls of llms,http://arxiv.org/abs/2311.13857v1,1
mental health conditions prevalent across various demographics necessitate efficient monitoring to mitigate their adverse impacts on life quality the surge in datadriven methodologies for mental health monitoring has underscored the importance of privacypreserving techniques in handling sensitive health data despite strides in federated learning for mental health monitoring existing approaches struggle with vulnerabilities to certain cyberattacks and data insufficiency in realworld applications in this paper we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency to accomplish this we integrate federated learning with two pivotal elements 1 differential privacy achieved by introducing noise into the updates and 2 transfer learning employing a pretrained universal model to adeptly address issues of data imbalance and insufficiency we evaluate the framework by a case study on stress detection employing a dataset of physiological and contextual data from a longitudinal study our finding show that the proposed approach can attain a 10 boost in accuracy and a 21 enhancement in recall while ensuring privacy protection,http://arxiv.org/abs/2402.10862v2,1
mental health challenges are on the rise in our modern society and the imperative to address mental disorders especially regarding anxiety depression and suicidal thoughts underscores the need for effective interventions this paper delves into the application of recent advancements in pretrained contextualized language models to introduce mindguide an innovative chatbot serving as a mental health assistant for individuals seeking guidance and support in these critical areas mindguide leverages the capabilities of langchain and its chatmodels specifically chatopenai as the bedrock of its reasoning engine the system incorporates key features such as langchains chatprompt template humanmessage prompt template conversationbuffermemory and llmchain creating an advanced solution for early detection and comprehensive support within the field of mental health additionally the paper discusses the implementation of streamlit to enhance the user experience and interaction with the chatbot this novel approach holds great promise for proactive mental health intervention and assistance,http://arxiv.org/abs/2403.05568v1,1
the field of digital mental health is advancing at a rapid pace passively collected data from user engagements with digital tools and services continue to contribute new insights into mental health and illness as the field of digital mental health grows a concerning norm has been established digital service users are given little say over how their data is collected shared or used to generate revenue for private companies given a long history of service user exclusion from data collection practices we propose an alternative approach that is attentive to this history the consentforward paradigm this paradigm embeds principles of affirmative consent in the design of digital mental health tools and services strengthening trust through designing around individual choices and needs and proactively protecting users from unexpected harm in this perspective we outline practical steps to implement this paradigm toward ensuring that people searching for care have the safest experiences possible,http://arxiv.org/abs/2404.14548v1,1
providing timely support and intervention is crucial in mental health settings as the need to engage youth comfortable with texting increases mental health providers are exploring and adopting textbased media such as chatbots communitybased forums online therapies with licensed professionals and helplines operated by trained responders to support these textbased media for mental healthparticularly for crisis carewe are developing a system to perform passive emotionsensing using a combination of keystroke dynamics and sentiment analysis our early studies of this system posit that the analysis of short text messages and keyboard typing patterns can provide emotion information that may be used to support both clients and responders we use our preliminary findings to discuss the way forward for applying ai to support mental health providers in providing better care,http://arxiv.org/abs/2406.11135v1,1
nlp in mental health has been primarily social media focused real world practitioners also have high case loads and often domain specific variables of which modern llms lack context we take a dataset made by recruiting 644 participants including individuals diagnosed with bipolar disorder bd schizophrenia sz and healthy controls hc participants undertook tasks derived from a standardized mental health instrument and the resulting data were transcribed and annotated by experts across five clinical variables this paper demonstrates the application of contemporary language models in sequencetosequence tasks to enhance mental health research specifically we illustrate how these models can facilitate the deployment of mental health instruments data collection and data annotation with high accuracy and scalability we show that small models are capable of annotation for domainspecific clinical variables data collection for mentalhealth instruments and perform better then commercial large models,http://arxiv.org/abs/2406.12687v1,1
as mental health issues globally escalate there is a tremendous need for advanced digital support systems we introduce mentalagora a novel framework employing large language models enhanced by interaction between multiple agents for tailored mental health support this framework operates through three stages strategic debating tailored counselor creation and response generation enabling the dynamic customization of responses based on individual user preferences and therapeutic needs we conduct experiments utilizing a highquality evaluation dataset therapytalk crafted with mental health professionals shwoing that mentalagora generates expertaligned and user preferenceenhanced responses our evaluations including experiments and user studies demonstrate that mentalagora aligns with professional standards and effectively meets user preferences setting a new benchmark for digital mental health interventions,http://arxiv.org/abs/2407.02736v1,1
mental health has become a growing concern among university students while medication is a common treatment understanding how university students manage their medication for mental health symptoms in realworld practice has not been fully explored in this study we conducted semistructured interviews with university students to understand the unique challenges in the mental health medication management process and their coping strategies particularly examining the role of various technologies in this process we discovered that due to struggles with selfacceptance and the interdependent relationship between medication symptoms schedules and life changes the medication management process for students was a highly dynamic journey involving frequent dosage changes thus students adopted flexible strategies of using minimal technology to manage their medication in different situations while maintaining a high degree of autonomy based on our findings we propose design implications for future technologies to seamlessly integrate into their daily lives and assist students in managing their mental health medications,http://arxiv.org/abs/2408.07784v1,1
this paper introduces mhgpt a lightweight generative pretrained transformer trained on mental healthrelated social media and pubmed articles finetuned for specific mental health tasks mhgpt was evaluated under limited hardware constraints and compared with stateoftheart models like mentallama and gemma despite having only 198 billion parameters and using just 5 of the dataset mhgpt outperformed larger models and matched the performance of models trained on significantly more data the key contributions include integrating diverse mental health data creating a custom tokenizer and optimizing a smaller architecture for lowresource settings this research could advance aidriven mental health care especially in areas with limited computing power,http://arxiv.org/abs/2408.08261v1,1
we present theragen an advanced aipowered mental health chatbot utilizing the llama 2 7b model this approach builds upon recent advancements in language models and transformer architectures theragen provides allday personalized compassionate mental health care by leveraging a large dataset of 1 million conversational entries combining anonymized therapy transcripts online mental health discussions and psychological literature including apa resources our implementation employs transfer learning finetuning and advanced training techniques to optimize performance theragen offers a userfriendly interface for seamless interaction providing empathetic responses and evidencebased coping strategies evaluation results demonstrate high user satisfaction rates with 94 of users reporting improved mental wellbeing the system achieved a bleu score of 067 and a rouge score of 062 indicating strong response accuracy with an average response time of 1395 milliseconds theragen ensures realtime efficient support while not a replacement for professional therapy theragen serves as a valuable complementary tool significantly improving user wellbeing and addressing the accessibility gap in mental health treatments this paper details theragens architecture training methodology ethical considerations and future directions contributing to the growing field of aiassisted mental healthcare and offering a scalable solution to the pressing need for mental health support,http://arxiv.org/abs/2409.13748v1,1
this research work delves into the manifestation of hallucination within large language models llms and its consequential impacts on applications within the domain of mental health the primary objective is to discern effective strategies for curtailing hallucinatory occurrences thereby bolstering the dependability and security of llms in facilitating mental health interventions such as therapy counseling and the dissemination of pertinent information through rigorous investigation and analysis this study seeks to elucidate the underlying mechanisms precipitating hallucinations in llms and subsequently propose targeted interventions to alleviate their occurrence by addressing this critical issue the research endeavors to foster a more robust framework for the utilization of llms within mental health contexts ensuring their efficacy and reliability in aiding therapeutic processes and delivering accurate information to individuals seeking mental health support,http://arxiv.org/abs/2410.10853v1,1
leftbehind children lbcs numbering over 66 million in china face severe mental health challenges due to parental migration for work early screening and identification of atrisk lbcs is crucial yet challenging due to the severe shortage of mental health professionals especially in rural areas while the housetreeperson htp test shows higher child participation rates its requirement for expert interpretation limits its application in resourcescarce regions to address this challenge we propose psydraw a multiagent system based on multimodal large language models that assists mental health professionals in analyzing htp drawings the system employs specialized agents for feature extraction and psychological interpretation operating in two stages comprehensive feature analysis and professional report generation evaluation of htp drawings from 290 primary school students reveals that 7103 of the analyzes achieved high consistency with professional evaluations 2621 moderate consistency and only 241 low consistency the system identified 3103 of cases requiring professional attention demonstrating its effectiveness as a preliminary screening tool currently deployed in pilot schools method shows promise in supporting mental health professionals particularly in resourcelimited areas while maintaining high professional standards in psychological assessment,http://arxiv.org/abs/2412.14769v1,1
in the emotion regulation literature the amount of neuroimaging studies on cognitive reappraisal led the impression that the same topdown controlrelated neural mechanisms characterize all emotion regulation strategies however topdown processes may coexist with more bottomup and emotionfocused processes that partially bypass the recruitment of executive functions a case in point is acceptancebased strategies to better understand neural commonalities and differences behind different emotion regulation strategies in the present study we applied a metaanalytic method to fmri studies of taskrelated activity of reappraisal and acceptance results showed increased activity in leftinferior frontal gyrus and insula for both strategies and decreased activity in the basal ganglia for reappraisal and decreased activity in limbic regions for acceptance these findings are discussed in the context of a model of common and specific neural mechanisms of emotion regulation that support and expand the previous dualroutes models we suggest that emotion regulation may rely on a core inhibitory circuit and on strategyspecific topdown and bottomup processes distinct for different strategies,http://arxiv.org/abs/2305.16241v1,1
mental health problems among the global population are worsened during the coronavirus disease covid19 how individuals engage with online platforms such as google search and youtube undergoes drastic shifts due to pandemic and subsequent lockdowns such ubiquitous daily behaviors on online platforms have the potential to capture and correlate with clinically alarming deteriorations in mental health profiles in a noninvasive manner the goal of this study is to examine among college students the relationship between deteriorating mental health conditions and changes in user behaviors when engaging with google search and youtube during covid19 this study recruited a cohort of 49 students from a us college campus during january 2020 prior to the pandemic and measured the anxiety and depression levels of each participant this study followed up with the same cohort during may 2020 during the pandemic and the anxiety and depression levels were assessed again the longitudinal google search and youtube history data were anonymized and collected from individuallevel google search and youtube histories we developed 5 signals that can quantify shifts in online behaviors during the pandemic we then assessed the differences between groups with and without deteriorating mental health profiles in terms of these features significant features included latenight online activities continuous usages and time away from the internet porn consumptions and keywords associated with negative emotions social activities and personal affairs though further studies are required our results demonstrated the feasibility of utilizing pervasive online data to establish noninvasive surveillance systems for mental health conditions that bypasses many disadvantages of existing screening methods,http://arxiv.org/abs/2009.09076v1,1
vaccination has been promoted to mitigate the spread of the coronavirus disease 2019 covid19 vaccination is expected to reduce the probability of and alleviate the seriousness of covid19 infection accordingly this might significantly change an individuals subjective wellbeing and mental health however it is unknown how vaccinated people perceive the effectiveness of covid19 and how their subjective wellbeing and mental health change after vaccination we thus observed the same individuals on a monthly basis from march 2020 to september 2021 in all parts of japan then large sample panel data n54007 were independently constructed using the data we compared the individuals perceptions of covid19 subjective wellbeing and mental health before and after vaccination furthermore we compared the effect of vaccination on the perceptions of covid19 and mental health for females and males we used the fixedeffects model to control for individual timeinvariant characteristics the major findings were as follows first the vaccinated people perceived the probability of getting infected and the seriousness of covid19 to be lower than before vaccination this was observed not only when we used the whole sample but also when we used subsamples second using the whole sample subjective wellbeing and mental health improved the same results were also observed using the subsample of females whereas the improvements were not observed using a subsample of males,http://arxiv.org/abs/2203.07663v1,1
the psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient unlike general clinical discussions psychotherapys core components viz symptoms are hard to distinguish thus becoming a complex problem to summarize later a structured counseling conversation may contain discussions about symptoms history of mental health issues or the discovery of the patients behavior it may also contain discussion filler words irrelevant to a clinical summary we refer to these elements of structured psychotherapy as counseling components in this paper the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning we create a new dataset after annotating 129k utterances of counseling components and reference summaries for each dialogue further we propose consum a novel counselingcomponent guided summarization model consum undergoes three independent modules first to assess the presence of depressive symptoms it filters utterances utilizing the patient health questionnaire phq9 while the second and third modules aim to classify counseling components at last we propose a problemspecific mental health information capture mhic evaluation metric for counseling summaries our comparative study shows that we improve on performance and generate cohesive semantic and coherent summaries we comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements human and clinical evaluations on the summary show that consum generates quality summary further mental health experts validate the clinical acceptability of the consum lastly we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of mpathicai,http://arxiv.org/abs/2206.03886v1,1
there has been an increase in research in developing machine learning models for mental health detection or prediction in recent years due to increased mental health issues in society effective use of mental health prediction or detection models can help mental health practitioners redefine mental illnesses more objectively than currently done and identify illnesses at an earlier stage when interventions may be more effective however there is still a lack of standard in evaluating bias in such machine learning models in the field which leads to challenges in providing reliable predictions and in addressing disparities this lack of standards persists due to factors such as technical difficulties complexities of high dimensional clinical health data etc which are especially true for physiological signals this along with prior evidence of relations between some physiological signals with certain demographic identities restates the importance of exploring bias in mental health prediction models that utilize physiological signals in this work we aim to perform a fairness analysis and implement a multitask learning based bias mitigation method on anxiety prediction models using ecg data our method is based on the idea of epistemic uncertainty and its relationship with model weights and feature space representation our analysis showed that our anxiety prediction base model introduced some bias with regards to age income ethnicity and whether a participant is born in the us or not and our bias mitigation method performed better at reducing the bias in the model when compared to the reweighting mitigation technique our analysis on feature importance also helped identify relationships between heart rate variability and multiple demographic groupings,http://arxiv.org/abs/2208.03621v1,1
the latest large language models llms such as chatgpt exhibit strong capabilities in automated mental health analysis however existing relevant studies bear several limitations including inadequate evaluations lack of prompting strategies and ignorance of exploring llms for explainability to bridge these gaps we comprehensively evaluate the mental health analysis and emotional reasoning ability of llms on 11 datasets across 5 tasks we explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information based on these prompts we explore llms for interpretable mental health analysis by instructing them to generate explanations for each of their decisions we convey strict human evaluations to assess the quality of the generated explanations leading to a novel dataset with 163 humanassessed explanations we benchmark existing automatic evaluation metrics on this dataset to guide future related works according to the results chatgpt shows strong incontext learning ability but still has a significant gap with advanced taskspecific methods careful prompt engineering with emotional cues and expertwritten fewshot examples can also effectively improve performance on mental health analysis in addition chatgpt generates explanations that approach human performance showing its great potential in explainable mental health analysis,http://arxiv.org/abs/2304.03347v4,1
mental health significantly influences various aspects of our daily lives and its importance has been increasingly recognized by the research community and the general public particularly in the wake of the covid19 pandemic this heightened interest is evident in the growing number of publications dedicated to mental health in the past decade in this study our goal is to identify general trends in the field and pinpoint highimpact research topics by analyzing a large dataset of mental health research papers to accomplish this we collected abstracts from various databases and trained a customized sentencebert based embedding model leveraging the bertopic framework our dataset comprises 96676 research papers pertaining to mental health enabling us to examine the relationships between different topics using their abstracts to evaluate the effectiveness of the model we compared it against two other stateoftheart methods top2vec model and ldabert model the model demonstrated superior performance in metrics that measure topic diversity and coherence to enhance our analysis we also generated word clouds to provide a comprehensive overview of the machine learning models applied in mental health research shedding light on commonly utilized techniques and emerging trends furthermore we provide a github link to the dataset used in this paper ensuring its accessibility for further research endeavors,http://arxiv.org/abs/2308.13569v1,1
the past decade has been transformative for mental health research and practice the ability to harness large repositories of data whether from electronic health records ehr mobile devices or social media has revealed a potential for valuable insights into patient experiences promising early proactive interventions as well as personalized treatment plans recent developments in generative artificial intelligence particularly large language models llms show promise in leading digital mental health to uncharted territory patients are arriving at doctors appointments with information sourced from chatbots stateoftheart llms are being incorporated in medical software and ehr systems and chatbots from an everincreasing number of startups promise to serve as ai companions friends and partners this article presents contemporary perspectives on the opportunities and risks posed by llms in the design development and implementation of digital mental health tools we adopt an ecological framework and draw on the affordances offered by llms to discuss four application areas careseeking behaviors from individuals in need of care community care provision institutional and medical care provision and larger care ecologies at the societal level we engage in a thoughtful consideration of whether and how llmbased technologies could or should be employed for enhancing mental health the benefits and harms our article surfaces could serve to help shape future research advocacy and regulatory efforts focused on creating more responsible userfriendly equitable and secure llmbased tools for mental health treatment and intervention,http://arxiv.org/abs/2311.14693v1,1
online mental health support communities have grown in recent years for providing accessible mental and emotional health support through volunteer counselors despite millions of people participating in chat support on these platforms the clinical effectiveness of these communities on mental health symptoms remains unknown furthermore although volunteers receive some training based on established therapeutic skills studied in facetoface environments such as active listening and motivational interviewing it remains understudied how the usage of these skills in this online context affects peoples mental health status in our work we collaborate with one of the largest online peer support platforms and use both natural language processing and machine learning techniques to measure how oneonone support chats affect depression and anxiety symptoms we measure how the techniques and characteristics of support providers such as using affirmation empathy and past experience on the platform affect supportseekers mental health changes we find that online peer support chats improve both depression and anxiety symptoms with a statistically significant but relatively small effect size additionally support providers techniques such as emphasizing the autonomy of the client lead to better mental health outcomes however we also found that some behaviors eg persuading are actually harmful to depression and anxiety outcomes our work provides key understanding for mental health care in the online setting and designing training systems for online support providers,http://arxiv.org/abs/2312.10775v1,1
background rapid advancements in natural language processing have led to the development of large language models with the potential to revolutionize mental health care these models have shown promise in assisting clinicians and providing support to individuals experiencing various psychological challenges objective this study aims to compare the performance of two large language models gpt4 and chatgpt in responding to a set of 18 psychological prompts to assess their potential applicability in mental health care settings methods a blind methodology was employed with a clinical psychologist evaluating the models responses without knowledge of their origins the prompts encompassed a diverse range of mental health topics including depression anxiety and trauma to ensure a comprehensive assessment results the results demonstrated a significant difference in performance between the two models p 005 gpt4 achieved an average rating of 829 out of 10 while chatgpt received an average rating of 652 the clinical psychologists evaluation suggested that gpt4 was more effective at generating clinically relevant and empathetic responses thereby providing better support and guidance to potential users conclusions this study contributes to the growing body of literature on the applicability of large language models in mental health care settings the findings underscore the importance of continued research and development in the field to optimize these models for clinical use further investigation is necessary to understand the specific factors underlying the performance differences between the two models and to explore their generalizability across various populations and mental health conditions,http://arxiv.org/abs/2405.09300v1,1
pretrained language models plms have the potential to transform mental health support by providing accessible and culturally sensitive resources however despite this potential their effectiveness in mental health care and specifically for the arabic language has not been extensively explored to bridge this gap this study evaluates the effectiveness of foundational models for classification of questions and answers qa in the domain of mental health care we leverage the mentalqa dataset an arabic collection featuring qa interactions related to mental health in this study we conducted experiments using four different types of learning approaches traditional feature extraction plms as feature extractors finetuning plms and prompting large language models gpt35 and gpt4 in zeroshot and fewshot learning settings while traditional feature extractors combined with support vector machines svm showed promising performance plms exhibited even better results due to their ability to capture semantic meaning for example marbert achieved the highest performance with a jaccard score of 080 for question classification and a jaccard score of 086 for answer classification we further conducted an indepth analysis including examining the effects of finetuning versus nonfinetuning the impact of varying data size and conducting error analysis our analysis demonstrates that finetuning proved to be beneficial for enhancing the performance of plms and the size of the training data played a crucial role in achieving high performance we also explored prompting where fewshot learning with gpt35 yielded promising results there was an improvement of 12 for question and classification and 45 for answer classification based on our findings it can be concluded that plms and promptbased approaches hold promise for mental health support in arabic,http://arxiv.org/abs/2406.15966v1,1
mental health disorders are one of the most serious diseases in the world most people with such a disease lack access to adequate care which highlights the importance of training models for the diagnosis and treatment of mental health disorders however in the mental health domain privacy concerns limit the accessibility of personalized treatment data making it challenging to build powerful models in this paper we introduce mentalarena a selfplay framework to train language models by generating domainspecific personalized data where we obtain a better model capable of making a personalized diagnosis and treatment as a therapist and providing information as a patient to accurately model humanlike mental health patients we devise symptom encoder which simulates a real patient from both cognition and behavior perspectives to address intent bias during patienttherapist interactions we propose symptom decoder to compare diagnosed symptoms with encoded symptoms and dynamically manage the dialogue between patient and therapist according to the identified deviations we evaluated mentalarena against 6 benchmarks including biomedicalqa and mental health tasks compared to 6 advanced models our models finetuned on both gpt35 and llama38b significantly outperform their counterparts including gpt4o we hope that our work can inspire future research on personalized care code is available in,http://arxiv.org/abs/2410.06845v1,1
mental health issues significantly impact individuals daily lives yet many do not receive the help they need even with available online resources this study aims to provide diverse accessible stigmafree personalized and realtime mental health support through cuttingedge ai technologies it makes the following contributions 1 conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs 2 introducing soullmate an adaptive llmdriven system that integrates llm technologies chain retrievalaugmented generation rag prompt engineering and domain knowledge this system offers advanced features such as risk detection and proactive guidance dialogue and utilizes rag for personalized profile uploads and conversational information extraction 3 developing novel evaluation approaches for preliminary assessments and risk detection via professionally annotated interview data and reallife suicide tendency data 4 proposing the key indicator summarization kis proactive questioning strategy pqs and stacked multimodel reasoning smmr methods to enhance model performance and usability through contextsensitive response adjustments semantic coherence evaluations and enhanced accuracy of longcontext reasoning in language models this study contributes to advancing mental health support technologies potentially improving the accessibility and effectiveness of mental health care globally,http://arxiv.org/abs/2410.16322v1,1
the crisis of mental health issues is escalating effective counseling serves as a critical lifeline for individuals suffering from conditions like ptsd stress etc therapists forge a crucial therapeutic bond with clients steering them towards positivity unfortunately the massive shortage of professionals high costs and mental health stigma pose significant barriers to consulting therapists as a substitute virtual mental health assistants vmhas have emerged in the digital healthcare space however most existing vmhas lack the commonsense to understand the nuanced sentiments of clients to generate effective responses to this end we propose empres a novel sentimentguided mechanism incorporating commonsense awareness for generating responses by leveraging foundation models and harnessing commonsense knowledge empres aims to generate responses that effectively shape the clients sentiment towards positivity we evaluate the performance of empres on hope a benchmark counseling dataset and observe a remarkable performance improvement compared to the existing baselines across a suite of qualitative and quantitative metrics moreover our extensive empirical analysis and human evaluation show that the generation ability of empres is wellsuited and in some cases surpasses the gold standard further we deploy empres as a chat interface for users seeking mental health support we address the deployed systems effectiveness through an exhaustive user study with a significant positive response our findings show that 91 of users find the system effective 80 express satisfaction and over 8545 convey a willingness to continue using the interface and recommend it to others demonstrating the practical applicability of empres in addressing the pressing challenges of mental health support emphasizing user feedback and ethical considerations in a realworld context,http://arxiv.org/abs/2501.03088v1,1
large language models llms have attracted significant attention for potential applications in digital health while their application in mental health is subject to ongoing debate this systematic review aims to evaluate the usage of llms in mental health focusing on their strengths and limitations in early screening digital interventions and clinical applications adhering to prisma guidelines we searched pubmed ieee xplore scopus jmir and acm using keywords mental health or mental illness or mental disorder or psychiatry and large language models we included articles published between january 1 2017 and april 30 2024 excluding nonenglish articles 30 articles were evaluated which included research on mental health conditions and suicidal ideation detection through text n15 usage of llms for mental health conversational agents cas n7 and other applications and evaluations of llms in mental health n18 llms exhibit substantial effectiveness in detecting mental health issues and providing accessible destigmatized ehealth services however the current risks associated with the clinical use might surpass their benefits the study identifies several significant issues the lack of multilingual datasets annotated by experts concerns about the accuracy and reliability of the content generated challenges in interpretability due to the black box nature of llms and persistent ethical dilemmas these include the lack of a clear ethical framework concerns about data privacy and the potential for overreliance on llms by both therapists and patients which could compromise traditional medical practice despite these issues the rapid development of llms underscores their potential as new clinical aids emphasizing the need for continued research and development in this area,http://arxiv.org/abs/2403.15401v3,1
depression has been a leading cause of mentalhealth illnesses across the world while the loss of lives due to unmanaged depression is a subject of attention so is the lack of diagnostic tests and subjectivity involved using behavioural cues to automate depression diagnosis and stage prediction in recent years has relatively increased however the absence of labelled behavioural datasets and a vast amount of possible variations prove to be a major challenge in accomplishing the task this paper proposes a novel custom cm ensemble approach and focuses on a paradigm of a crossplatform smartphone application that takes multimodal inputs from a user through a series of predefined questions sends it to the cloud ml architecture and conveys back a depression quotient representative of its severity our app estimates the severity of depression based on a multiclass classification model by utilizing the language audio and visual modalities the given approach attempts to detect emphasize and classify the features of a depressed person based on the lowlevel descriptors for verbal and visual features and context of the language features when prompted with a question the model achieved a precision value of 088 and an accuracy of 9156 further optimization reveals the intramodality and intermodality relevance through the selection of the most influential features within each modality for decision making,http://arxiv.org/abs/2009.05651v2,1
with the increasing demands of emotion comprehension and regulation in our daily life a customized musicbased emotion regulation system is introduced by employing current eeg information and song features which predicts users emotion variation in the valencearousal model before recommending music the work shows that 1 a novel musicbased emotion regulation system with a commercial eeg device is designed without employing deterministic emotion recognition models for daily usage 2 the system considers users variant emotions towards the same song and by which calculate users emotion instability and it is in accordance with big five personality test 3 the system supports different emotion regulation styles with users designation of desired emotion variation and achieves an accuracy of over 085 with 2seconds eeg data 4 people feel easier to report their emotion variation comparing with absolute emotional states and would accept a more delicate music recommendation system for emotion regulation according to the questionnaire,http://arxiv.org/abs/2211.14609v1,1
effective communication and strong therapeutic relationships are critical to successful mental health interventions for example in 1957 carl rogers a pioneer of personcentred therapy proposed that an empowering relationship could in and of itself create the necessary and sufficient conditions for positive therapeutic outcomes 1 whilst modern psychological theories no longer favour an exclusive focus on relationships positive relationships and the dynamics of clienttherapist communication remain cornerstones of mental health intervention theories a more recent metareview concluded that across all interventions models irrespective of the theoretical approach the quality of the relationship between therapists and clients is the second leading determinant of successful clinical outcomes 2 over the past ten years we david coyle and gavin doherty have designed and evaluated a wide range to systems that provide support for psychological or talk based mental health interventions 3 here we briefly consider two recent examples in each case our aim was to enhance communication and reshape clinical practice in a manner that empowers patients gnats island is a computer game that supports facetoface interventions for adolescents 4 mindbalance is an online treatment programme for adults experiencing difficulties with depression 5,http://arxiv.org/abs/1307.3164v1,1
using the metrics of the world health organisation the global burden of disease study has found that mental health difficulties are currently the leading cause of disability in developed countries 1 projections also indicate that the global burden of mental health difficulties will continue to rise in the coming decades the human and economic costs of this trend will be substantial in this paper we discuss how effectively designed interactive systems developed through collaborative interdisciplinary efforts can play a significant role in helping to address this challenge our discussion is grounded in a description of four exploratory systems each of which has undergone initial clinical evaluations directions for future research on mental health technologies are also identified,http://arxiv.org/abs/1307.3174v1,1
heart rate hr and its variability hrv has been proposed as a marker for depressive symptoms and other aspects of mental health however the real correlation between them is presently uncertain as previous studies have generally been conducted on the basis of small samples in a sample of 113 adult male prisoners we analyzed correlations between five measures of hrhrv and five psychological measures of mental health aspects depression state and trait anxiety and social relationships we used nadarayawatson nonparametric regression in both directions and agestratified spearman correlation to detect possible relations despite strong correlations among hrhrv measures and among psychological measures correlations between hrhrv and psychological measures were low and nonsignificant for the overall sample however we found an age dependency suggesting some correlations in younger people hr with staistate r 039 with hadsanxiety r 052 both p 005 overall the general utility of hrhrv as a marker for mental health across populations remains unclear future research should address age and other potential confounders more consistently,http://arxiv.org/abs/1501.05842v1,1
the intersection of data visualization and humanrobot interaction hri is a burgeoning field understanding communicating and processing different kinds of data for creating versatile visualizations can benefit hri conversely expressing different kinds of data generated from hri through effective visualizations can provide interesting insights our work adds to the literature of this growing domain in this paper we present our exploratory work on visualizing mental health data on a social robot particularly we discuss development of mental health data visualizations using a participatory design pd approach as a first step with mental health data visualization on a social robot this work paves the way for relevant further work and using social robots as data visualization tools,http://arxiv.org/abs/2210.06469v1,1
mental health resources available via websites and mobile apps provide support such as advice journaling and elements from cognitive behavioral therapy the proliferation of spoken conversational agents such as alexa siri and google home has led to an increasing interest in developing mental health apps for these devices we present the pilot study outcomes of an alexa skill that allows users to conduct depression and anxiety selftests ten participants were given access to the alexa skill for twoweeks followed by an online evaluation of the skills usability and trust our preliminary evaluation suggests that participants trusted the skill and scored the usability and user experience as average usage of the skill was low with most participants using the skill only once in view of workinprogress we also present a discussion of implementation and study design challenges to guide the current literature on designing spoken conversational agents for mental health applications,http://arxiv.org/abs/2008.03892v1,1
application of machine learning algorithms to the medical domain is an emerging trend that helps to advance medical knowledge at the same time there is a significant a lack of explainable studies that promote informed transparent and interpretable use of machine learning algorithms in this paper we present explainable multiclass classification of the covid19 mental health data in machine learning study we aim to find the potential factors to influence a personal mental health during the covid19 pandemic we found that random forest rf and gradient boosting gb have scored the highest accuracy of 6808 and 6819 respectively with lime prediction accuracy 655 for rf and 618 for gb we then compare a posthoc system local interpretable modelagnostic explanations or lime and an antehoc system gini importance in their ability to explain the obtained machine learning results to the best of these authors knowledge our study is the first explainable machine learning study of the mental health data collected during covid19 pandemics,http://arxiv.org/abs/2105.13430v1,1
for how many days during the past 30 days was your mental health not good the responses to this question measure selfreported mental health and can be linked to important covariates in the national health and nutrition examination survey nhanes however these count variables present major distributional challenges the data are overdispersed zeroinflated bounded by 30 and heaped in five and sevenday increments to meet these challenges we design a semiparametric estimation and inference framework for count data regression the datagenerating process is defined by simultaneously transforming and rounding star a latent gaussian regression model the transformation is estimated nonparametrically and the rounding operator ensures the correct support for the discrete and bounded data maximum likelihood estimators are computed using an em algorithm that is compatible with any continuous data model estimable by least squares star regression includes asymptotic hypothesis testing and confidence intervals variable selection via information criteria and customized diagnostics simulation studies validate the utility of this framework star is deployed to study the factors associated with selfreported mental health and demonstrates substantial improvements in goodnessoffit compared to existing count data regression models,http://arxiv.org/abs/2106.09114v2,1
students mental health problems have been explored previously in higher education literature in various contexts including empirical work involving quantitative and qualitative methods nevertheless comparatively few research could be found aiming for computational methods that learn information directly from data without relying on set parameters for a predetermined equation as an analytical method this study aims to investigate the performance of machine learning ml models used in higher education ml models considered are naive bayes support vector machine knearest neighbor logistic regression stochastic gradient descent decision tree random forest xgboost extreme gradient boosting decision tree and ngboost natural algorithm considering the factors of mental health illness among students we follow three phases of data processing segmentation feature extraction and classification we evaluate these ml models against classification performance metrics such as accuracy precision recall f1 score and predicted run time the empirical analysis includes two contributions 1 it examines the performance of various ml models on a surveybased educational dataset inferring a significant classification performance by a treebased xgboost algorithm 2 it explores the feature importance variables from the datasets to infer the significant importance of social support learning environment and childhood adversities on a students mental health illness,http://arxiv.org/abs/2202.13495v1,1
this paper studies the shortterm effects of ambient temperature on mental health using data on nearly half a million helpline calls in germany leveraging locationbased routing of helpline calls and random daytoday weather fluctuations i find a negative effect of temperature extremes on mental health as revealed by an increase in the demand for telephone counseling services on days with an average temperature above 25degc 77degf and below 0degc 32degf call volume is 34 and 51 percent higher respectively than on midtemperature days mechanism analysis reveals pronounced adverse effects of cold temperatures on social and psychological wellbeing and of hot temperatures on psychological wellbeing and violence more broadly the findings of this work contribute to our understanding of how changing climatic conditions will affect population mental health and associated social costs in the near future,http://arxiv.org/abs/2207.04992v2,1
mental health disorders may cause severe consequences on all the countries economies and health for example the impacts of the covid19 pandemic such as isolation and travel ban can make us feel depressed identifying early signs of mental health disorders is vital for example depression may increase an individuals risk of suicide the stateoftheart research in identifying mental disorder patterns from textual data uses handlabelled training sets especially when a domain experts knowledge is required to analyse various symptoms this task could be timeconsuming and expensive to address this challenge in this paper we study and analyse the various clinical and nonclinical approaches to identifying mental health disorders we leverage the domain knowledge and expertise in cognitive science to build a domainspecific knowledge base kb for the mental health disorder concepts and patterns we present a weaker form of supervision by facilitating the generating of training data from a domainspecific knowledge base kb we adopt a typical scenario for analysing social media to identify major depressive disorder symptoms from the textual content generated by social users we use this scenario to evaluate how our knowledgebased approach significantly improves the quality of results,http://arxiv.org/abs/2207.06254v1,1
there has been a significant expansion in the use of online social networks osns to support people experiencing mental health issues this paper studies the role of instagram influencers who specialize in coaching people with mental health issues using a dataset of 97k posts we characterize such users linguistic and behavioural features we explore how these observations impact audience engagement as measured by likes we show that the support provided by these accounts varies based on their selfdeclared professional identities for instance instagram accounts that declare themselves as authors offer less support than accounts that label themselves as coach we show that increasing information support in general communication positively affects user engagement however the effect of vocabulary on engagement is not consistent across the instagram account types our findings shed light on this understudied topic and guide how mental health practitioners can improve outreach,http://arxiv.org/abs/2211.06013v1,1
proper allocation of law enforcement resources remains a critical issue in crime prediction and prevention that operates by characterizing spatially aggregated crime activities and a multitude of predictor variables of interest despite the critical nature of proper resource allocation for mental health incidents there has been little progress in statistical modeling of the geospatial nature of mental health events in little rock arkansas in this article we provide insights into the spatial nature of mental health data from little rock arkansas between 2015 and 2018 under a supervised spatial modeling framework while extending the popular risk terrain modeling caplan et al 2011 2015 drawve 2016 approach we provide evidence of spatial clustering and identify the important features influencing such heterogeneity via a spatially informed hierarchy of generalized linear models spatial regression models and a tree based method viz poisson regression spatial durbin error model manski model and random forest the insights obtained from these different models are presented here along with their relative predictive performances the inferential tools developed here can be used in a broad variety of spatial modeling contexts and have the potential to aid both law enforcement agencies and the city in properly allocating resources,http://arxiv.org/abs/2212.05486v1,1
in recent years there has been a surge of interest in research on automatic mental health detection mhd from social media data leveraging advances in natural language processing and machine learning techniques while significant progress has been achieved in this interdisciplinary research area the vast majority of work has treated mhd as a binary classification task the multiclass classification setup is however essential if we are to uncover the subtle differences among the statistical patterns of language use associated with particular mental health conditions here we report on experiments aimed at predicting six conditions anxiety attention deficit hyperactivity disorder bipolar disorder posttraumatic stress disorder depression and psychological stress from reddit social media posts we explore and compare the performance of hybrid and ensemble models leveraging transformerbased architectures bert and roberta and bilstm neural networks trained on withintext distributions of a diverse set of linguistic features this set encompasses measures of syntactic complexity lexical sophistication and diversity readability and registerspecific ngram frequencies as well as sentiment and emotion lexicons in addition we conduct feature ablation experiments to investigate which types of features are most indicative of particular mental health conditions,http://arxiv.org/abs/2212.09839v1,1
mental health counseling remains a major challenge in modern society due to cost stigma fear and unavailability we posit that generative artificial intelligence ai models designed for mental health counseling could help improve outcomes by lowering barriers to access to this end we have developed a deep learning dl dialogue system called serena the system consists of a core generative model and postprocessing algorithms the core generative model is a 27 billion parameter seq2seq transformer finetuned on thousands of transcripts of personcenteredtherapy pct sessions the series of postprocessing algorithms detects contradictions improves coherency and removes repetitive answers serena is implemented and deployed on url which currently offers limited free services while the dialogue system is capable of responding in a qualitatively empathetic and engaging manner occasionally it displays hallucination and longterm incoherence overall we demonstrate that a deep learning mental health dialogue system has the potential to provide a lowcost and effective complement to traditional human counselors with less barriers to access,http://arxiv.org/abs/2301.09412v1,1
large language models llm have been successful in several natural language understanding tasks and could be relevant for natural language processing nlpbased mental health application research in this work we report the performance of llmbased chatgpt with gpt35turbo backend in three textbased mental health classification tasks stress detection 2class classification depression detection 2class classification and suicidality detection 5class classification we obtained annotated social media posts for the three classification tasks from public datasets then chatgpt api classified the social media posts with an input prompt for classification we obtained f1 scores of 073 086 and 037 for stress detection depression detection and suicidality detection respectively a baseline model that always predicted the dominant class resulted in f1 scores of 035 060 and 019 the zeroshot classification accuracy obtained with chatgpt indicates a potential use of language models for mental health classification tasks,http://arxiv.org/abs/2303.15727v1,1
psychiatrists diagnose mental disorders via the linguistic use of patients still due to data privacy existing passive mental health monitoring systems use alternative features such as activity app usage and location via mobile devices we propose fedtherapist a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacypreserving way via federated learning we explore multiple model designs by comparing their performance and overhead for fedtherapist to overcome the complex nature of ondevice language model training on smartphones we further propose a contextaware language learning call methodology to effectively utilize smartphones large and noisy text for mental health signal sensing our irbapproved evaluation of the prediction of selfreported depression stress anxiety and mood from 46 participants shows higher accuracy of fedtherapist compared with the performance with nonlanguage features achieving 015 auroc improvement and 821 mae reduction,http://arxiv.org/abs/2310.16538v1,1
people with dementia pwd often present verbal agitation such as cursing screaming and persistently complaining verbal agitation can impose mental distress on informal caregivers eg family friends which may cause severe mental illnesses such as depression and anxiety disorders to improve informal caregivers mental health we explore design opportunities by interviewing 11 informal caregivers suffering from verbal agitation of pwd in particular we first characterize how the predictability of verbal agitation impacts informal caregivers mental health and how caregivers coping strategies vary before during and after verbal agitation based on our findings we propose design opportunities to improve the mental health of informal caregivers suffering from verbal agitation distracting pwd insitu support before prompting justintime maneuvers information support during and comfort and education social information support after we discuss our reflections on cultural disparities between participants our work envisions a broader design space for supporting informal caregivers wellbeing and describes when and how that support could be provided,http://arxiv.org/abs/2311.10912v1,1
large language models llms have become valuable assets in mental health showing promise in both classification tasks and counseling applications this paper offers a perspective on using llms in mental health applications it discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability the paper also distinguishes between the often interchangeable terms explainability and interpretability advocating for developing inherently interpretable methods instead of relying on potentially hallucinated selfexplanations generated by llms despite the advancements in llms human counselors empathetic understanding nuanced interpretation and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling the use of llms should be approached with a judicious and considerate mindset viewing them as tools that complement human expertise rather than seeking to replace it,http://arxiv.org/abs/2311.11267v2,1
dialogue systems are increasingly integrated into mental health support to help clients facilitate exploration gain insight take action and ultimately heal themselves a practical and userfriendly dialogue system should be clientcentric focusing on the clients behaviors however existing dialogue systems publicly available for mental health support often concentrate solely on the counselors strategies rather than the behaviors expressed by clients this can lead to unreasonable or inappropriate counseling strategies and corresponding responses generated by the dialogue system to address this issue we propose psychat a clientcentric dialogue system that provides psychological support through online chat the clientcentric dialogue system comprises five modules client behavior recognition counselor strategy selection input packer response generator and response selection both automatic and human evaluations demonstrate the effectiveness and practicality of our proposed dialogue system for reallife mental health support furthermore the case study demonstrates that the dialogue system can predict the clients behaviors select appropriate counselor strategies and generate accurate and suitable responses,http://arxiv.org/abs/2312.04262v2,1
social media usage has been shown to have both positive and negative consequences for users mental health several studies indicated that peer feedback plays an important role in the relationship between social media use and mental health in this research we analyse the impact of receiving online feedback on users emotional experience social connectedness and selfesteem in an experimental study we let users interact with others on a facebooklike system over the course of a week while controlling for the amount of positive reactions they receive from their peers we find that experiencing little to no reaction from others does not only elicit negative emotions and stress amongst users but also induces low levels of selfesteem in contrast receiving much positive online feedback evokes feelings of social connectedness and reduces overall loneliness on a societal level our study can help to better understand the mechanisms through which social media use impacts mental health in a positive or negative way on a methodological level we provide a new opensource tool for designing and conducting social media experiments,http://arxiv.org/abs/2312.11914v1,1
mental health challenges pose considerable global burdens on individuals and communities recent data indicates that more than 20 of adults may encounter at least one mental disorder in their lifetime on the one hand the advancements in large language models have facilitated diverse applications yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health on the other hand across various applications an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language this study presents an initial evaluation of large language models in addressing this gap due to this we compare the performance of llama2 and chatgpt with classical machine as well as deep learning models our results on the daicwoz dataset show that transformerbased models like bert or xlnet outperform the large language models,http://arxiv.org/abs/2401.04592v2,1
mental health is a pressing concern in todays digital age particularly among youth who are deeply intertwined with technology despite the influx of technology solutions addressing mental health issues youth often remain sidelined during the design process while codesign methods have been employed to improve participation by youth many such initiatives are limited to design activities and lack training for youth to research and develop solutions for themselves in this case study we detail our 8week remote collaborative research initiative called youth welltech designed to facilitate remote codesign sprints aimed at equipping youth with the tools and knowledge to envision and design tech futures for their own communities we pilot this initiative with 12 student technology evangelists across 8 countries globally to foster the sharing of mental health challenges and diverse perspectives we highlight insights from our experiences running this global program remotely its structure and recommendations for coresearch,http://arxiv.org/abs/2401.05824v1,1
prior research on young adults mental health helpseeking mostly focuses on one particular resource such as a mobile app or digital platform paying less attention to their lived experiences interacting with the ecosystem of resources we conducted indepth interviews with 18 participants about their helpseeking and nonhelpseeking experiences guided by social ecological theory we proposed a sociotechnical ecosystem framework for mental health care consisting of four levels of resources including technological interpersonal community and societal level resources using this framework we identified two types of support systems for helpseeking singleresource support system and multiresource support system these resources support young adults helpseeking via three mechanisms textitcaregiving textitcaremediating and textitcareoutreaching forming various pathways to care we then pointed out the barriers to resource use at each level and the general challenges in finding a support system our findings contributed to a conceptual framework to categorize mental health care it also serves as a practical framework to identify challenges in the pathways to care and discover design implications,http://arxiv.org/abs/2401.08994v1,1
lgbtq individuals are increasingly turning to chatbots powered by large language models llms to meet their mental health needs however little research has explored whether these chatbots can adequately and safely provide tailored support for this demographic we interviewed 18 lgbtq and 13 nonlgbtq participants about their experiences with llmbased chatbots for mental health needs lgbtq participants relied on these chatbots for mental health support likely due to an absence of support in real life notably while llms offer prompt support they frequently fall short in grasping the nuances of lgbtqspecific challenges although finetuning llms to address lgbtq needs can be a step in the right direction it isnt the panacea the deeper issue is entrenched in societal discrimination consequently we call on future researchers and designers to look beyond mere technical refinements and advocate for holistic strategies that confront and counteract the societal biases burdening the lgbtq community,http://arxiv.org/abs/2402.09260v1,1
despite the increasing demand for aibased mental health monitoring tools their practical utility for clinicians is limited by the lack of interpretabilitythe clpsych 2024 shared task chim et al 2024 aims to enhance the interpretability of large language models llms particularly in mental health analysis by providing evidence of suicidality through linguistic content we propose a dualprompting approach i knowledgeaware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental healthspecific llm and ii evidence summarization by employing an llmbased consistency evaluator comprehensive experiments demonstrate the effectiveness of combining domainspecific information revealing performance improvements and the approachs potential to aid clinicians in assessing mental state progression,http://arxiv.org/abs/2402.14854v1,1
this paper addresses the quality of annotations in mental health datasets used for nlpbased depression level estimation from social media texts while previous research relies on social mediabased datasets annotated with binary categories ie depressed or nondepressed recent datasets such as d2s and primate aim for nuanced annotations using phq9 symptoms however most of these datasets rely on crowd workers without the domain knowledge for annotation focusing on the primate dataset our study reveals concerns regarding annotation validity particularly for the lack of interest or pleasure symptom through reannotation by a mental health professional we introduce finer labels and textual spans as evidence identifying a notable number of false positives our refined annotations to be released under a data use agreement offer a higherquality test set for anhedonia detection this study underscores the necessity of addressing annotation quality issues in mental health datasets advocating for improved methodologies to enhance nlp model reliability in mental health assessments,http://arxiv.org/abs/2403.00438v1,1
the intersections of mental health and computing education is underexamined in this systematic literature review we evaluate the stateoftheart of research in mental health and wellbeing interventions assessments and concerns like anxiety and depression in computer science and computing education the studies evaluated occurred across the computing education pipeline from introductory to phd courses and found some commonalities contributing to high reporting of anxiety and depression in those studied in addition interventions that were designed to address mental health topics often revolved around selfguidance based on our review of the literature we recommend increasing sample sizes and focusing on the design and development of tools and interventions specifically designed for computing professionals and students,http://arxiv.org/abs/2405.03416v1,1
the limited availability of psychologists necessitates efficient identification of individuals requiring urgent mental healthcare this study explores the use of natural language processing nlp pipelines to analyze text data from online mental health forums used for consultations by analyzing forum posts these pipelines can flag users who may require immediate professional attention a crucial challenge in this domain is data privacy and scarcity to address this we propose utilizing readily available curricular texts used in institutes specializing in mental health for pretraining the nlp pipelines this helps us mimic the training process of a psychologist our work presents casebert that flags potential mental health disorders based on forum text casebert demonstrates superior performance compared to existing methods achieving an f1 score of 091 for depression and 088 for anxiety two of the most commonly reported mental health disorders our code and data are publicly available,http://arxiv.org/abs/2406.00314v3,1
the advancement of large language models llms has demonstrated strong capabilities across various applications including mental health analysis however existing studies have focused on predictive performance leaving the critical issue of fairness underexplored posing significant risks to vulnerable populations despite acknowledging potential biases previous works have lacked thorough investigations into these biases and their impacts to address this gap we systematically evaluate biases across seven social factors eg gender age religion using ten llms with different prompting methods on eight diverse mental health datasets our results show that gpt4 achieves the best overall balance in performance and fairness among llms although it still lags behind domainspecific models like mentalroberta in some cases additionally our tailored fairnessaware prompts can effectively mitigate bias in mental health predictions highlighting the great potential for fair analysis in this field,http://arxiv.org/abs/2406.12033v2,1
as we build towards developing interactive systems that can recognize human emotional states and respond to individual needs more intuitively and empathetically in more personalized and contextaware computing time this is especially important regarding mental health support with a rising need for immediate nonintrusive help tailored to each individual individual mental health and the complex nature of human emotions call for novel approaches beyond conventional proactive and reactivebased chatbot approaches in this position paper we will explore how to create chatbots that can sense interpret and intervene in emotional signals by combining realtime facial expression analysis physiological signal interpretation and language models this is achieved by incorporating facial affect detection into existing practical and ubiquitous passive sensing contexts thus empowering them with the capabilities to the ubiquity of sensing behavioral primitives to recognize interpret and respond to human emotions in parallel the system employs cognitivebehavioral therapy tools such as cognitive reframing and mood journals leveraging the therapeutic intervention potential of chatbots in mental health contexts finally we propose a project to build a system that enhances the emotional understanding of chatbots to engage users in chatbased intervention thereby helping manage their mood,http://arxiv.org/abs/2406.15942v1,1
we aim to develop a tool for understanding how the mental health of youth aged less than 18 years evolve over time through administrative records of mental health related emergency department mhed visits in two decades administrative health data usually contain rich information for investigating public health issues however many restrictions and regulations apply to their use moreover the data are usually not in a conventional format since administrative databases are created and maintained to serve nonresearch purposes and only information for people who seek health services is accessible analysis of administrative health data is thus challenging in general in the mhed data analyses we are particularly concerned with i evaluating dynamic patterns and impacts with doublycensored recurrent event data and ii recalibrating estimators developed based on truncated data by leveraging summary statistics from the population the findings are verified empirically via simulation we have established the asymptotic properties of the inference procedures the contributions of this paper are twofold we present innovative strategies for processing doublycensored recurrent event data and overcoming the truncation induced by the data collection in addition through exploring the pediatric mhed visit records we provide new insights into childrenyouths mental health changes over time,http://arxiv.org/abs/2407.09761v1,1
this study introduces psycho analyst a custom gpt model based on openais gpt4 optimized for prescreening mental health disorders enhanced with dsm5 phq8 detailed data descriptions and extensive training data the model adeptly decodes nuanced linguistic indicators of mental health disorders it utilizes a dualtask framework that includes binary classification and a threestage phq8 score computation involving initial assessment detailed breakdown and independent assessment showcasing refined analytic capabilities validation with the daicwoz dataset reveals f1 and macrof1 scores of 0929 and 0949 respectively along with the lowest mae and rmse of 289 and 369 in phq8 scoring these results highlight the models precision and transformative potential in enhancing public mental health support improving accessibility costeffectiveness and serving as a second opinion for professionals,http://arxiv.org/abs/2408.01614v2,1
integrating physiological signals such as electroencephalogram eeg with other data such as interview audio may offer valuable multimodal insights into psychological states or neurological disorders recent advancements with large language models llms position them as prospective health agents for mental health assessment however current research predominantly focus on single data modalities presenting an opportunity to advance understanding through multimodal data our study aims to advance this approach by investigating multimodal data using llms for mental health assessment specifically through zeroshot and fewshot prompting three datasets are adopted for depression and emotion classifications incorporating eeg facial expressions and audio text the results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment notably integrating eeg alongside commonly used llm modalities such as audio and images demonstrates promising potential moreover our findings reveal that 1shot learning offers greater benefits compared to zeroshot learning methods,http://arxiv.org/abs/2408.07313v1,1
in this study we introduce angst a novel firstofits kind benchmark for depressionanxiety comorbidity classification from social media posts unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions angst enables multilabel classification allowing each post to be simultaneously identified as indicating depression andor anxiety comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silverlabeled posts angst posits a more representative sample of online mental health discourse moreover we benchmark angst using various stateoftheart language models ranging from mentalbert to gpt4 our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios while gpt4 generally outperforms other models none achieve an f1 score exceeding 72 in multiclass comorbid classification underscoring the ongoing challenges in applying language models to mental health diagnostics,http://arxiv.org/abs/2410.03908v1,1
in domainspecific contexts particularly mental health abstractive summarization requires advanced techniques adept at handling specialized content to generate domainrelevant and faithful summaries in response to this we introduce a guided summarizer equipped with a dualencoder and an adapted decoder that utilizes novel domainspecific guidance signals ie mental health terminologies and contextually rich sentences from the source document to enhance its capacity to align closely with the content and context of guidance thereby generating a domainrelevant summary additionally we present a postediting correction model to rectify errors in the generated summary thus enhancing its consistency with the original content in detail evaluation on the mentsum dataset reveals that our model outperforms existing baseline models in terms of both rouge and factcc scores although the experiments are specifically designed for mental health posts the methodology weve developed offers broad applicability highlighting its versatility and effectiveness in producing highquality domainspecific summaries,http://arxiv.org/abs/2411.01485v1,1
the intersection of technology and mental health has spurred innovative approaches to assessing emotional wellbeing particularly through computational techniques applied to audio data analysis this study explores the application of convolutional neural network cnn and long shortterm memory lstm models on wavelet extracted features and melfrequency cepstral coefficients mfccs for emotion detection from spoken speech data augmentation techniques feature extraction normalization and model training were conducted to evaluate the models performance in classifying emotional states results indicate that the cnn model achieved a higher accuracy of 61 compared to the lstm models accuracy of 56 both models demonstrated better performance in predicting specific emotions such as surprise and anger leveraging distinct audio features like pitch and speed variations recommendations include further exploration of advanced data augmentation techniques combined feature extraction methods and the integration of linguistic analysis with speech characteristics for improved accuracy in mental health diagnostics collaboration for standardized dataset collection and sharing is recommended to foster advancements in affective computing and mental health care interventions,http://arxiv.org/abs/2412.10469v1,1
the increasing demand for mental health services has highlighted the need for innovative solutions particularly in the realm of psychological conversational ai where the availability of sensitive data is scarce in this work we explored the development of a system tailored for mental health support with a novel approach to psychological assessment based on explainable emotional profiles in combination with empathetic conversational models offering a promising tool for augmenting traditional care particularly where immediate expertise is unavailable our work can be divided into two main parts intrinsecaly connected to each other first we present raclette a conversational system that demonstrates superior emotional accuracy compared to stateoftheart benchmarks in both understanding users emotional states and generating empathetic responses during conversations while progressively building an emotional profile of the user through their interactions second we show how the emotional profiles of a user can be used as interpretable markers for mental health assessment these profiles can be compared with characteristic emotional patterns associated with different mental disorders providing a novel approach to preliminary screening and support,http://arxiv.org/abs/2412.20068v1,1
advances in large language models llms have empowered a variety of applications however there is still a significant gap in research when it comes to understanding and enhancing the capabilities of llms in the field of mental health in this work we present a comprehensive evaluation of multiple llms on various mental health prediction tasks via online text data including alpaca alpacalora flant5 gpt35 and gpt4 we conduct a broad range of experiments covering zeroshot prompting fewshot prompting and instruction finetuning the results indicate a promising yet limited performance of llms with zeroshot and fewshot prompt designs for mental health tasks more importantly our experiments show that instruction finetuning can significantly boost the performance of llms for all tasks simultaneously our bestfinetuned models mentalalpaca and mentalflant5 outperform the best prompt design of gpt35 25 and 15 times bigger by 109 on balanced accuracy and the best of gpt4 250 and 150 times bigger by 48 they further perform on par with the stateoftheart taskspecific language model we also conduct an exploratory case study on llms capability on mental health reasoning tasks illustrating the promising capability of certain models such as gpt4 we summarize our findings into a set of action guidelines for potential methods to enhance llms capability for mental health tasks meanwhile we also emphasize the important limitations before achieving deployability in realworld mental health settings such as known racial and gender bias we highlight the important ethical risks accompanying this line of research,http://arxiv.org/abs/2307.14385v4,1
with the development of web technology social media texts are becoming a rich source for automatic mental health analysis as traditional discriminative methods bear the problem of low interpretability the recent large language models have been explored for interpretable mental health analysis on social media which aims to provide detailed explanations along with predictions the results show that chatgpt can generate approachinghuman explanations for its correct classifications however llms still achieve unsatisfactory classification performance in a zeroshotfewshot manner domainspecific finetuning is an effective solution but faces 2 challenges 1 lack of highquality training data 2 no opensource llms for interpretable mental health analysis were released to lower the finetuning cost to alleviate these problems we build the first multitask and multisource interpretable mental health instruction imhi dataset on social media with 105k data samples the raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks we use expertwritten fewshot prompts and collected labels to prompt chatgpt and obtain explanations from its responses to ensure the reliability of the explanations we perform strict automatic and human evaluations on the correctness consistency and quality of generated data based on the imhi dataset and llama2 foundation models we train mentalllama the first opensource llm series for interpretable mental health analysis with instructionfollowing capability we also evaluate the performance of mentalllama on the imhi evaluation benchmark with 10 test sets where their correctness for making predictions and the quality of explanations are examined the results show that mentalllama approaches stateoftheart discriminative methods in correctness and generates highquality explanations,http://arxiv.org/abs/2309.13567v3,1
the integration of large language models llms in mental health care is an emerging field there is a need to systematically review the application outcomes and delineate the advantages and limitations in clinical settings this review aims to provide a comprehensive overview of the use of llms in mental health care assessing their efficacy challenges and potential for future applications a systematic search was conducted across multiple databases including pubmed web of science google scholar arxiv medrxiv and psyarxiv in november 2023 all forms of original research peerreviewed or not published or disseminated between october 1 2019 and december 2 2023 are included without language restrictions if they used llms developed after t5 and directly addressed research questions in mental health care settings from an initial pool of 313 articles 34 met the inclusion criteria based on their relevance to llm application in mental health care and the robustness of reported outcomes diverse applications of llms in mental health care are identified including diagnosis therapy patient engagement enhancement etc key challenges include data availability and reliability nuanced handling of mental states and effective evaluation methods despite successes in accuracy and accessibility improvement gaps in clinical applicability and ethical considerations were evident pointing to the need for robust data standardized evaluations and interdisciplinary collaboration llms hold substantial promise for enhancing mental health care for their full potential to be realized emphasis must be placed on developing robust datasets development and evaluation frameworks ethical guidelines and interdisciplinary collaborations to address current limitations,http://arxiv.org/abs/2401.02984v2,1
mental health conditions cause a great deal of distress or impairment depression alone will affect 11 of the worlds population the application of artificial intelligence ai and bigdata technologies to mental health has great potential for personalizing treatment selection prognosticating monitoring for relapse detecting and helping to prevent mental health conditions before they reach clinicallevel symptomatology and even delivering some treatments however unlike similar applications in other fields of medicine there are several unique challenges in mental health applications which currently pose barriers towards the implementation of these technologies specifically there are very few widely used or validated biomarkers in mental health leading to a heavy reliance on patient and clinician derived questionnaire data as well as interpretation of new signals such as digital phenotyping in addition diagnosis also lacks the same objective gold standard as in other conditions such as oncology where clinicians and researchers can often rely on pathological analysis for confirmation of diagnosis in this chapter we discuss the major opportunities limitations and techniques used for improving mental healthcare through ai and bigdata we explore both the computational clinical and ethical considerations and best practices as well as lay out the major researcher directions for the near future,http://arxiv.org/abs/1903.12071v1,1
an increasing number of mental health services are offered through mobile systems a paradigm called mhealth although there is an unprecedented growth in the adoption of mhealth systems partly due to the covid19 pandemic concerns about data privacy risks due to security breaches are also increasing whilst some studies have analyzed mhealth apps from different angles including security there is relatively little evidence for data privacy issues that may exist in mhealth apps used for mental health services whose recipients can be particularly vulnerable this paper reports an empirical study aimed at systematically identifying and understanding data privacy incorporated in mental health apps we analyzed 27 topranked mental health apps from google play store our methodology enabled us to perform an indepth privacy analysis of the apps covering static and dynamic analysis data sharing behaviour serverside tests privacy impact assessment requests and privacy policy evaluation furthermore we mapped the findings to the linddun threat taxonomy describing how threats manifest on the studied apps the findings reveal important data privacy issues such as unnecessary permissions insecure cryptography implementations and leaks of personal data and credentials in logs and web requests there is also a high risk of user profiling as the apps development do not provide foolproof mechanisms against linkability detectability and identifiability data sharing among third parties and advertisers in the current apps ecosystem aggravates this situation based on the empirical findings of this study we provide recommendations to be considered by different stakeholders of mhealth apps in general and apps developers in particular,http://arxiv.org/abs/2201.09006v1,1
introduction to improve current public health strategies in suicide prevention and mental health governments researchers and private companies increasingly use information and communication technologies and more specifically artificial intelligence and big data these technologies are promising but raise ethical challenges rarely covered by current legal systems it is essential to better identify and prevent potential ethical risks objectives the canada protocol mhsp is a tool to guide and support professionals users and researchers using ai in mental health and suicide prevention methods a checklist was constructed based upon ten international reports on ai and ethics and two guides on mental health and new technologies 329 recommendations were identified of which 43 were considered as applicable to mental health and ai the checklist was validated using a two round delphi consultation results 16 experts participated in the first round of the delphi consultation and 8 participated in the second round of the original 43 items 38 were retained they concern five categories description of the autonomous intelligent system n8 privacy and transparency n8 security n6 healthrelated risks n8 biases n8 the checklist was considered relevant by most users and could need versions tailored to each category of target users,http://arxiv.org/abs/1907.07493v1,1
covid19 pandemic has adversely and disproportionately impacted people suffering from mental health issues and substance use problems this has been exacerbated by social isolation during the pandemic and the social stigma associated with mental health and substance use disorders making people reluctant to share their struggles and seek help due to the anonymity and privacy they provide social media emerged as a convenient medium for people to share their experiences about their day to day struggles reddit is a wellrecognized social media platform that provides focused and structured forums called subreddits that users subscribe to and discuss their experiences with others temporal assessment of the topical correlation between social media postings about mental healthsubstance use and postings about coronavirus is crucial to better understand public sentiment on the pandemic and its evolving impact especially related to vulnerable populations in this study we conduct a longitudinal topical analysis of postings between subreddits rdepression ranxiety rsuicidewatch and rcoronavirus and postings between subreddits ropiates ropiatesrecovery raddiction and rcoronavirus from january 2020 october 2020 our results show a high topical correlation between postings in rdepression and rcoronavirus in september 2020 further the topical correlation between postings on substance use disorders and coronavirus fluctuates showing the highest correlation in august 2020 by monitoring these trends from platforms such as reddit epidemiologists and mental health professionals can gain insights into the challenges faced by communities for targeted interventions,http://arxiv.org/abs/2011.10518v1,1
in this paper we present novel research methods for collecting and analyzing personal financial data alongside mental health factors illustrated through a n1 case study using data from one individual with bipolar disorder while we have not found statistically significant trends nor our findings are generalizable beyond this case our approach provides an insight into the challenges of accessing objective financial data we outline what data is currently available what can be done with it and what factors to consider when working with financial data more specifically using these methods researchers might be able to identify symptomatic traces of mental ill health in personal financial data such as identifying early warning signs and thereby enable preemptive care for individuals with serious mental illnesses based on this work we have also explored future directions for developing interventions to support financial wellbeing furthermore we have described the technical ethical and equity challenges for financial datadriven assessments and intervention methods as well as provided a broad research agenda to address these challenges by leveraging objective personalized financial data in a privacypreserving and ethical manner help lead to a shift in mental health care,http://arxiv.org/abs/2204.05448v4,1
people often utilise online media eg facebook reddit as a platform to express their psychological distress and seek support stateoftheart nlp techniques demonstrate strong potential to automatically detect mental health issues from text research suggests that mental health issues are reflected in emotions eg sadness indicated in a persons choice of language therefore we developed a novel emotionannotated mental health corpus emoment consisting of 2802 facebook posts 14845 sentences extracted from two south asian countries sri lanka and india three clinical psychology postgraduates were involved in annotating these posts into eight categories including mental illness eg depression and emotions eg sadness anger emoment corpus achieved very good interannotator agreement of 983 ie with two or more agreement and fleiss kappa of 082 our roberta based models achieved an f1 score of 076 and a macroaveraged f1 score of 077 for the first task ie predicting a mental health condition from a post and the second task ie extent of association of relevant posts with the categories defined in our taxonomy respectively,http://arxiv.org/abs/2208.08486v1,1
dialogue safety remains a pervasive challenge in opendomain humanmachine interaction existing approaches propose distinctive dialogue safety taxonomies and datasets for detecting explicitly harmful responses however these taxonomies may not be suitable for analyzing response safety in mental health support in realworld interactions a model response deemed acceptable in casual conversations might have a negligible positive impact on users seeking mental health support to address these limitations this paper aims to develop a theoretically and factually grounded taxonomy that prioritizes the positive impact on helpseekers additionally we create a benchmark corpus with finegrained labels for each dialogue session to facilitate further research we analyze the dataset using popular language models including bertbase robertalarge and chatgpt to detect and understand unsafe responses within the context of mental health support our study reveals that chatgpt struggles to detect safety categories with detailed safety definitions in a zero and fewshot paradigm whereas the finetuned model proves to be more suitable the developed dataset and findings serve as valuable benchmarks for advancing research on dialogue safety in mental health support with significant implications for improving the design and deployment of conversation agents in realworld applications we release our code and data here,http://arxiv.org/abs/2307.16457v1,1
online mental health communities omhcs such as reddit have witnessed a surge in popularity as goto platforms for seeking information and support in managing mental health needs platforms like reddit offer immediate interactions with peers granting users a vital space for seeking mental health assistance however the largely unregulated nature of these platforms introduces intricate challenges for both users and society at large this study explores the factors that drive peer engagement within counseling threads aiming to enhance our understanding of this critical phenomenon we introduce becope a novel behavior encoded peer counseling dataset comprising over 10118 posts and 58279 comments sourced from 21 mental healthspecific subreddits the dataset is annotated using three major finegrained behavior labels a intent b criticism and c readability along with the emotion labels our analysis indicates the prominence of selfcriticism as the most prevalent form of criticism expressed by helpseekers accounting for a significant 43 of interactions intriguingly we observe that individuals who explicitly express their need for help are 1801 more likely to receive assistance compared to those who present surveys or engage in rants furthermore we highlight the pivotal role of wellarticulated problem descriptions showing that superior readability effectively doubles the likelihood of receiving the soughtafter support our study emphasizes the essential role of omhcs in offering personalized guidance and unveils behaviordriven engagement patterns,http://arxiv.org/abs/2309.01618v1,1
research in psychopathology has shown that at an aggregate level the patterns of emotional change over time emotion dynamics are indicators of ones mental health ones patterns of emotion change have traditionally been determined through selfreports of emotions however there are known issues with accuracy bias and ease of data collection recent approaches to determining emotion dynamics from ones everyday utterances addresses many of these concerns but it is not yet known whether these measures of utterance emotion dynamics ued correlate with mental health diagnoses here for the first time we study the relationship between tweet emotion dynamics and mental health disorders we find that each of the ued metrics studied varied by the users selfdisclosed diagnosis for example average valence was significantly higher ie more positive text in the control group compared to users with adhd mdd and ptsd valence variability was significantly lower in the control group compared to adhd depression bipolar disorder mdd ptsd and ocd but not ppd rise and recovery rates of valence also exhibited significant differences from the control this work provides important early evidence for how linguistic cues pertaining to emotion dynamics can play a crucial role as biosocial markers for mental illnesses and aid in the understanding diagnosis and management of mental health disorders,http://arxiv.org/abs/2310.17369v2,1
digital mental health dmh interventions such as textmessagebased lessons and activities offer immense potential for accessible mental health support while these interventions can be effective realworld experimental testing can further enhance their design and impact adaptive experimentation utilizing algorithms like thompson sampling for contextual multiarmed bandit mab problems can lead to continuous improvement and personalization however it remains unclear when these algorithms can simultaneously increase user experience rewards and facilitate appropriate data collection for socialbehavioral scientists to analyze with sufficient statistical confidence although a growing body of research addresses the practical and statistical aspects of mab and other adaptive algorithms further exploration is needed to assess their impact across diverse realworld contexts this paper presents a software system developed over two years that allows textmessaging intervention components to be adapted using bandit and other algorithms while collecting data for sidebyside comparison with traditional uniform random nonadaptive experiments we evaluate the system by deploying a textmessagebased dmh intervention to 1100 users recruited through a large mental health nonprofit organization and share the path forward for deploying this system at scale this system not only enables applications in mental health but could also serve as a model testbed for adaptive experimentation algorithms in other domains,http://arxiv.org/abs/2310.18326v1,1
amid growing global mental health concerns particularly among vulnerable groups natural language processing offers a tremendous potential for early detection and intervention of peoples mental disorders via analyzing their postings and discussions on social media platforms however ultrasparse training data often due to vast vocabularies and lowfrequency words hinders the analysis accuracy multilabeling and cooccurrences of symptoms may also blur the boundaries in distinguishing similarcorelated disorders to address these issues we propose a novel semantic feature preprocessing technique with a threefolded structure 1 mitigating the feature sparsity with a weak classifier 2 adaptive feature dimension with modulus loops and 3 deepmining and extending features among the contexts with enhanced semantic features we train a machine learning model to predict and classify mental disorders we utilize the reddit mental health dataset 2022 to examine conditions such as anxiety borderline personality disorder bpd and bipolardisorder bd and present solutions to the data sparsity challenge highlighted by 9981 nonzero elements after applying our preprocessing technique the feature sparsity decreases to 854 overall our methods when compared to seven benchmark models demonstrate significant performance improvements 80 in accuracy 0069 in precision 0093 in recall 0102 in f1 score and 0059 in auc this research provides foundational insights for mental health prediction and monitoring providing innovative solutions to navigate challenges associated with ultrasparse data feature and intricate multilabel classification in the domain of mental health analysis,http://arxiv.org/abs/2311.05075v1,1
employee wellbeing is a critical concern in the contemporary workplace as highlighted by the american psychological associations 2021 report indicating that 71 of employees experience stress or tension this stress contributes significantly to workplace attrition and absenteeism with 61 of attrition and 16 of sick days attributed to poor mental health a major challenge for employers is that employees often remain unaware of their mental health issues until they reach a crisis point resulting in limited utilization of corporate wellbeing benefits this research addresses this challenge by presenting a groundbreaking stress detection algorithm that provides realtime support preemptively leveraging automated chatbot technology the algorithm objectively measures mental health levels by analyzing chat conversations offering personalized treatment suggestions in realtime based on linguistic biomarkers the study explores the feasibility of integrating these innovations into practical learning applications within realworld contexts and introduces a chatbotstyle system integrated into the broader employee experience platform this platform encompassing various features aims to enhance overall employee wellbeing detect stress in real time and proactively engage with individuals to improve support effectiveness demonstrating a 22 increase when assistance is provided early overall the study emphasizes the importance of fostering a supportive workplace environment for employees mental health,http://arxiv.org/abs/2402.01592v1,1
understanding the conversation abilities of large language models llms can help lead to its more cautious and appropriate deployment this is especially important for safetycritical domains like mental health where someones life may depend on the exact wording of a response to an urgent question in this paper we propose a novel framework for evaluating the nuanced conversation abilities of llms within it we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature while we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains we apply them to the mental health field we use our framework to evaluate several popular frontier llms including some gpt and llama models through a verified mental health dataset our results show that gpt4 turbo can perform significantly more similarly to verified therapists than other selected llms we conduct additional analysis to examine how llm conversation performance varies across specific mental health topics our results indicate that gpt4 turbo performs well in achieving high correlation with verified therapists in particular topics such as parenting and relationships we believe our contributions will help researchers develop better llms that in turn will more positively support peoples lives,http://arxiv.org/abs/2403.09705v1,1
timely identification is essential for the efficient handling of mental health illnesses such as depression however the current research fails to adequately address the prediction of mental health conditions from social media data in lowresource african languages like swahili this study introduces two distinct approaches utilising modelagnostic metalearning and leveraging large language models llms to address this gap experiments are conducted on three datasets translated to lowresource language and applied to four mental health tasks which include stress depression depression severity and suicidal ideation prediction we first apply a metalearning model with selfsupervision which results in improved model initialisation for rapid adaptation and crosslingual transfer the results show that our metatrained model performs significantly better than standard finetuning methods outperforming the baseline finetuning in macro f1 score with 18 and 08 over xlmr and mbert in parallel we use llms incontext learning capabilities to assess their performance accuracy across the swahili mental health prediction tasks by analysing different crosslingual prompting approaches our analysis showed that swahili prompts performed better than crosslingual prompts but less than english prompts our findings show that incontext learning can be achieved through crosslingual transfer through carefully crafted prompt templates with examples and instructions,http://arxiv.org/abs/2404.09045v1,1
large language models llms are already being piloted for clinical use in hospital systems like nyu langone danafarber and the nhs a proposed deployment use case is psychotherapy where a llmpowered chatbot can treat a patient undergoing a mental health crisis deployment of llms for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care however recent highprofile failures like damaging dieting advice offered by the tessa chatbot to patients with eating disorders have led to doubt about their reliability in highstakes and safetycritical settings in this work we develop an evaluation framework for determining whether llm response is a viable and ethical path forward for the automation of mental health treatment our framework measures equity in empathy and adherence of llm responses to motivational interviewing theory using human evaluation with trained clinicians and automatic qualityofcare metrics grounded in psychology research we compare the responses provided by peertopeer responders to those provided by a stateoftheart llm we show that llms like gpt4 use implicit and explicit cues to infer patient demographics like race we then show that there are statistically significant discrepancies between patient subgroups responses to black posters consistently have lower empathy than for any other demographic group 213 lower than the control group promisingly we do find that the manner in which responses are generated significantly impacts the quality of the response we conclude by proposing safety guidelines for the potential deployment of llms for mental health response,http://arxiv.org/abs/2405.12021v2,1
large language models llms are raging over the medical domain and their momentum has carried over into the mental health domain leading to the emergence of few mental health llms although such mental health llms could provide reasonable suggestions for psychological counseling how to develop an authentic and effective doctorpatient relationship dpr through llms is still an important problem to fill this gap we dissect dpr into two key attributes ie the psychologists empathy and proactive guidance we thus present wundtgpt an empathetic and proactive mental health large language model that is acquired by finetuning it with instruction and real conversation between psychologists and patients it is designed to assist psychologists in diagnosis and help patients who are reluctant to communicate facetoface understand their psychological conditions its uniqueness lies in that it could not only pose purposeful questions to guide patients in detailing their symptoms but also offer warm emotional reassurance in particular wundtgpt incorporates collection of questions chain of psychodiagnosis and empathy constraints into a comprehensive prompt for eliciting llms questions and diagnoses additionally wundtgpt proposes a reward model to promote alignment with empathetic mental health professionals which encompasses two key factors cognitive empathy and emotional empathy we offer a comprehensive evaluation of our proposed model based on these outcomes we further conduct the manual evaluation based on proactivity effectiveness professionalism and coherence we notice that wundtgpt can offer professional and effective consultation the model is available at huggingface,http://arxiv.org/abs/2406.15474v1,1
over one in five adults in the us lives with a mental illness in the face of a shortage of mental health professionals and offline resources online shortform video content has grown to serve as a crucial conduit for disseminating mental health help and resources however the ease of content creation and access also contributes to the spread of misinformation posing risks to accurate diagnosis and treatment detecting and understanding engagement with such content is crucial to mitigating their harmful effects on public health we perform the first quantitative study of the phenomenon using youtube shorts and bitchute as the sites of study we contribute mentalmisinfo a novel labeled mental health misinformation mhmisinfo dataset of 739 videos 639 from youtube and 100 from bitchute and 135372 comments in total using an expertdriven annotation schema we first found that fewshot incontext learning with large language models llms are effective in detecting mhmisinfo videos next we discover distinct and potentially alarming linguistic patterns in how audiences engage with mhmisinfo videos through commentary on both videosharing platforms across the two platforms comments could exacerbate prevailing stigma with some groups showing heightened susceptibility to and alignment with mhmisinfo we discuss technical and public healthdriven adaptive solutions to tackling the epidemic of mental health misinformation online,http://arxiv.org/abs/2407.02662v1,1
the significance of mental health classification is paramount in contemporary society where digital platforms serve as crucial sources for monitoring individuals wellbeing however existing social media mental health datasets primarily consist of textonly samples potentially limiting the efficacy of models trained on such data recognising that humans utilise crossmodal information to comprehend complex situations or issues we present a novel approach to address the limitations of current methodologies in this work we introduce a multimodal and multiteacher knowledge distillation model for mental health classification leveraging insights from crossmodal human understanding unlike conventional approaches that often rely on simple concatenation to integrate diverse features our model addresses the challenge of appropriately representing inputs of varying natures eg texts and sounds to mitigate the computational complexity associated with integrating all features into a single model we employ a multimodal and multiteacher architecture by distributing the learning process across multiple teachers each specialising in a particular feature extraction aspect we enhance the overall mental health classification performance through experimental validation we demonstrate the efficacy of our model in achieving improved performance,http://arxiv.org/abs/2407.09020v3,1
mental health disorders are among the most prevalent diseases worldwide affecting nearly one in four people despite their widespread impact the intervention rate remains below 25 largely due to the significant cooperation required from patients for both diagnosis and intervention the core issue behind this low treatment rate is stigma which discourages over half of those affected from seeking help this paper presents mindguard an accessible stigmafree and professional mobile mental healthcare system designed to provide mental health first aid the heart of mindguard is an innovative edge llm equipped with professional mental health knowledge that seamlessly integrates objective mobile sensor data with subjective ecological momentary assessment records to deliver personalized screening and intervention conversations we conduct a broad evaluation of mindguard using open datasets spanning four years and realworld deployment across various mobile devices involving 20 subjects for two weeks remarkably mindguard achieves results comparable to gpt4 and outperforms its counterpart with more than 10 times the model size we believe that mindguard paves the way for mobile llm applications potentially revolutionizing mental healthcare practices by substituting selfreporting and intervention conversations with passive integrated monitoring within daily life thus ensuring accessible and stigmafree mental health support,http://arxiv.org/abs/2409.10064v1,1
computational approaches to predicting mental health conditions in social media have been substantially explored in the past years multiple surveys have been published on this topic providing the community with comprehensive accounts of the research in this area among all mental health conditions depression is the most widely studied due to its worldwide prevalence the covid19 global pandemic starting in early 2020 has had a great impact on mental health worldwide harsh measures employed by governments to slow the spread of the virus eg lockdowns and the subsequent economic downturn experienced in many countries have significantly impacted peoples lives and mental health studies have shown a substantial increase of above 50 in the rate of depression in the population in this context we present a survey on natural language processing nlp approaches to modeling depression in social media providing the reader with a postcovid19 outlook this survey contributes to the understanding of the impacts of the pandemic on modeling depression in social media we outline how stateoftheart approaches and new datasets have been used in the context of the covid19 pandemic finally we also discuss ethical issues in collecting and processing mental health data considering fairness accountability and ethics,http://arxiv.org/abs/2410.08793v1,1
neurofeedback is a noninvasive brain training with longterm medical and nonmedical applications despite the existence of several emotion regulation studies using neurofeedback further investigation is needed to understand interactions of the brain regions involved in the process we implemented eeg neurofeedback with simultaneous fmri using a modified happinessinducing task through autobiographical memories to upregulate positive emotion the results showed increased activity of prefrontal occipital parietal and limbic regions and increased functional connectivity between prefrontal parietal limbic system and insula in the experimental group new connectivity links were identified by comparing the functional connectivity of different experimental conditions within the experimental group and between the experimental and control groups the proposed multimodal approach quantified the changes in the brain activity up to 19 increase and connectivity fdrcorrected for multiple comparison q 005 during emotion regulation inbetween prefrontal parietal limbic and insula regions psychometric assessments confirmed significant changes in positive and negative mood states by neurofeedback with a pvalue smaller than 0002 in the experimental group this study quantifies the effects of eeg neurofeedback in changing functional connectivity of all brain regions involved in emotion regulation for the brain regions involved in emotion regulation we found significant bold and functional connectivity increases due to neurofeedback in the experimental group but no learning effect was observed in the control group the results reveal the neurobiological substrate of emotion regulation by the eeg neurofeedback and separate the effect of the neurofeedback and the recall of the autobiographical memories,http://arxiv.org/abs/2006.06829v1,1
delivering treatment recommendations via pervasive electronic devices such as mobile phones has the potential to be a viable and scalable treatment medium for longterm health behavior management but active experimentation of treatment options can be timeconsuming expensive and altogether unethical in some cases there is a growing interest in methodological approaches that allow an experimenter to learn and evaluate the usefulness of a new treatment strategy before deployment we present the first development of a treatment recommender system for emotion regulation using realworld historical mobile digital data from n 114 high socially anxious participants to test the usefulness of new emotion regulation strategies we explore a number of offline contextual bandits estimators for learning and propose a general framework for learning algorithms our experimentation shows that the proposed doubly robust offline learning algorithms performed significantly better than baseline approaches suggesting that this type of recommender algorithm could improve emotion regulation given that emotion regulation is impaired across many mental illnesses and such a recommender algorithm could be scaled up easily this approach holds potential to increase access to treatment for many people we also share some insights that allow us to translate contextual bandit models to this complex realworld data including which contextual features appear to be most important for predicting emotion regulation strategy effectiveness,http://arxiv.org/abs/2008.09472v1,1
background mobile phone sensor technology has great potential in providing behavioral markers of mental health however this promise has not yet been brought to fruition objective the objective of our study was to examine challenges involved in developing an app to extract behavioral markers of mental health from passive sensor data methods both technical challenges and acceptability of passive data collection for mental health research were assessed based on literature review and results obtained from a feasibility study socialise a mobile phone app developed at the black dog institute was used to collect sensor data bluetooth global positioning system and battery status and investigate views and experiences of a group of people with lived experience of mental health challenges n32 results on average sensor data were obtained for 55 android and 45 iphone os of scheduled scans battery life was reduced from 213 hours to 188 hours when scanning every 5 minutes with a reduction of 25 hours or 12 despite this relatively small reduction most participants reported that the app had a noticeable effect on their battery life in addition to battery life the purpose of data collection trust in the organization that collects data and perceived impact on privacy were identified as main factors for acceptability conclusions based on the findings of the feasibility study and literature review we recommend a commitment to open science and transparent reporting and stronger partnerships and communication with users sensing technology has the potential to greatly enhance the delivery and impact of mental health care realizing this requires all aspects of mobile phone sensor technology to be rigorously assessed,http://arxiv.org/abs/1805.09158v2,1
purpose we present an approach for forecasting mental health conditions and emotions of a given population during the covid19 pandemic in argentina based on language expressions used in social media this approach permits anticipating high prevalence periods in short to mediumterm time horizons design mental health conditions and emotions are captured via markers which link social media contents with lexicons first we build descriptive timelines for decision makers to monitor the evolution of markers and their correlation with crisis events second we model the timelines as time series and support their forecasting which in turn serve to identify high prevalence points for the estimated markers findings results showed that different time series forecasting strategies offer different capabilities in the best scenario the emergence of high prevalence periods of emotions and mental health disorders can be satisfactorily predicted with a neural network strategy even when limited data is available in early stages of a crisis eg 7 days originality although there have been efforts in the literature to predict mental states of individuals the analysis of mental health at the collective level has received scarce attention we take a step forward by proposing a forecasting approach for analyzing the mental health of a given population or group of individuals at a larger scale practical implications we believe that this work contributes to a better understanding of how psychological processes related to crisis manifest in social media being a valuable asset for the design implementation and monitoring of health prevention and communication policies,http://arxiv.org/abs/2101.04540v4,1
objective this study aims to identify the social determinants of mental health among undergraduate students in bangladesh a developing nation in south asia our goal is to identify the broader social determinants of mental health among this population study the manifestation of these determinants in their daytoday life and explore the feasibility of selfmonitoring tools in helping them identify the specific factors or relationships that impact their mental health methods we conducted a 21day study with 38 undergraduate students from seven universities in bangladesh we conducted two semistructured interviews one prestudy and one poststudy during the 21day study participants used an android application to selfreport and selfmonitor their mood after each phone conversation the app prompted participants to report their mood after each phone conversation and provided graphs and charts so that participants could independently review their mood and conversation patterns results our results show that academics family job and economic condition romantic relationships and religion are the major social determinants of mental health among undergraduate students in bangladesh our app helped the participants pinpoint the specific issues related to these factors as participants could review the pattern of their moods and emotions from past conversation history although our app does not provide any explicit recommendation participants took certain steps on their own to improve their mental health eg reduced the frequency of communication with certain persons conclusions overall the findings from this study would provide better insights for the researchers to design better solutions to help the younger population from this part of the world,http://arxiv.org/abs/2109.02838v1,1
traditionally the regime of mental healthcare has followed an episodic psychotherapy model wherein patients seek care from a provider through a prescribed treatment plan developed over multiple provider visits recent advances in wearable and mobile technology have generated increased interest in digital mental healthcare that enables individuals to address episodic mental health symptoms however these efforts are typically reactive and symptomfocused and do not provide comprehensive wraparound customized treatments that capture an individuals holistic mental health model as it unfolds over time recognizing that each individual is unique we present the notion of personalized mental health navigation mhn a therapistintheloop cybernetic goalbased system that deploys a continuous cyclic loop of measurement estimation guidance to steer the individuals mental health state towards a healthy zone we outline the major components of mhn that is premised on the development of an individuals personal mental health state holistically represented by a highdimensional cover of multiple knowledge layers such as emotion biological patterns sociology behavior and cognition we demonstrate the feasibility of the personalized mhn approach via a 12month pilot case study for holistic stress management in college students and highlight an instance of a therapistintheloop intervention using mhn for monitoring estimating and proactively addressing moderately severe depression over a sustained period of time we believe mhn paves the way to transform mental healthcare from the current passive episodic reactive process where individuals seek help to address symptoms that have already manifested to a continuous and navigational paradigm that leverages a personalized model of the individual promising to deliver timely interventions to individuals in a holistic manner,http://arxiv.org/abs/2012.09131v1,1
mental health problems impact quality of life of millions of people around the world however diagnosis of mental health disorders is a challenging problem that often relies on selfreporting by patients about their behavioral patterns therefore there is a need for new strategies for diagnosis of mental health problems the recent introduction of bodyarea networks consisting of a plethora of accurate sensors embedded in smartwatches and smartphones and deep neural networks dnns points towards a possible solution however disease diagnosis based on wmss and dnns and their deployment on edge devices remains a challenging problem to this end we propose a framework called mhdeep that utilizes commercially available wmss and efficient dnn models to diagnose three important mental health disorders schizoaffective major depressive and bipolar mhdeep uses eight different categories of data obtained from sensors integrated in a smartwatch and smartphone due to limited available data mhdeep uses a synthetic data generation module to augment real data with synthetic data drawn from the same probability distribution we use the synthetic dataset to pretrain the dnn models thus imposing a prior on the weights we use a growandprune dnn synthesis approach to learn both the architecture and weights during the training process we use three different data partitions to evaluate the mhdeep models trained with data collected from 74 individuals we conduct data instance level and patient level evaluations mhdeep achieves an average test accuracy of 904 873 and 824 respectively for classifications between healthy instances and schizoaffective disorder instances major depressive disorder instances and bipolar disorder instances at the patient level mhdeep dnns achieve an accuracy of 100 100 and 900 for the three mental health disorders respectively,http://arxiv.org/abs/2102.10435v1,1
mental health disorders are the leading cause of healthrelated problems globally it is projected that mental health disorders will be the leading cause of morbidity among adults as the incidence rates of anxiety and depression grows globally recently extended reality xr a general term covering virtual reality vr augmented reality ar and mixed reality mr is paving a new way to deliver mental health care in this paper we conduct a scoping review on the development and application of xr in the area of mental disorders we performed a scoping database search to identify the relevant studies indexed in google scholar pubmed and the acm digital library a search period between august 2016 and december 2023 was defined to select articles related to the usage of vr ar and mr in a mental health context we identified a total of 85 studies from 27 countries across the globe by performing data analysis we found that most of the studies focused on developed countries such as the us 1647 and germany 1294 none of the studies were for african countries the majority of the articles reported that xr techniques led to a significant reduction in symptoms of anxiety or depression more studies were published in the year 2021 ie 3176 n 31 this could indicate that mental disorder intervention received a higher attention when covid19 emerged most studies n 65 focused on a population between 18 and 65 years old only a few studies focused on teenagers n 2 also more studies were done experimentally n 67 7882 rather than by analytical and modeling approaches n 8 941 this shows that there is a rapid development of xr technology for mental health care furthermore these studies showed that xr technology can effectively be used for evaluating mental disorders in similar or better way as the conventional approaches,http://arxiv.org/abs/2204.01348v2,1
virtual mental health assistants vmhas have become a prevalent method for receiving mental health counseling in the digital healthcare space an assistive counseling conversation commences with natural openended topics to familiarize the client with the environment and later converges into more finegrained domainspecific topics unlike other conversational systems which are categorized as opendomain or taskoriented systems vmhas possess a hybrid conversational flow these counseling bots need to comprehend various aspects of the conversation such as dialogueacts intents etc to engage the client in an effective conversation although the surge in digital health research highlights applications of many generalpurpose response generation systems they are barely suitable in the mental health domain the prime reason is the lack of understanding in mental health counseling moreover in general dialogueact guided response generators are either limited to a templatebased paradigm or lack appropriate semantics to this end we propose reader a responseact guided reinforced dialogue generation model for the mental health counseling conversations reader is built on transformer to jointly predict a potential dialogueact dt1 for the next utterance aka responseact and to generate an appropriate response ut1 through the transformerreinforcementlearning trl with proximal policy optimization ppo we guide the response generator to abide by dt1 and ensure the semantic richness of the responses via bertscore in our reward computation we evaluate reader on hope a benchmark counseling conversation dataset and observe that it outperforms several baselines across several evaluation metrics meteor rouge and bertscore we also furnish extensive qualitative and quantitative analyses on results including error analysis human evaluation etc,http://arxiv.org/abs/2301.12729v1,1
sustainable development goals sdgs give the un a road map for development with agenda 2030 as a target sdg3 good health and wellbeing ensures healthy lives and promotes wellbeing for all ages digital technologies can support sdg3 burnout and even depression could be reduced by encouraging better preventive health due to the lack of patient knowledge and focus to take care of their health it is necessary to help patients before it is too late new trends such as positive psychology and mindfulness are highly encouraged in the usa digital twins dts can help with the continuous monitoring of emotion using physiological signals eg collected via wearables dts facilitate monitoring and provide constant health insight to improve quality of life and wellbeing with better personalization healthcare dts challenges are standardizing data formats communication protocols and data exchange mechanisms as an example iso has the isoiec jtc 1sc 41 internet of things iot and dts working group with standards such as isoiec 2182332021 iot interoperability for iot systems part 3 semantic interoperability isoiec cd 30178 iot data format value and coding to achieve those data integration and knowledge challenges we designed the mental health knowledge graph ontology and dataset to boost mental health as an example explicit knowledge is described such as chocolate contains magnesium which is recommended for depression the knowledge graph kg acquires knowledge from ontologybased mental health projects classified within the lov4iot ontology catalog emotion depression and mental health furthermore the kg is mapped to standards when possible standards from etsi smartm2m can be used such as saref4ehaw to represent medical devices and sensors but also ituwho iso w3c nist and ieee standards relevant to mental health can be considered,http://arxiv.org/abs/2406.13791v3,1
objective this study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots which are increasingly popular due to their accessibility humanlike interactions and contextaware support materials and methods we created an evaluation framework with 100 benchmark questions and ideal responses and five guideline questions for chatbot responses this framework validated by mental health experts was tested on a gpt35turbobased chatbot automated evaluation methods explored included large language model llmbased scoring an agentic approach using realtime data and embedding models to compare chatbot responses against ground truth standards results the results highlight the importance of guidelines and ground truth for improving llm evaluation accuracy the agentic method dynamically accessing reliable information demonstrated the best alignment with human assessments adherence to a standardized expertvalidated framework significantly enhanced chatbot response safety and reliability discussion our findings emphasize the need for comprehensive experttailored safety evaluation metrics for mental health chatbots while llms have significant potential careful implementation is necessary to mitigate risks the superior performance of the agentic approach underscores the importance of realtime data access in enhancing chatbot reliability conclusion the study validated an evaluation framework for mental health chatbots proving its effectiveness in improving safety and reliability future work should extend evaluations to accuracy bias empathy and privacy to ensure holistic assessment and responsible integration into healthcare standardized evaluations will build trust among users and professionals facilitating broader adoption and improved mental health support through technology,http://arxiv.org/abs/2408.04650v1,1
emotion regulation plays a key role in human behavior and overall wellbeing neurofeedback is a noninvasive selfbrain training technique used for emotion regulation to enhance brain function and treatment of mental disorders through behavioral changes previous neurofeedback research often focused on using activity from a single brain region as measured by fmri or power from one or two eeg electrodes in a new study we employed connectivitybased eeg neurofeedback through recalling positive autobiographical memories and simultaneous fmri to upregulate positive emotion in our novel approach the feedback was determined by the coherence of eeg electrodes rather than the power of one or two electrodes we compared the efficiency of this connectivitybased neurofeedback to traditional activitybased neurofeedback through multiple experiments the results showed that connectivitybased neurofeedback effectively improved bold signal change and connectivity in key emotion regulation regions such as the amygdala thalamus and insula and increased eeg frontal asymmetry which is a biomarker for emotion regulation and treatment of mental disorders such as ptsd anxiety and depression and coherence among eeg channels the psychometric evaluations conducted both before and after the neurofeedback experiments revealed that participants demonstrated improvements in enhancing positive emotions and reducing negative emotions when utilizing connectivitybased neurofeedback as compared to traditional activitybased and sham neurofeedback approaches these findings suggest that connectivitybased neurofeedback may be a superior method for regulating emotions and could be a useful alternative therapy for mental disorders providing individuals with greater control over their brain and mental functions,http://arxiv.org/abs/2204.01087v3,1
interpersonal communication plays a key role in managing peoples emotions especially on digital platforms studies have shown that people use social media and consume online content to regulate their emotions and find support for rest and recovery however these platforms are not designed for emotion regulation which limits their effectiveness in this regard to address this issue we propose an approach to enhance interpersonal emotion regulation ier on online platforms through content recommendation the objective is to empower users to regulate their emotions while actively or passively engaging in online platforms by crafting media content that aligns with ier strategies particularly empathic responding the proposed recommendation system is expected to blend systeminitiated and userinitiated emotion regulation paving the way for realtime ier practices on digital media platforms to assess the efficacy of this approach a mixedmethod research design is used including the analysis of textbased social media data and a user survey digital applications has served as facilitators in this process given the widespread recognition of digital media applications for digital emotion regulation der the study collects 375k instances of user posts and interactions on reddit over a year to design a contextual multiarmed bandits cmab based recommendation system using features from user activity and preferences the experimentation shows that the empathic recommendations generated by the proposed recommendation system are preferred by users over widely accepted er strategies such as distraction and avoidance,http://arxiv.org/abs/2408.07704v1,1
online mental health treatment has the premise to meet the increasing demand for mental health treatment at a lower cost than traditional treatment however online treatment suffers from high dropout rates which might negate their cost effectiveness predictive models might aid in early identification of deviating clients which allows to target them directly to prevent dropout and improve treatment outcomes we propose a twostaged multiobjective optimization process to automatically infer model structures based on ecological momentary assessment for prediction of future symptom development the proposed multiobjective optimization approach results in a temporalcausal network model with the best prediction performance for each concept this allows for a selection of a disorderspecific model structure based on the envisioned field of application,http://arxiv.org/abs/1809.04494v1,1
mental healthcare has seen numerous benefits from interactive technologies and artificial intelligence various interventions have successfully used intelligent technologies to automate the assessment and evaluation of psychological treatments and mental wellbeing and functioning these technologies include different types of robots video games and conversational agents the paper critically analyzes existing solutions with the outlooks for their future in particular we igive an overview of the technology for mental health ii critically analyze the technology against the proposed criteria and iii provide the design outlooks for these technologies,http://arxiv.org/abs/2105.05306v1,1
the popularization of the internet created a revitalized digital media with monetization driven by clicks journalists have reprioritized their content for the highly competitive atmosphere of online news the resulting negativity bias is harmful and can lead to anxiety and mood disturbance we utilized a pipeline of 4 sentiment analysis models trained on various datasets using sequential lstm bert and svm models when combined the application a mobile app solely displays uplifting and inspiring stories for users to read results have been successful 1300 users rate the app at 49 stars and 85 report improved mental health by using it,http://arxiv.org/abs/2108.07706v1,1
this position paper describes the implementation and initial findings of a game called personal investigator pi pi is an online 3d detective game that implements a model of brief solution focused therapy bsft it aims to help teenagers overcome mental health problems and engage with traditional mental health care services it is predicted that the combination of goaloriented gaming with a model of goaloriented therapy will help to attract and sustain the interest of teenagers a group that therapists often have difficulty engaging with pi is the first game to integrate this established psychotherapy approach into an engaging online 3d game,http://arxiv.org/abs/2207.02310v1,1
barriers to accessing mental health assessments including cost and stigma continues to be an impediment in mental health diagnosis and treatment machine learning approaches based on speech samples could help in this direction in this work we develop machine learning solutions to diagnose anxiety disorders from audio journals of patients we work on a novel anxiety dataset provided through collaboration with kintsugi mindful wellness inc and experiment with several models of varying complexity utilizing audio text and a combination of multiple modalities we show that the multimodal and audio embeddings based approaches achieve good performance in the task achieving an auc roc score of 068069,http://arxiv.org/abs/2312.15272v1,1
this article presents a method for promptbased mental health screening from a large and noisy dataset of social media text our method uses gpt 35 prompting to distinguish publications that may be more relevant to the task and then uses a straightforward bagofwords text classifier to predict actual user labels results are found to be on pair with a bert mixture of experts classifier and incurring only a fraction of its training costs,http://arxiv.org/abs/2401.05912v2,1
this research project aims to tackle the growing mental health challenges in todays digital age it employs a modified pretrained bert model to detect depressive text within social media and users web browsing data achieving an impressive 93 test accuracy simultaneously the project aims to incorporate physiological signals from wearable devices such as smartwatches and eeg sensors to provide longterm tracking and prognosis of mood disorders and emotional states this comprehensive approach holds promise for enhancing early detection of depression and advancing overall mental health outcomes,http://arxiv.org/abs/2401.13722v1,1
dialogue systems for mental health care aim to provide appropriate support to individuals experiencing mental distress while extensive research has been conducted to deliver adequate emotional support existing studies cannot identify individuals who require professional medical intervention and cannot offer suitable guidance we introduce the diagnostic emotional support conversation task for an advanced mental health management system we develop the desc dataset to assess depression symptoms while maintaining user experience by utilizing taskspecific utterance generation prompts and a strict filtering algorithm evaluations by professional psychological counselors indicate that desc has a superior ability to diagnose depression than existing data additionally conversational quality evaluation reveals that desc maintains fluent consistent and coherent dialogues,http://arxiv.org/abs/2408.06044v1,1
the onset of old age brings physiological and mental changes with anxiety and depression being common mental disorders that can trigger other health issues and reduce lifespan however due to a global shortage of mental health professionals combined with a growing population and limited awareness these disorders often go undiagnosed music therapy offers a reliable method to address psychological emotional and cognitive needs this paper presents an approach that monitors anxiety and depression symptoms in real time using lowcomplexity body sensors followed by automated personalised music therapy reducing the dependence on therapists and improving mental health care accessibility,http://arxiv.org/abs/2410.02552v1,1
social media platforms particularly reddits repilepsy community offer a unique perspective into the experiences of individuals with epilepsy pwe and their caregivers this study analyzes 57k posts and 533k comments to explore key themes across demographics such as age gender and relationships our findings highlight significant discussions on epilepsyrelated challenges including depression with 3975 of posts indicating severe symptoms driving restrictions workplace concerns and pregnancyrelated issues in women with epilepsy we introduce a novel engagement metric fp which incorporates post length sentiment scores and readability to quantify community interaction this analysis underscores the importance of integrated care addressing both neurological and mental health challenges faced by pwe the insights from this study inform strategies for targeted support and awareness interventions,http://arxiv.org/abs/2412.01692v1,1
we propose a pipeline for gaining insights into complex diseases by training llms on challenging social media text data classification tasks obtaining explanations for the classification outputs and performing qualitative and quantitative analysis on the explanations we report initial results on predicting explaining and systematizing the explanations of predicted reports on mental health concerns in people reporting lyme disease concerns we report initial results on predicting future adhd concerns for people reporting anxiety disorder concerns and demonstrate preliminary results on visualizing the explanations for predicting that a person with anxiety concerns will in the future have adhd concerns,http://arxiv.org/abs/2412.10414v1,1
due to the shift of civilization from the industrial age to the information age mathematical literacy has become a necessity in the twentyfirst century however in order to learn and contribute to the mathematical community one has to be in a state of good mental health when i say good mental health i mean one has to develop a set of healthy coping strategies be in a positive learning environment and have a social support system traditionally universities have been the venue of such higher learning if these institutions want to remain as thriving grounds for higher education and engines of research postsecondary institutions need to be aware of these factors and actively contribute to the wellbeing of its students and faulty unfortunately students do not always receive the support necessary to be mentally healthy the purpose of this paper is to examine how social awareness and sensitivity of mental health in a university setting is a key component for individuals to flourish academically and grow personally several voluntary surveys completed by my first and second year math students at the university of waterloo will be presented the surveys investigated the students experiences at the university particularly in the mathematics department in addition this paper explores an alternative way to structure math classes specifically assignment scheduling in order to help students develop healthy study habits the scores for the alternativescheduled assignments are statistically analyzed and compared to scores of the typicalscheduled assignments currently implemented at the university of waterloo finally this paper briefly discusses the importance of socialization specifically for young mathematicians and scientists and potential consequences of reduced social exposure in the digital age,http://arxiv.org/abs/1511.06699v1,1
mental health counseling is an enterprise with profound societal importance where conversations play a primary role in order to acquire the conversational skills needed to face a challenging range of situations mental health counselors must rely on training and on continued experience with actual clients however in the absence of large scale longitudinal studies the nature and significance of this developmental process remain unclear for example prior literature suggests that experience might not translate into consequential changes in counselor behavior this has led some to even argue that counseling is a profession without expertise in this work we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution we use our framework to conduct a large longitudinal study of mental health counseling conversations tracking over 3400 counselors across their tenure we reveal that overall counselors do indeed change their conversational behavior to become more diverse across interactions developing an individual voice that distinguishes them from other counselors furthermore a finergrained investigation shows that the rate and nature of this diversification vary across functionally different conversational components,http://arxiv.org/abs/1906.07194v1,1
background adverse childhood experiences aces a set of negative events and processes that a person might encounter during childhood and adolescence have been proven to be linked to increased risks of a multitude of negative health outcomes and conditions when children reach adulthood and beyond objective to better understand the relationship between aces and their relevant risk factors with associated health outcomes and to eventually design and implement preventive interventions access to an integrated coherent dataset is needed therefore we implemented a formal ontology as a resource to allow the mental health community to facilitate data integration and knowledge modeling and to improve aces surveillance and research methods we use advanced knowledge representation and semantic web tools and techniques to implement the ontology the current implementation of the ontology is expressed in the description logic alcriqd a sublogic of web ontology language owl 2 results the aces ontology has been implemented and made available to the mental health community and the public via the bioportal repository moreover multiple usecase scenarios have been introduced to showcase and evaluate the usability of the ontology in action the ontology was created to be used by major actors in the aces community with different applications from the diagnosis of individuals and predicting potential negative outcomes that they might encounter to the prevention of aces in a population and designing interventions and policies conclusions the aces ontology provides a uniform and reusable semantic network and an integrated knowledge structure for mental health practitioners and researchers to improve aces surveillance and evaluation,http://arxiv.org/abs/1912.05530v1,1
in late december 2019 the novel coronavirus sarscov2 and the resulting disease covid19 were first identified in wuhan china the disease slipped through containment measures with the first known case in the united states being identified on january 20th 2020 in this paper we utilize survey data from the interuniversity consortium for political and social research and apply several statistical and machine learning models and techniques such as decision trees multinomial logistic regression naive bayes knearest neighbors support vector machines neural networks random forests gradient tree boosting xgboost catboost lightgbm synthetic minority oversampling and chisquared test to analyze the impacts the covid19 pandemic has had on the mental health of frontline workers in the united states through the interpretation of the many models applied to the mental health survey data we have concluded that the most important factor in predicting the mental health decline of a frontline worker is the healthcare role the individual is in nurse emergency room staff surgeon etc followed by the amount of sleep the individual has had in the last week the amount of covid19 related news an individual has consumed on average in a day the age of the worker and the usage of alcohol and cannabis,http://arxiv.org/abs/2112.00227v2,1
the issue of internet addiction has become a serious social and health issue in east asian countries there are only a few treatment programs for internet addiction and their effectiveness with people from east asian remains unclear as support and treatment develop it is necessary to understand cultural preferences for dealing with this concern using data from the east asian social survey eass this study examined preferred sources of assistance for help with internet use problems in four countries china japan south korea and taiwan preferences for kin versus nonkin support use of alternative medicine and professional mental health assistance were examined as were betweencountry differences in support preferences the results indicate a strong preference for seeking assistance from close relatives followed by nonkin support ie close friends and coparticipants in religious institutions alternative medicine and professional mental health services respectively while there is a strong preference for family support over 80 of survey respondents were open to seeking formal or informal mental health support outside the family there were some significant differences between countries with south koreans being more likely to seek nonkin support and professional support for internet addiction concerns compared to chinese these differences are discussed in the context of cultural and policy developments in east asian countries findings suggest the need for a more holistic approach to treating low mental health concerns,http://arxiv.org/abs/1902.00757v1,1
although combination antiretroviral therapy art is highly effective in suppressing viral load for people with hiv pwh many art agents may exacerbate central nervous system cnsrelated adverse effects including depression therefore understanding the effects of art drugs on the cns function especially mental health can help clinicians personalize medicine with less adverse effects for pwh and prevent them from discontinuing their art to avoid undesirable health outcomes and increased likelihood of hiv transmission the emergence of electronic health records offers researchers unprecedented access to hiv data including individuals mental health records drug prescriptions and clinical information over time however modeling such data is very challenging due to highdimensionality of the drug combination space the individual heterogeneity and sparseness of the observed drug combinations we develop a bayesian nonparametric approach to learn drug combination effect on mental health in pwh adjusting for sociodemographic behavioral and clinical factors the proposed method is built upon the subsettree kernel method that represents drug combinations in a way that synthesizes known regimen structure into a single mathematical representation it also utilizes a distancedependent chinese restaurant process to cluster heterogeneous population while taking into account individuals treatment histories we evaluate the proposed approach through simulation studies and apply the method to a dataset from the womens interagency hiv study yielding interpretable and promising results our method has clinical utility in guiding clinicians to prescribe more informed and effective personalized treatment based on individuals treatment histories and clinical characteristics,http://arxiv.org/abs/2004.05487v1,1
recent years have seen a rise in social media platforms that provide peertopeer support to individuals suffering from mental distress studies on the impact of these platforms have focused on either shortterm scales of singlepost threads or longterm changes over arbitrary period of time months or years while important such arbitrary periods do not necessarily follow users progressions through acute periods of distress using data from talklife a mental health platform we find that user activity follows a distinct pattern of high activity periods with interleaving periods of no activity and propose a method for identifying such bursts and breaks in activity we then show how studying activity during bursts can provide a personalized mediumterm analysis for a key question in online mental health communities what characteristics of user activity lead some users to find support and help while others fall short using two independent outcome metrics moments of cognitive change and selfreported changes in mood during a burst of activity we identify two actionable features that can improve outcomes for users persistence within bursts and giving complex emotional support to others our results demonstrate the value of considering bursts as a natural unit of analysis for psychosocial change in online mental health communities,http://arxiv.org/abs/2004.10330v1,1
the covid19 pandemic has affected all aspects of society not only bringing health hazards but also posing challenges to public order governments and mental health moreover it is the first one in history in which people from around the world uses social media to massively express their thoughts and concerns this study aims at examining the stages of crisis response and recovery as a sociological problem by operationalizing a wellknown model of crisis stages in terms of a psycholinguistic analysis based on a large collection of twitter data spanning from march to august 2020 in argentina we present a thematic analysis on the differences in language used in social media posts and look at indicators that reveal the different stages of a crisis and the country response thereof the analysis was combined with a study of the temporal prevalence of mental health conversations across the time span beyond the argentinian casestudy the proposed approach and analyses can be applied to any public largescale data this approach can provide insights for the design of public health politics oriented to monitor and eventually intervene during the different stages of a crisis and thus improve the adverse mental health effects on the population,http://arxiv.org/abs/2011.11024v1,1
with strong marketing advocacy of the benefits of cannabis use for improved mental health cannabis legalization is a priority among legislators however preliminary scientific research does not conclusively associate cannabis with improved mental health in this study we explore the relationship between depression and consumption of cannabis in a targeted social media corpus involving personal use of cannabis with the intent to derive its potential mental health benefit we use tweets that contain an association among three categories annotated by domain experts reason effect and addiction the stateoftheart natural langauge processing techniques fall short in extracting these relationships between cannabis phrases and the depression indicators we seek to address the limitation by using domain knowledge specifically the drug abuse ontology for addiction augmented with diagnostic and statistical manual of mental disorders lexicons for mental health because of the lack of annotations due to the limited availability of the domain experts time we use supervised contrastive learning in conjunction with gpt3 trained on a vast corpus to achieve improved performance even with limited supervision experimental results show that our method can significantly extract cannabisdepression relationships better than the stateoftheart relation extractor highquality annotations can be provided using a nearest neighbor approach using the learned representations that can be used by the scientific community to understand the association between cannabis and depression better,http://arxiv.org/abs/2102.01222v1,1
covid19 poses disproportionate mental health consequences to the public during different phases of the pandemic we use a computational approach to capture the specific aspects that trigger an online communitys anxiety about the pandemic and investigate how these aspects change over time first we identified nine subjects of anxiety soas in a sample of reddit posts n86 from rcovid19support using thematic analysis then we quantified reddit users anxiety by training algorithms on a manually annotated sample n793 to automatically label the soas in a larger chronological sample n6535 the nine soas align with items in various recently developed pandemic anxiety measurement scales we observed that reddit users concerns about health risks remained high in the first eight months of the pandemic these concerns diminished dramatically despite the surge of cases occurring later in general users language disclosing the soas became less intense as the pandemic progressed however worries about mental health and the future increased steadily throughout the period covered in this study people also tended to use more intense language to describe mental health concerns than health risks or death concerns our results suggest that this online groups mental health condition does not necessarily improve despite covid19 gradually weakening as a health threat due to appropriate countermeasures our system lays the groundwork for population health and epidemiology scholars to examine aspects that provoke pandemic anxiety in a timely fashion,http://arxiv.org/abs/2209.13595v1,1
developing specialized dialogue systems for mental health support requires multiturn conversation data which has recently garnered increasing attention however gathering and releasing largescale reallife multiturn conversations that could facilitate advancements in mental health support presents challenges in data privacy protection and the time and cost involved in crowdsourcing to address these challenges we introduce smile a singleturn to multiturn inclusive language expansion technique that prompts chatgpt to rewrite public singleturn dialogues into multiturn ones our work begins by analyzing language transformation and validating the feasibility of our proposed method we conduct a study on dialogue diversity including lexical features semantic features and dialogue topics demonstrating the effectiveness of our method further we employ our method to generate a largescale lifelike and diverse dialogue dataset named smilechat consisting of 55k dialogues finally we utilize the collected corpus to develop a mental health chatbot mechat to better assess the quality of smilechat we collect a smallscale reallife counseling dataset conducted by data anonymization both automatic and human evaluations demonstrate significant improvements in our dialogue system and confirm that smilechat is highquality code data and model are publicly available at,http://arxiv.org/abs/2305.00450v3,1
language models have the potential to assess mental health using social media data by analyzing online posts and conversations these models can detect patterns indicating mental health conditions like depression anxiety or suicidal thoughts they examine keywords language markers and sentiment to gain insights into an individuals mental wellbeing this information is crucial for early detection intervention and support improving mental health care and prevention strategies however using language models for mental health assessments from social media has two limitations 1 they do not compare posts against clinicians diagnostic processes and 2 its challenging to explain language model outputs using concepts that the clinician can understand ie clinicianfriendly explanations in this study we introduce process knowledgeinfused learning pkil a new learning paradigm that layers clinical process knowledge structures on language model outputs enabling clinicianfriendly explanations of the underlying language model predictions we rigorously test our methods on existing benchmark datasets augmented with such clinical process knowledge and release a new dataset for assessing suicidality pkil performs competitively achieving a 70 agreement with users while other xai methods only achieve 47 agreement average interrater agreement of 072 our evaluations demonstrate that pkil effectively explains model predictions to clinicians,http://arxiv.org/abs/2306.09824v1,1
the demand for psychological counselling has grown significantly in recent years particularly with the global outbreak of covid19 which has heightened the need for timely and professional mental health support online psychological counselling has emerged as the predominant mode of providing services in response to this demand in this study we propose the psyllm framework an aibased assistive tool leveraging large language models llms for questionanswering in psychological consultation settings to ease the demand for mental health professions our framework combines pretrained llms with realworld professional qa from psychologists and extensively crawled psychological articles the psyllm framework serves as a frontend tool for healthcare professionals allowing them to provide immediate responses and mindfulness activities to alleviate patient stress additionally it functions as a screening tool to identify urgent cases requiring further assistance we evaluated the framework using intrinsic metrics such as perplexity and extrinsic evaluation metrics with human participant assessments of response helpfulness fluency relevance and logic the results demonstrate the effectiveness of the psyllm framework in generating coherent and relevant answers to psychological questions this article discusses the potential and limitations of using large language models to enhance mental health support through ai technologies,http://arxiv.org/abs/2307.11991v2,1
mental health diseases affect childrens lives and wellbeings which have received increased attention since the covid19 pandemic analyzing psychiatric clinical notes with topic models is critical to evaluating childrens mental status over time however few topic models are built for longitudinal settings and most existing approaches fail to capture temporal trajectories for each document to address these challenges we develop a dynamic topic model with consistent topics and individualized temporal dependencies on the evolving document metadata our model preserves the semantic meaning of discovered topics over time and incorporates heterogeneity among documents in particular when documents can be categorized we propose a classifierfree approach to maximize topic heterogeneity across different document groups we also present an efficient variational optimization procedure adapted for the multistage longitudinal setting in this case study we apply our method to the psychiatric clinical notes from a large tertiary pediatric hospital in southern california and achieve a 38 increase in the overall coherence of extracted topics our real data analysis reveals that children tend to express more negative emotions during state shutdowns and more positive when schools reopen furthermore it suggests that sexual and gender minority sgm children display more pronounced reactions to major covid19 events and a greater sensitivity to vaccinerelated news than nonsgm children this study examines childrens mental health progression during the pandemic and offers clinicians valuable insights to recognize disparities in childrens mental health related to their sexual and gender identities,http://arxiv.org/abs/2312.14180v2,1
quarantine is a widelyadopted measure during health crises caused by highlycontagious diseases like covid19 yet it poses critical challenges to public mental health given this context emotional disclosure on social media in the form of keeping a diary emerges as a popular way for individuals to express emotions and record their mental health status however the exploration of emotional disclosure via diarykeeping on social media during quarantine is underexplored understanding which could be beneficial to facilitate emotional connections and enlighten health intervention measures focusing on this particular form of selfdisclosure this work proposes a quantitative approach to figure out the prevalence and changing patterns of emotional disclosure during quarantine and the possible factors contributing to the negative emotions we collected 58 796 posts with the quarantine diary keyword on weibo a popular social media website in china through text classification we capture diverse emotion categories that characterize public emotion disclosure during quarantine such as annoyed anxious boring happy hopeful and appreciative based on temporal analysis we uncover the changing patterns of emotional disclosure from longterm perspectives and periodbased perspectives eg the gradual decline of all negative emotions and the upsurge of the annoyed emotion near the end of quarantine leveraging topic modeling we also encapsulate the possible influencing factors of negative emotions such as freedom restriction and solitude and uncertainty of infection and supply we reflect on how our findings could deepen the understanding of mental health on social media and further provide practical and design implications to mitigate mental health issues during quarantine,http://arxiv.org/abs/2401.07230v1,1
among young adults suicide is indias leading cause of death accounting for an alarming national suicide rate of around 16 in recent years machine learning algorithms have emerged to predict suicidal behavior using various behavioral traits but to date the efficacy of machine learning algorithms in predicting suicidal behavior in the indian context has not been explored in literature in this study different machine learning algorithms and ensembles were developed to predict suicide behavior based on childhood trauma different mental health parameters and other behavioral factors the dataset was acquired from 391 individuals from a wellness center in india information regarding their childhood trauma psychological wellness and other mental health issues was acquired through standardized questionnaires results revealed that cascade ensemble learning methods using a support vector machine decision trees and random forest were able to classify suicidal behavior with an accuracy of 9504 using data from childhood trauma and mental health questionnaires the study highlights the potential of using these machine learning ensembles to identify individuals with suicidal tendencies so that targeted interinterventions could be provided efficiently,http://arxiv.org/abs/2401.17705v1,1
addressing the critical shortage of mental health resources for effective screening diagnosis and treatment remains a significant challenge this scarcity underscores the need for innovative solutions particularly in enhancing the accessibility and efficacy of therapeutic support embodied agents with advanced interactive capabilities emerge as a promising and costeffective supplement to traditional caregiving methods crucial to these agents effectiveness is their ability to simulate nonverbal behaviors like backchannels that are pivotal in establishing rapport and understanding in therapeutic contexts but remain underexplored to improve the rapportbuilding capabilities of embodied agents we annotated backchannel smiles in videos of intimate facetoface conversations over topics such as mental health illness and relationships we hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles using cues from speech prosody and language along with the demographics of the speaker and listener we found them to contain significant predictors of the intensity of backchannel smiles based on our findings we introduce backchannel smile production in embodied agents as a generation problem our attentionbased generative model suggests that listener information offers performance improvements over the baseline speakercentric generation approach conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more humanlike and is an attractive alternative for nonpersonal conversations over agent without backchannel smiles,http://arxiv.org/abs/2402.08837v1,1
mental health disorders significantly impact people globally regardless of background education or socioeconomic status however access to adequate care remains a challenge particularly for underserved communities with limited resources text mining tools offer immense potential to support mental healthcare by assisting professionals in diagnosing and treating patients this study addresses the scarcity of arabic mental health resources for developing such tools we introduce mentalqa a novel arabic dataset featuring conversationalstyle questionandanswer qa interactions to ensure data quality we conducted a rigorous annotation process using a welldefined schema with quality control measures data was collected from a questionanswering medical platform the annotation schema for mental health questions and corresponding answers draws upon existing classification schemes with some modifications question types encompass six distinct categories diagnosis treatment anatomy physiology epidemiology healthy lifestyle and provider choice answer strategies include information provision direct guidance and emotional support three experienced annotators collaboratively annotated the data to ensure consistency our findings demonstrate high interannotator agreement with fleiss kappa of 061 for question types and 098 for answer strategies indepth analysis revealed insightful patterns including variations in question preferences across age groups and a strong correlation between question types and answer strategies mentalqa offers a valuable foundation for developing arabic text mining tools capable of supporting mental health professionals and individuals seeking information,http://arxiv.org/abs/2405.12619v1,1
mental illness remains one of the most critical public health issues despite its importance many mental health professionals highlight a disconnect between their training and actual realworld patient practice to help bridge this gap we propose patientpsi a novel patient simulation framework for cognitive behavior therapy cbt training to build patientpsi we construct diverse patient cognitive models based on cbt principles and use large language models llms programmed with these cognitive models to act as a simulated therapy patient we propose an interactive training scheme patientpsitrainer for mental health trainees to practice a key skill in cbt formulating the cognitive model of the patient through roleplaying a therapy session with patientpsi to evaluate patientpsi we conducted a comprehensive user study of 13 mental health trainees and 20 experts the results demonstrate that practice using patientpsitrainer enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks videos and roleplay with nonpatients based on the experts perceptions patientpsi is perceived to be closer to real patient interactions than gpt4 and patientpsitrainer holds strong promise to improve trainee competencies our code and data are released at url,http://arxiv.org/abs/2405.19660v3,1
over the past decade there has been a severe staffing shortage in mental healthcare exacerbated by increased demand for mental health services due to covid19 this demand is projected to increase over the next decade or so necessitating proactive workforce planning to ensure sufficient staffing for ongoing service delivery despite the subjects critical significance the present literature lacks thorough research dedicated to developing a model that addresses the longterm workforce needs required for efficient mental healthcare planning furthermore our interactions with mental health practitioners within the united kingdoms national health service nhs revealed the practical need for such a model to address this gap we aim to develop a hybrid predictive and prescriptive modelling framework which combines longterm probabilistic forecasting with an analytical stockflow model designed specifically for mental health workforce planning given the vital role of nurses who account for onethird of the total mental health workforce we focus on modelling the headcount of nurses but the proposed model can be generalised to other types of workforce planning in the healthcare sector using statistical and machine learning approaches and realworld data from nhs we first identify factors contributing to variations in workforce requirements then develop a longterm forecasting model to estimate future workforce needs and finally integrate it into an analytical stockflow method to provide policy analysis our findings highlight the unsustainable nature of present staffing plans showing a growing nursing shortage furthermore the policy analysis demonstrates the ineffectiveness of blanket remedies highlighting the need for regionallevel policy developments,http://arxiv.org/abs/2406.17463v1,1
large language models llms are emerging as promising tools for mental health care offering scalable support through their ability to generate humanlike responses however the effectiveness of these models in clinical settings remains unclear this scoping review aimed to assess the current generative applications of llms in mental health care focusing on studies where these models were tested with human participants in realworld scenarios a systematic search across apa psycnet scopus pubmed and web of science identified 726 unique articles of which 17 met the inclusion criteria these studies encompassed applications such as clinical assistance counseling therapy and emotional support however the evaluation methods were often nonstandardized with most studies relying on ad hoc scales that limit comparability and robustness privacy safety and fairness were also frequently underexplored moreover reliance on proprietary models such as openais gpt series raises concerns about transparency and reproducibility while llms show potential in expanding mental health care access especially in underserved areas the current evidence does not fully support their use as standalone interventions more rigorous standardized evaluations and ethical oversight are needed to ensure these tools can be safely and effectively integrated into clinical practice,http://arxiv.org/abs/2408.11288v1,1
mental disorders such as anxiety and depression have become a global issue that affects the regular lives of people across different ages without proper detection and treatment anxiety and depression can hinder the sufferers study work and daily life fortunately recent advancements of digital and ai technologies provide new opportunities for better mental health care and many efforts have been made in developing automatic anxiety and depression assessment techniques however this field still lacks a publicly available largescale dataset that can facilitate the development and evaluation of aibased techniques to address this limitation we have constructed a new largescale textbfmultitextbfmodal textbfpsychological assessment corpus mmpsy on anxiety and depression assessment of mandarinspeaking adolescents the mmpsy contains audios and extracted transcripts of responses from automated anxiety or depression assessment interviews along with the selfreported anxiety or depression evaluations of the participants using standard mental health assessment questionnaires our dataset contains over 7700 postprocessed recordings of interviews for anxiety assessment and over 4200 recordings for depression assessment using this dataset we have developed a novel deeplearning based mental disorder estimation model named textbfmentalperceiver to detect anxiousdepressive mental states from recorded audio and transcript data extensive experiments on our mmpsy and the commonlyused daicwoz datasets have shown the effectiveness and superiority of our proposed mentalperceiver model in anxiety and depression detection the mmpsy dataset will be made publicly available later to facilitate the research and development of aibased techniques in the mental health care field,http://arxiv.org/abs/2408.12088v1,1
wider access to therapeutic care is one of the biggest challenges in mental health treatment due to institutional barriers some people seeking mental health support have turned to large language models llms for personalized therapy even though these models are largely unsanctioned and untested we investigate the potential and limitations of using llms as providers of evidencebased therapy by using mixed methods clinical metrics using helpert a prompt run on a large language model using the same process and training as a comparative group of peer counselors we replicated publicly accessible mental health conversations rooted in cognitive behavioral therapy cbt to compare session dynamics and counselors cbtbased behaviors between original peer support sessions and their reconstructed helpert sessions two licensed cbttrained clinical psychologists evaluated the sessions using the cognitive therapy rating scale and provided qualitative feedback our findings show that the peer sessions are characterized by empathy small talk therapeutic alliance and shared experiences but often exhibit therapist drift conversely helpert reconstructed sessions exhibit minimal therapist drift and higher adherence to cbt methods but display a lack of collaboration empathy and cultural understanding through ctrs ratings and psychologists feedback we highlight the importance of humanai collaboration for scalable mental health our work outlines the ethical implication of imparting humanlike subjective qualities to llms in therapeutic settings particularly the risk of deceptive empathy which may lead to unrealistic patient expectations and potential harm,http://arxiv.org/abs/2409.02244v1,1
this study addresses the critical issue of genderbased violences gbv impact on womens mental health gbv encompassing physical and sexual aggression often results in longlasting adverse effects for the victims including anxiety depression posttraumatic stress disorder ptsd and substance abuse artificial intelligence aibased speech technologies have proven valuable for mental health assessments however these technologies experience performance challenges when confronted with speakers whose data has not been used for training our research presents a novel approach to speakeragnostic detection of the genderbased violence victim condition gbvvc focusing on the development of robust ai models capable of generalization across diverse speakers leveraging advanced deep learning models and domainadversarial training techniques we minimize speaker identitys influence achieving a 2695 relative reduction in speaker identification ability while enhancing the gbvvc detection by a 637 relative improvement in the accuracy this shows that models can focus on discriminative paralinguistic biomarkers that enhance the gbvvc prediction and reduce the subjectspecific traits impact additionally our models predictions moderately correlate with preclinical ptsd symptoms emphasizing the link between gbv and mental health this work paves the way for aipowered tools to aid mental health professionals in addressing this societal issue offering a promising baseline for further research,http://arxiv.org/abs/2411.18177v1,1
we introduce a generalpurpose humanintheloop dual dialogue system to support mental health care professionals the system codesigned with care providers is conceptualized to assist them in interacting with care seekers rather than functioning as a fully automated dialogue system solution the ai assistant within the system reduces the cognitive load of mental health care providers by proposing responses analyzing conversations to extract pertinent themes summarizing dialogues and recommending localized relevant content and internetbased cognitive behavioral therapy exercises these functionalities are achieved through a multiagent system design where each specialized supportive agent is characterized by a large language model in evaluating the multiagent system we focused specifically on the proposal of responses to emotionally distressed care seekers we found that the proposed responses matched a reasonable human quality in demonstrating empathy showing its appropriateness for augmenting the work of mental health care providers,http://arxiv.org/abs/2411.18429v2,1
for the early identification diagnosis and treatment of mental health illnesses the integration of deep learning dl and machine learning ml has started playing a significant role by evaluating complex data from imaging genetics and behavioral assessments these technologies have the potential to significantly improve clinical outcomes however they also present unique challenges related to data integration and ethical issues this survey reviews the development of ml and dl methods for the early diagnosis and treatment of mental health issues it examines a range of applications with a particular emphasis on behavioral assessments genetic and biomarker analysis and medical imaging for diagnosing diseases like depression bipolar disorder and schizophrenia predictive modeling for illness progression is further discussed focusing on the role of risk prediction models and longitudinal studies key findings highlight how ml and dl can improve diagnostic accuracy and treatment outcomes while addressing methodological inconsistencies data integration challenges and ethical concerns the study emphasizes the importance of building realtime monitoring systems for individualized treatment enhancing data fusion techniques and fostering interdisciplinary collaboration future research should focus on overcoming these obstacles to ensure the valuable and ethical application of ml and dl in mental health services,http://arxiv.org/abs/2412.06147v1,1
the problem we consider considers estimating a multivariate longitudinal panel data model whose outcomes can be a combination of discrete and continuous variables this problem is challenging because the likelihood is usually analytically intractable our article makes both a methodological contribution and also a substantive contribution to the application the methodological contribution is to introduce into the panel data literature a particle metropolis within gibbs method to carry out bayesian inference using a hamiltonian monte carlo neal 2011 proposal for sampling the vector of unknown parameters we note that in panel data models the our second contribution is to apply our method to carry out a serious analysis of the impact of serious life events on mental health and excessive alcohol consumption the dependence between these two outcomes may be more pronounced when consumption of alcohol is excessive and mental health poor which in turn has implications for how life events impact the joint distribution of the outcomes,http://arxiv.org/abs/1706.03953v3,1
in recent years we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks such as similarity entailment and sentiment analysis here we introduce a new task understanding of mental health concepts derived from cognitive behavioural therapy cbt we define a mental health ontology based on the cbt principles annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform nondeeplearning models in this difficult task this understanding module will be an essential component of a statistical dialogue system delivering therapy,http://arxiv.org/abs/1809.00640v1,1
with more than 300 million people depressed worldwide depression is a global problem due to access barriers such as social stigma cost and treatment availability 60 of mentallyill adults do not receive any mental health services effective and efficient diagnosis relies on detecting clinical symptoms of depression automatic detection of depressive symptoms would potentially improve diagnostic accuracy and availability leading to faster intervention in this work we present a machine learning method for measuring the severity of depressive symptoms our multimodal method uses 3d facial expressions and spoken language commonly available from modern cell phones it demonstrates an average error of 367 points 153 relative on the clinicallyvalidated patient health questionnaire phq scale for detecting major depressive disorder our model demonstrates 833 sensitivity and 826 specificity overall this paper shows how speech recognition computer vision and natural language processing can be combined to assist mental health patients and practitioners this technology could be deployed to cell phones worldwide and facilitate lowcost universal access to mental health care,http://arxiv.org/abs/1811.08592v2,1
precision medicine has received attention both in and outside the clinic we focus on the latter by exploiting the relationship between individuals social interactions and their mental health to develop a predictive model of ones likelihood to be depressed or anxious from rich dynamic social network data to our knowledge we are the first to do this existing studies differ from our work in at least one aspect they do not model social interaction data as a network they do so but analyze static network data they examine correlation between social networks and health but without developing a predictive model or they study other individual traits but not mental health in a systematic and comprehensive evaluation we show that our predictive model that uses dynamic social network data is superior to its static network as well as nonnetwork equivalents when run on the same data,http://arxiv.org/abs/1908.02614v1,1
many statistical models have high accuracy on test benchmarks but are not explainable struggle in lowresource scenarios cannot be reused for multiple tasks and cannot easily integrate domain expertise these factors limit their use particularly in settings such as mental health where it is difficult to annotate datasets and model outputs have significant impact we introduce a micromodel architecture to address these challenges our approach allows researchers to build interpretable representations that embed domain knowledge and provide explanations throughout the models decision process we demonstrate the idea on multiple mental health tasks depression classification ptsd classification and suicidal risk assessment our systems consistently produce strong results even in lowresource scenarios and are more interpretable than alternative methods,http://arxiv.org/abs/2109.13770v1,1
the mental disorder of online users is determined using social media posts the major challenge in this domain is to avail the ethical clearance for using the user generated text on social media platforms academic re searchers identified the problem of insufficient and unlabeled data for mental health classification to handle this issue we have studied the effect of data augmentation techniques on domain specific user generated text for mental health classification among the existing well established data augmentation techniques we have identified easy data augmentation eda conditional bert and back translation bt as the potential techniques for generating additional text to improve the performance of classifiers further three different classifiers random forest rf support vector machine svm and logistic regression lr are employed for analyzing the impact of data augmentation on two publicly available social media datasets the experiments mental results show significant improvements in classifiers performance when trained on the augmented data,http://arxiv.org/abs/2112.10064v1,1
the present work discusses the pertinence of a sociotype construct both theoretically and empirically oriented the term based on the conceptual chain genotypephenotypesociotype suggests an evolutionary preference in the human species for some determined averages of social relationships this core pattern or sociotype has been explored herein for the networking relationships of young people165 university students filling in a 20items questionnaire on their social interactions in spite that this is a preliminary study interesting results have been obtained on gender conversation time mental health sociability level and satisfaction with personal relationships this sociotype hypothesis could be a timely enterprise for mental health and quality of life policies,http://arxiv.org/abs/1405.4136v1,1
more than two thirds of mental health problems have their onset during childhood or adolescence identifying children at risk for mental illness later in life and predicting the type of illness is not easy we set out to develop a platform to define subtypes of childhood socialemotional development using longitudinal multifactorial traitbased measures subtypes discovered through this study could ultimately advance psychiatric knowledge of the early behavioural signs of mental illness to this extent we have examined two types of models latent class mixture models and gpbased models our findings indicate that while gp models come close in accuracy of predicting future trajectories lcmms predict the trajectories as well in a fraction of the time unfortunately neither of the models are currently accurate enough to lead to immediate clinical impact the available data related to the development of childhood mental health is often sparse with only a few time points measured and require novel methods with improved efficiency and accuracy,http://arxiv.org/abs/1612.01055v1,1
with ubiquity of social media platforms millions of people are sharing their online persona by expressing their thoughts moods emotions feelings and even their daily struggles with mental health issues voluntarily and publicly on social media unlike the most existing efforts which study depression by analyzing textual content we examine and exploit multimodal big data to discern depressive behavior using a wide variety of features including individuallevel demographics by developing a multimodal framework and employing statistical techniques for fusing heterogeneous sets of features obtained by processing visual textual and user interaction data we significantly enhance the current stateoftheart approaches for identifying depressed individuals on twitter improving the average f1score by 5 percent as well as facilitate demographic inference from social media for broader applications besides providing insights into the relationship between demographics and mental health our research assists in the design of a new breed of demographicaware health interventions,http://arxiv.org/abs/1902.06843v1,1
the outbreak of coronavirus disease 2019 covid19 recently has affected human life to a great extent besides direct physical and economic threats the pandemic also indirectly impact peoples mental health conditions which can be overwhelming but difficult to measure the problem may come from various reasons such as unemployment status stayathome policy fear for the virus and so forth in this work we focus on applying natural language processing nlp techniques to analyze tweets in terms of mental health we trained deep models that classify each tweet into the following emotions anger anticipation disgust fear joy sadness surprise and trust we build the emoct emotioncovid19tweet dataset for the training purpose by manually labeling 1000 english tweets furthermore we propose and compare two methods to find out the reasons that are causing sadness and fear,http://arxiv.org/abs/2004.10899v3,1
depression and posttraumatic stress disorder ptsd are psychiatric conditions commonly associated with experiencing a traumatic event estimating mental health status through noninvasive techniques such as activitybased algorithms can help to identify successful early interventions in this work we used locomotor activity captured from 1113 individuals who wore a research grade smartwatch posttrauma a convolutional variational autoencoder vae architecture was used for unsupervised feature extraction from four weeks of actigraphy data by using vae latent variables and the participants pretrauma physical health status as features a logistic regression classifier achieved an area under the receiver operating characteristic curve auc of 064 to estimate mental health outcomes the results indicate that the vae model is a promising approach for actigraphy data analysis for mental health outcomes in longterm studies,http://arxiv.org/abs/2011.07406v2,1
in this paper we analyze the interplay between the use of offensive language and mental health we acquired publicly available datasets created for offensive language identification and depression detection and we train computational models to compare the use of offensive language in social media posts written by groups of individuals with and without selfreported depression diagnosis we also look at samples written by groups of individuals whose posts show signs of depression according to recent related studies our analysis indicates that offensive language is more frequently used in the samples written by individuals with selfreported depression as well as individuals showing signs of depression the results discussed here open new avenues in research in politenessoffensiveness and mental health,http://arxiv.org/abs/2105.14888v2,1
great research interests have been attracted to devise ai services that are able to provide mental health support however the lack of corpora is a main obstacle to this research particularly in chinese language in this paper we propose psyqa a chinese dataset of psychological health support in the form of question and answer pair psyqa is crawled from a chinese mental health service platform and contains 22k questions and 56k long and wellstructured answers based on the psychological counseling theories we annotate a portion of answer texts with typical strategies for providing support and further present indepth analysis of both lexical features and strategy patterns in the counseling answers we also evaluate the performance of generating counseling answers with the generative pretrained models results show that utilizing strategies enhances the fluency and helpfulness of generated answers but there is still a large space for future research,http://arxiv.org/abs/2106.01702v1,1
it may be difficult for some individuals to open up and share their thoughts and feelings in front of a mental health expert for those who are more at ease with a virtual agent conversational agents can serve as an intermediate step in the right direction the conversational agent must therefore be empathetic and able to conduct freeflowing conversations to this effect we present an approach for creating a generative empathetic opendomain chatbot that can be used for mental health applications we leverage large scale pretraining and empathetic conversational data to make the responses more empathetic in nature and a multiturn dialogue arrangement to maintain context our models achieve stateoftheart results on the empathetic dialogues test set,http://arxiv.org/abs/2111.08545v1,1
research community has witnessed substantial growth in the detection of mental health issues and their associated reasons from analysis of social media we introduce a new dataset for causal analysis of mental health issues in social media posts cams our contributions for causal analysis are twofold causal interpretation and causal categorization we introduce an annotation schema for this task of causal analysis we demonstrate the efficacy of our schema on two different datasets i crawling and annotating 3155 reddit posts and ii reannotating the publicly available sdcnl dataset of 1896 instances for interpretable causal analysis we further combine these into the cams dataset and make this resource publicly available along with associated source code we present experimental results of models learned from cams dataset and demonstrate that a classic logistic regression model outperforms the next best cnnlstm model by 49 accuracy,http://arxiv.org/abs/2207.04674v1,1
as a vital aspect of individuals quality of life mental health has been included as an important component of the un sustainable development goals this study focuses on a specific aspect of mental health depression and examines its relationship with commute patterns using survey data from 1528 residents in beijing china we find that every 10 additional minutes of commute time is associated with 11 higher likelihood of depression we test for the mechanisms of the commutedepression link and find that commute is associated with depression as a direct stressor rather than triggering higher work stress when decomposing commute time into modespecific time we found that time on mopedsmotorcycles has the strongest association with depression moreover the commutedepression associations are stronger for older workers and bluecollar workers hence policies that could reduce commute time encourage work from home improve jobhousing balance or increase motorcyclists safety would help promote mental health,http://arxiv.org/abs/2207.07990v1,1
social robots have been used to assist with mental wellbeing in various ways such as to help children with autism improve on their social skills and executive functioning such as joint attention and bodily awareness they are also used to help older adults by reducing feelings of isolation and loneliness as well as supporting mental wellbeing of teens and children however existing work in this sphere has only shown support for mental health through social robots by responding interactively to human activity to help them learn relevant skills we hypothesize that humans can also get help from social robots in mental wellbeing by releasing or sharing their mental health data with the social robots in this paper we present a humanrobot interaction hri study to evaluate this hypothesis during the fiveday study a total of fiftyfive n55 participants shared their inthemoment mood and stress levels with a social robot we saw a majority of positive results indicating it is worth conducting future work in this direction and the potential of social robots to largely support mental wellbeing,http://arxiv.org/abs/2208.04389v1,1
to study the causes of the 2021 great resignation we use text analysis to investigate the changes in work and quitrelated posts between 2018 and 2021 on reddit we find that the reddit discourse evolution resembles the dynamics of the us quit and layoff rates furthermore when the covid19 pandemic started conversations related to working from home switching jobs workrelated distress and mental health increased we distinguish between general workrelated and specific quitrelated discourse changes using a differenceindifferences method our main finding is that mental health and workrelated distress topics disproportionally increased among quitrelated posts since the onset of the pandemic likely contributing to the great resignation along with better labor market conditions some relief came beginningtomid2021 when these concerns decreased our study validates the use of forums such as reddit for studying emerging economic phenomena in real time complementing traditional labor market surveys and administrative data,http://arxiv.org/abs/2208.07926v1,1
with recent developments in digitization of clinical psychology nlp research community has revolutionized the field of mental health detection on social media existing research in mental health analysis revolves around the crosssectional studies to classify users intent on social media for indepth analysis we investigate existing classifiers to solve the problem of causal categorization which suggests the inefficiency of learning based methods due to limited training samples to handle this challenge we use transformer models and demonstrate the efficacy of a pretrained transfer learning on cams dataset the experimental result improves the accuracy and depicts the importance of identifying causeandeffect relationships in the underlying text,http://arxiv.org/abs/2301.02589v2,1
automatically generating short summaries from users online mental health posts could save counselors reading time and reduce their fatigue so that they can provide timely responses to those seeking help for improving their mental state recent transformersbased summarization models have presented a promising approach to abstractive summarization they go beyond sentence selection and extractive strategies to deal with more complicated tasks such as novel word generation and sentence paraphrasing nonetheless these models have a prominent shortcoming their training strategy is not quite efficient which restricts the models performance in this paper we include a curriculum learning approach to reweigh the training samples bringing about an efficient learning procedure we apply our model on extreme summarization dataset of mentsum posts a dataset of mental health related posts from reddit social media compared to the stateoftheart model our proposed method makes substantial gains in terms of rouge and bertscore evaluation metrics yielding 35 rouge1 104 rouge2 and 47 rougel 15 bertscore relative improvements,http://arxiv.org/abs/2302.00954v1,1
the papageno effect concerns how media can play a positive role in preventing and mitigating suicidal ideation and behaviors with the increasing ubiquity and widespread use of social media individuals often express and share lived experiences and struggles with mental health however there is a gap in our understanding about the existence and effectiveness of the papageno effect in social media which we study in this paper in particular we adopt a causalinference framework to examine the impact of exposure to mental health coping stories on individuals on twitter we obtain a twitter dataset with sim2m posts by sim10k individuals we consider engaging with coping stories as the treatment intervention and adopt a stratified propensity score approach to find matched cohorts of treatment and control individuals we measure the psychosocial shifts in affective behavioral and cognitive outcomes in longitudinal twitter data before and after engaging with the coping stories our findings reveal that engaging with coping stories leads to decreased stress and depression and improved expressive writing diversity and interactivity our work discusses the practical and platform design implications in supporting mental wellbeing,http://arxiv.org/abs/2302.09885v1,1
virtual mental health assistants vmhas are seeing continual advancements to support the overburdened global healthcare system that gets 60 million primary care visits and 6 million emergency room er visits annually these systems are built by clinical psychologists psychiatrists and artificial intelligence ai researchers for cognitive behavioral therapy cbt at present the role of vmhas is to provide emotional support through information focusing less on developing a reflective conversation with the patient a more comprehensive safe and explainable approach is required to build responsible vmhas to ask followup questions or provide a wellinformed response this survey offers a systematic critical review of the existing conversational agents in mental health followed by new insights into the improvements of vmhas with contextual knowledge datasets and their emerging role in clinical decision support we also provide new directions toward enriching the user experience of vmhas with explainability safety and wholesome trustworthiness finally we provide evaluation metrics and practical considerations for vmhas beyond the current literature to build trust between vmhas and patients in active communications,http://arxiv.org/abs/2304.13191v1,1
the recent shift to remote learning and work has aggravated longstanding problems such as the problem of monitoring the mental health of individuals and the progress of students towards learning targets we introduce a novel latent process model with a view to monitoring the progress of individuals towards a hardtomeasure target of interest measured by a set of variables the latent process model is based on the idea of embedding both individuals and variables measuring progress towards the target of interest in a shared metric space interpreted as an interaction map that captures interactions between individuals and variables the fact that individuals are embedded in the same metric space as the target helps assess the progress of individuals towards the target we demonstrate with the help of simulations and applications that the latent process model enables a novel look at mental health and online educational assessments in disadvantaged subpopulations,http://arxiv.org/abs/2305.09804v2,1
wellness in trivial terms combines physical social and mental wellbeing while mental health is neglected longterm success in a person life is mostly determined by his psychological health and contentment for a person in distress professional mental health services are quite expensive unpopular and invite a lot of hesitation hence it would be effective to use an android application that can offer day to day therapeutic assistance meditation sessions and guidance since it can cater to a massive community instantly in this paper we propose a mobile and web application amity with a chat group and chatbot created using a machine learning approach we have also built a dataset to train the chatbot model that we propose in this paper we briefly introduce the dataset and the machine learning model in section 3 in section 4 we include the architecture and the development details of the hybrid application next we present our results on usability and the efficiency of the idea we propose,http://arxiv.org/abs/2305.11871v1,1
precision psychiatry is an ermerging field that aims to provide individualized approaches to mental health care multivariate analysis and machine learning are used to create outcome prediction models based on clinical data such as demographics symptom assessments genetic information and brain imaging while much emphasis has been placed on technical innovation the complex and varied nature of mental health presents significant challenges to the successful implementation of these models from this perspective i review ten challenges in the field of precision psychiatry including the need for studies on realworld populations and realistic clinical outcome definitions consideration of treatmentrelated factors such as placebo effects and nonadherence to prescriptions fairness prospective validation in comparison to current practice and implementation studies of prediction models are other key issues that are currently understudied a shift is proposed from retrospective studies based on linear and static concepts of disease towards prospective research that considers the importance of contextual factors and the dynamic and complex nature of mental health,http://arxiv.org/abs/2306.12462v1,1
our paper investigates the use of discourse embedding techniques to develop a community recommendation system that focuses on mental health support groups on social media social media platforms provide a means for users to anonymously connect with communities that cater to their specific interests however with the vast number of online communities available users may face difficulties in identifying relevant groups to address their mental health concerns to address this challenge we explore the integration of discourse information from various subreddit communities using embedding techniques to develop an effective recommendation system our approach involves the use of contentbased and collaborative filtering techniques to enhance the performance of the recommendation system our findings indicate that the proposed approach outperforms the use of each technique separately and provides interpretability in the recommendation process,http://arxiv.org/abs/2307.03892v1,1
during the current mental health crisis the importance of identifying potential indicators of mental issues from social media content has surged overlooking the multifaceted nature of mental and social wellbeing can have detrimental effects on ones mental state in traditional therapy sessions professionals manually pinpoint the origins and outcomes of underlying mental challenges a process both detailed and timeintensive we introduce an approach to this intricate mental health analysis by framing the identification of wellness dimensions in reddit content as a wellness concept extraction and categorization challenge weve curated a unique dataset named wellxplain comprising 3092 entries and totaling 72813 words drawing from halbert l dunns wellregarded wellness theory our team formulated an annotation framework along with guidelines this dataset also includes humanmarked textual segments offering clear reasoning for decisions made in the wellness concept categorization process our aim in publishing this dataset and analyzing initial benchmarks is to spearhead the creation of advanced language models tailored for healthcarefocused concept extraction and categorization,http://arxiv.org/abs/2308.13710v1,1
large language models llms have demonstrated remarkable performance across various informationseeking and reasoning tasks these computational systems drive stateoftheart dialogue systems such as chatgpt and bard they also carry substantial promise in meeting the growing demands of mental health care albeit relatively unexplored as such this study sought to examine llms capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting we selected five llms version 35 and version 4 of the generative pretraining gpt vicuna fastchatt5 pathways language model palm version 2 and falcon7binstruct based on a simple instructional prompt these models responded to utterances derived from the empatheticdialogues ed dataset using three empathyrelated metrics we compared their responses to those from traditional response generation dialogue systems which were finetuned on the ed dataset along with humangenerated responses notably we discovered that responses from the llms were remarkably more empathetic in most scenarios we position our findings in light of catapulting advancements in creating empathetic conversational systems,http://arxiv.org/abs/2310.08017v1,1
artificial intelligence ai has revolutionized various fields including medicine and mental health support one promising application is chatgpt an advanced conversational ai model that uses deep learning techniques to provide humanlike responses this review paper explores the potential impact of chatgpt in psychiatry and its various applications highlighting its role in therapy and counseling techniques selfhelp and coping strategies mindfulness and relaxation techniques screening and monitoring education and information dissemination specialized support group and family support learning and training expressive and artistic therapies telepsychiatry and online support and crisis management and prevention while chatgpt offers personalized accessible and scalable support it is essential to emphasize that it should not replace the expertise and guidance of qualified mental health professionals ethical considerations such as user privacy data security and human oversight are also discussed by examining the potential and challenges this paper sheds light on the responsible integration of chatgpt in psychiatric research and practice fostering improved mental health outcomes,http://arxiv.org/abs/2311.09131v1,1
the covid19 pandemic has intensified the urgency for effective and accessible mental health interventions in peoples daily lives mobile health mhealth solutions such as ai chatbots and mindfulness apps have gained traction as they expand beyond traditional clinical settings to support daily life however the effectiveness of current mhealth solutions is impeded by the lack of contextawareness personalization and modularity to foster their reusability this paper introduces careforme a contextual multiarmed bandit cmab recommendation framework for mental health designed with contextawareness personalization and modularity at its core careforme harnesses mobile sensing and integrates online learning algorithms with user clustering capability to deliver timely personalized recommendations with its modular design careforme serves as both a customizable recommendation framework to guide future research and a collaborative platform to facilitate interdisciplinary contributions in mhealth research we showcase careformes versatility through its implementation across various platforms eg discord telegram and its customization to diverse recommendation features,http://arxiv.org/abs/2401.15188v1,1
we collected and analyzed instagram direct messages dms from 173 youth aged 1321 including 86 lgbtq youth we examined youths riskflagged social media trace data with their selfreported mental health outcomes to examine how the differing online experiences of lgbtq youth compare with their heterosexual counterparts we found that lgbtq youth experienced significantly more highrisk online interactions compared to heterosexual youth lgbtq youth reported overall poorer mental health with online harassment specifically amplifying selfharm and injury lgbtq youths mental wellbeing linked positively to sexual messages unlike heterosexual youth qualitatively we found that most of the riskflagged messages of lgbtq youth were sexually motivated however a silver lining was that they sought support for their sexual identity from peers on the platform the study highlights the importance of tailored online safety and inclusive design for lgbtq youth with implications for chi community advancements in fostering a supportive online environments,http://arxiv.org/abs/2402.08974v1,1
recent advancements in large language models llms have accelerated their usage in various domains given the fact that psychiatric interviews are goaloriented and structured dialogues between the professional interviewer and the interviewee it is one of the most underexplored areas where llms can contribute substantial value here we explore the use of llms for enhancing psychiatric interviews by analyzing counseling data from north korean defectors with traumatic events and mental health issues specifically we investigate whether llms can 1 delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms and 2 summarize stressors and symptoms based on the interview dialogue transcript here the transcript data was labeled by mental health experts for training and evaluation of llms our experimental results show that appropriately prompted llms can achieve high performance on both the symptom delineation task and the summarization task this research contributes to the nascent field of applying llms to psychiatric interview and demonstrates their potential effectiveness in aiding mental health practitioners,http://arxiv.org/abs/2403.17428v1,1
early detection of depressive episodes is crucial in managing mental health disorders such as major depressive disorder mdd and bipolar disorder however existing methods often necessitate active participation or are confined to clinical settings addressing this gap we introduce pupilsense a novel deep learningdriven mobile system designed to discreetly track pupillary responses as users interact with their smartphones in their daily lives this study presents a proofofconcept exploration of pupilsenses capabilities where we captured realtime pupillary data from users in naturalistic settings our findings indicate that pupilsense can effectively and passively monitor indicators of depressive episodes offering a promising tool for continuous mental health assessment outside laboratory environments this advancement heralds a significant step in leveraging ubiquitous mobile technology for proactive mental health care potentially transforming how depressive episodes are detected and managed in everyday contexts,http://arxiv.org/abs/2404.14590v1,1
current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation less so on data collection however recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability these aspects are particularly impactful on sensitive domains such as mental health and neurological disorders where speech data are used to develop ai applications aimed at improving the health of patients and supporting healthcare providers in this paper we chart the landscape of available speech datasets for this domain to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity we present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into a checklist focused on ethical concerns to foster more responsible research,http://arxiv.org/abs/2406.04116v1,1
the detection of depression through nonverbal cues has gained significant attention previous research predominantly centred on identifying depression within the confines of controlled laboratory environments often with the supervision of psychologists or counsellors unfortunately datasets generated in such controlled settings may struggle to account for individual behaviours in reallife situations in response to this limitation we present the extended dvlog dataset encompassing a collection of 1 261 youtube vlogs additionally the emergence of large language models llms like gpt35 and gpt4 has sparked interest in their potential they can act like mental health professionals yet the readiness of these llm models to be used in reallife settings is still a concern as they can give wrong responses that can harm the users we introduce a virtual agent serving as an initial contact for mental health patients offering cognitive behavioral therapy cbtbased responses it comprises two core functions 1 identifying depression in individuals and 2 delivering cbtbased therapeutic responses our mistral model achieved impressive scores of 701 and 309 for distortion assessment and classification along with a bert score of 887 moreover utilizing the tvlt model on our multimodal extended dvlog dataset yielded outstanding results with an impressive f1score of 678,http://arxiv.org/abs/2406.10561v1,1
as mental health issues for young adults present a pressing public health concern daily digital mood monitoring for early detection has become an important prospect an active research area digital phenotyping involves collecting and analysing data from personal digital devices such as smartphones usage and sensors and wearables to infer behaviours and mental health whilst this data is standardly analysed using statistical and machine learning approaches the emergence of large language models llms offers a new approach to make sense of smartphone sensing data despite their effectiveness across various domains llms remain relatively unexplored in digital mental health particularly in integrating mobile sensor data our study aims to bridge this gap by employing llms to predict affect outcomes based on smartphone sensing data from university students we demonstrate the efficacy of zeroshot and fewshot embedding llms in inferring general wellbeing our findings reveal that llms can make promising predictions of affect measures using solely smartphone sensing data this research sheds light on the potential of llms for affective state prediction emphasizing the intricate link between smartphone behavioral patterns and affective states to our knowledge this is the first work to leverage llms for affective state prediction and digital phenotyping tasks,http://arxiv.org/abs/2407.08240v1,1
supportseekers selfdisclosure of their suffering experiences thoughts and feelings in the post can help them get needed peer support in online mental health communities omhcs however such mental health selfdisclosure could be challenging images can facilitate the manifestation of relevant experiences and feelings in the text yet relevant images are not always available in this paper we present a technical prototype named mentalimager and validate in a human evaluation study that it can generate topical and emotionalrelevant images based on the seekers drafted posts or specified keywords two user studies demonstrate that mentalimager not only improves seekers satisfaction with their selfdisclosure in their posts but also invokes supportproviders empathy for the seekers and willingness to offer help such improvements are credited to the generated images which help seekers express their emotions and inspire them to add more details about their experiences and feelings we report concerns on mentalimager and discuss insights for supporting selfdisclosure in omhcs,http://arxiv.org/abs/2409.14859v1,1
in mental health counseling condensing dialogues into concise and relevant summaries aka counseling notes holds pivotal significance large language models llms exhibit remarkable capabilities in various generative tasks however their adaptation to domainspecific intricacies remains challenging especially within mental health contexts unlike standard llms mental health experts first plan to apply domain knowledge in writing summaries our work enhances llms ability by introducing a novel planning engine to orchestrate structuring knowledge alignment to achieve highorder planning we divide knowledge encapsulation into two major phases i holding dialogue structure and ii incorporating domainspecific knowledge we employ a planning engine on llama2 resulting in a novel framework piece our proposed system employs knowledge filteringcumscaffolding to encapsulate domain knowledge additionally piece leverages sheaf convolution learning to enhance its understanding of the dialogues structural nuances we compare piece with 14 baseline methods and observe a significant improvement across rouge and bleurt scores further expert evaluation and analyses validate the generation quality to be effective sometimes even surpassing the gold standard we further benchmark piece with other llms and report improvement including llama2 272 mistral 204 and zephyr 159 to justify the generalizability of the planning engine,http://arxiv.org/abs/2409.14907v1,1
social interactions promote wellbeing yet challenges like geographic distance and mental health conditions can limit inperson engagement advances in ai agents are transferring communication particularly in mental health where ai chatbots provide accessible nonjudgmental support however a key challenge is how effectively these systems can express empathy which is crucial in humancentered design current research highlights a gap in understanding how ai can authentically convey empathy particularly as issues like anxiety depression and loneliness increase our research focuses on this gap by comparing empathy expression in humanhuman versus humanai interactions using personal narratives and statistical analysis we examine empathy levels elicited by humans and ai including gpt4o and finetuned versions of the model this work aims to enhance the authenticity of aidriven empathy contributing to the future design of more reliable and effective mental health support systems that foster meaningful social interactions,http://arxiv.org/abs/2409.15550v1,1
this paper explores the link between education and the decision to start smoking as well as the decision to quit smoking data is gathered from ipums cps and centers for disease control and prevention probit analysis with the use of probability weight and robust standard error indicates that every additional year of education will reduce the 23 percentage point of the smoking probability and will add 353 percentage point in quitting likelihood holding home restriction public restriction cigarette price family income age gender race and ethnicity constant i believe that tobacco epidemic is a serious global issue that may be mitigated by using careful regulations on smoking restriction and education,http://arxiv.org/abs/2011.14834v1,2
anonymized electronic medical records are an increasingly popular source of research data however these datasets often lack race and ethnicity information this creates problems for researchers modeling human disease as race and ethnicity are powerful confounders for many health exposures and treatment outcomes race and ethnicity are closely linked to populationspecific genetic variation we showed that deep neural networks generate more accurate estimates for missing racial and ethnic information than competing methods eg logistic regression random forest riddle yielded significantly better classification performance across all metrics that were considered accuracy crossentropy loss error and area under the curve for receiver operating characteristic plots all p 106 we made specific efforts to interpret the trained neural network models to identify quantify and visualize medical features which are predictive of race and ethnicity we used these characterizations of informative features to perform a systematic comparison of differential disease patterns by race and ethnicity the fact that clinical histories are informative for imputing race and ethnicity could reflect 1 a skewed distribution of blue and whitecollar professions across racial and ethnic groups 2 uneven accessibility and subjective importance of prophylactic health 3 possible variation in lifestyle such as dietary habits and 4 differences in background genetic variation which predispose to diseases,http://arxiv.org/abs/1707.01623v2,2
open source software oss projects are typically the result of collective efforts performed by developers with different backgrounds although the quality of developers contributions should be the only factor influencing the evaluation of the contributions to oss projects recent studies have shown that diversity issues are correlated with the acceptance or rejection of developers contributions this paper assists this emerging stateoftheart body on diversity research with the first empirical study that analyzes how developers perceptible race and ethnicity relates to the evaluation of the contributions in oss we performed a largescale quantitative study of oss projects in github we extracted the developers perceptible race and ethnicity from their names in github using the nameprism tool and applied regression modeling of contributions ie pull requests data from ghtorrent and github we observed that among the developers whose perceptible race and ethnicity was captured by the tool only 1656 were perceptible as nonwhite developers contributions from perceptible white developers have about 610 higher odds of being accepted when compared to contributions from perceptible nonwhite developers and submitters with perceptible nonwhite races and ethnicities are more likely to get their pull requests accepted when the integrator is estimated to be from their same race and ethnicity rather than when the integrator is estimated to be white our initial analysis shows a low number of nonwhite developers participating in oss furthermore the results from our regression analysis lead us to believe that there may exist differences between the evaluation of the contributions from different perceptible races and ethnicities thus our findings reinforce the need for further studies on racial and ethnic diversity in software engineering to foster healthier oss communities,http://arxiv.org/abs/2104.06143v1,2
to answer questions about racial inequality and fairness we often need a way to infer race and ethnicity from names one way to infer race and ethnicity from names is by relying on the census bureaus list of popular last names the list however suffers from at least three limitations 1 it only contains last names 2 it only includes popular last names and 3 it is updated once every 10 years to provide better generalization and higher accuracy when first names are available we model the relationship between characters in a name and race and ethnicity using various techniques a model using long shortterm memory works best with outofsample accuracy of 85 the bestperforming lastname model achieves outofsample accuracy of 81 to illustrate the utility of the models we apply them to campaign finance data to estimate the share of donations made by people of various racial groups and to news data to estimate the coverage of various races and ethnicities in the news,http://arxiv.org/abs/1805.02109v2,2
a common practice in evidencebased decisionmaking uses estimates of conditional probabilities pyx obtained from research studies to predict outcomes y on the basis of observed covariates x given this information decisions are then based on the predicted outcomes researchers commonly assume that the predictors used in the generation of the evidence are the same as those used in applying the evidence ie the meaning of x in the two circumstances is the same this may not be the case in realworld settings across a widerange of settings ranging from clinical practice or education policy demographic attributes eg age race ethnicity are often classified differently in research studies than in decision settings this paper studies identification in such settings we propose a formal framework for prediction with what we term differential covariate classification dcc using this framework we analyze partial identification of probabilistic predictions and assess how various assumptions influence the identification regions we apply the findings to a range of settings focusing mainly on differential classification of individuals race and ethnicity in clinical medicine we find that bounds on pyx can be wide and the information needed to narrow them available only in special cases these findings highlight an important problem in using evidence in decision making a problem that has not yet been fully appreciated in debates on classification in public policy and medicine,http://arxiv.org/abs/2501.02318v1,2
there is active debate over whether to consider patient race and ethnicity when estimating disease risk by accounting for race and ethnicity it is possible to improve the accuracy of risk predictions but there is concern that their use may encourage a racialized view of medicine in diabetes risk models despite substantial gains in statistical accuracy from using race and ethnicity the gains in clinical utility are surprisingly modest these modest clinical gains stem from two empirical patterns first the vast majority of individuals receive the same screening recommendation regardless of whether race or ethnicity are included in risk models and second for those who do receive different screening recommendations the difference in utility between screening and not screening is relatively small our results are based on broad statistical principles and so are likely to generalize to many other riskbased clinical decisions,http://arxiv.org/abs/2306.10220v1,2
computational social science studies often contextualize content analysis within standard demographics since demographics are unavailable on many social media platforms eg twitter numerous studies have inferred demographics automatically despite many studies presenting proof of concept inference of race and ethnicity training of practical systems remains elusive since there are few annotated datasets existing datasets are small inaccurate or fail to cover the four most common racial and ethnic groups in the united states we present a method to identify selfreports of race and ethnicity from twitter profile descriptions despite errors inherent in automated supervision we produce models with good performance when measured on gold standard selfreport survey data the result is a reproducible method for creating largescale training resources for race and ethnicity,http://arxiv.org/abs/2005.00635v2,2
we use place of birth information from the social security administration linked to earnings data from the longitudinal employerhousehold dynamics program and detailed race and ethnicity data from the 2010 census to study how longterm earnings differentials vary by place of birth for different selfidentified race and ethnicity categories we focus on foreignborn persons from countries that are heavily hispanic and from countries in the middle east and north africa mena we find substantial heterogeneity of longterm earnings differentials within country of birth some of which will be difficult to detect when the reporting format changes from the current twoquestion version to the new singlequestion version because they depend on selfidentifications that place the individual in two distinct categories within the singlequestion format specifically hispanic and white or black and mena and white or black we also study the usaborn children of these same immigrants longterm earnings differences for the 2nd generation also vary as a function of selfidentified ethnicity and race in ways that changing to the singlequestion format could affect,http://arxiv.org/abs/2407.12775v1,2
we model the covid19 coronavirus epidemic in china we use early reported case data to predict the cumulative number of reported cases to a final size the key features of our model are the timing of implementation of major public policies restricting social movement the identification and isolation of unreported cases and the impact of asymptomatic infectious cases,http://arxiv.org/abs/2002.12298v1,2
the multidisciplinary and socially anchored nature of feminist studies presents unique challenges for bibliometric analysis as this research area transcends traditional disciplinary boundaries and reflects discussions from feminist and lgbtqia social movements this paper proposes a novel approach for identifying gendersex related publications scattered across diverse scientific disciplines using the dimensions database we employ bibliometric techniques natural language processing nlp and manual curation to compile a dataset of scientific publications that allows for the analysis of gender studies and its influence across different disciplines this is achieved through a methodology that combines a core of specialized journals with a comprehensive keyword search over titles these keywords are obtained by applying topic modeling bertopic to the corpus of titles and abstracts from the core this methodological strategy divided into two stages reflects the dynamic interaction between gender studies and its dialogue with different disciplines this hybrid system surpasses basic keyword search by mitigating potential biases introduced through manual keyword enumeration the resulting dataset comprises over 19 million scientific documents published between 1668 and 2023 spanning four languages this dataset enables a characterization of gender studies in terms of addressed topics citation and collaboration dynamics and institutional and regional participation by addressing the methodological challenges of studying morethandisciplinary research areas this approach could also be adapted to delineate other conversations where disciplinary boundaries are difficult to disentangle,http://arxiv.org/abs/2411.18306v1,2
in 2018 the us census bureau designed a new data reconstruction and reidentification attack and tested it against their 2010 data release the specific attack executed by the bureau allows an attacker to infer the race and ethnicity of respondents with average 75 precision for 85 of the respondents assuming that the attacker knows the correct age sex and address of the respondents they interpreted the attack as exceeding the bureaus privacy standards and so introduced stronger privacy protections for the 2020 census in the form of the topdown algorithm tda this paper demonstrates that race and ethnicity can be inferred from the tdaprotected census data with substantially better precision and recall using less prior knowledge only the respondents address race and ethnicity can be inferred with average 75 precision for 98 of the respondents and can be inferred with 100 precision for 11 of the respondents the inference is done by simply assuming that the raceethnicity of the respondent is that of the majority raceethnicity for the respondents census block the conclusion to draw from this simple demonstration is not that the bureaus data releases lack adequate privacy protections indeed it is the purpose of the data releases to allow this kind of inference the problem rather is that the bureaus criteria for measuring privacy is flawed and overly pessimistic,http://arxiv.org/abs/2202.04872v2,2
the design of datadriven dashboards that inform municipalities on ongoing changes in infections within their community is addressed in this research daily reports of covid19 infections published by the state of wisconsin as the initial surge in the pandemic ensued during the october 2020 to september 2021 time frame is considered as a case study of particular interest is the identification of regions and population groups distinguished by race and ethnicity that may be experiencing a disproportional rate of infections over time this study integrates the municipalitylevel daily positive cases that are disaggregated by race and ethnicity and population size data derived from the us census bureau the goal is to present timely datadriven information in a manner that is accessible to the general population is relatable to the constituents and promotes community engagement in managing and mitigating the infections a statistical metric referred to as the rank difference and its persistence over time is used to capture the disproportional incidence of covid19 positive cases on particular race and ethnic groups in relation to their population size a persistence index is derived to identify regions that continually exhibit positive rank differences on a daily time scale and indicate disparity in disease incidence the analysis leads to the identification that several municipalities in wisconsin that are located in regions of low population and away from the denser urban centers are those that continue to exhibit disparity in the infection rates for blackafrican american and hispaniclatino population groups examples of a dashboard that can be utilized to capture both aggregate level and temporal patterns of covid19 infections are presented,http://arxiv.org/abs/2211.12583v1,2
the research question this report addresses is how and to what extent those directly involved with the design development and employment of a specific black box algorithm can be certain that it is not unlawfully discriminating directly andor indirectly against particular persons with protected characteristics eg gender race and ethnicity,http://arxiv.org/abs/1604.07180v1,2
income inequality between different races in the us is especially large this difference is even larger when gender is involved in a complementary study we have developed a dynamic microeconomic model accurately describing the evolution of male and female incomes since 1930 here we extend our analysis and model the disparity between black and white population in the us separately for males and females unfortunately income microdata provided by the us census bureau for other races and ethnic groups are not time compatible or too short for modelling purposes we are forced to constrain our analysis to black and white population but all principal results can be extrapolated to other races and ethnicities our analysis shows that black females and white males are two poles of the overall income inequality the prediction of income distribution for two extreme cases with one model is the main challenge of this study,http://arxiv.org/abs/2007.06530v1,2
this paper presents racebert a transformerbased model for predicting race and ethnicity from character sequences in names and an accompanying python package using a transformerbased model trained on a us florida voter registration dataset the model predicts the likelihood of a name belonging to 5 us census race categories white black hispanic asian pacific islander american indian alaskan native i build on sood and laohaprapanon 2018 by replacing their lstm model with transformerbased models pretrained bert model and a roberta model trained from scratch and compare the results to the best of my knowledge racebert achieves stateoftheart results in race prediction using names with an average f1score of 086 a 41 improvement over the previous stateoftheart and improvements between 1517 for nonwhite names,http://arxiv.org/abs/2112.03807v3,2
we provide the largest compiled publicly available dictionaries of first middle and last names for the purpose of imputing race and ethnicity using for example bayesian improved surname geocoding bisg the dictionaries are based on the voter files of six southern states that collect selfreported racial data upon voter registration our data cover a much larger scope of names than any comparable dataset containing roughly one million first names 11 million middle names and 14 million surnames individuals are categorized into five mutually exclusive racial and ethnic groups white black hispanic asian and other and racialethnic counts by name are provided for every name in each dictionary counts can then be normalized rowwise or columnwise to obtain conditional probabilities of race given name or name given race these conditional probabilities can then be deployed for imputation in a data analytic task for which ground truth racial and ethnic data is not available,http://arxiv.org/abs/2208.12443v1,2
public engagement pe initiatives can lead to a long term public support of science however most of the real impact of pe initiatives within the context of longterm science policy is not completely understood an examination of the national aeronautics and space administrations nasa hubble space telescope james webb space telescope and international sunearth explorer 3 reveal how large grassroots movements led by citizen scientists and space aficionados can have profound effects on public policy we explore the role and relevance of public grassroots movements in the policy of space astronomy initiatives present some recent cases which illustrate policy decisions involving broader interest groups and consider new avenues of pe including crowdfunding and crowdsourcing,http://arxiv.org/abs/1408.4987v2,2
bayesian improved surname geocoding bisg is a ubiquitous tool for predicting race and ethnicity using an individuals geolocation and surname here we demonstrate that statistical dependence of surname and geolocation within racialethnic categories in the united states results in biases for minority subpopulations and we introduce a rakingbased improvement our method augments the data used by bisgdistributions of race by geolocation and race by surnamewith the distribution of surname by geolocation obtained from state voter files we validate our algorithm on state voter registration lists that contain selfidentified raceethnicity,http://arxiv.org/abs/2304.09126v3,2
fairnessaware statistical learning is critical for datadriven decisionmaking to mitigate discrimination against protected attributes such as gender race and ethnicity this is especially important for highstake decisionmaking such as insurance underwriting and annuity pricing this paper proposes a new fairnessregularized principal component analysis fair pca in the context of highdimensional factor models an efficient gradient descent algorithm is constructed with adaptive selection criteria for hyperparameter tuning the fair pca is applied to mortality modelling to mitigate gender discrimination in annuity pricing the model performance has been validated through both simulation studies and empirical data analysis,http://arxiv.org/abs/2412.04663v1,2
health care decisions are increasingly informed by clinical decision support algorithms but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care further complicating the problem clinical data often have missing or poor quality racial and ethnic information which can lead to misleading assessments of algorithmic bias we present novel statistical methods that allow for the use of probabilities of racialethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities we propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error we also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe realworld scenarios where our theoretical results are likely to apply we present a case study using imputed race and ethnicity from the bayesian improved surname geocoding bisg algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment our novel methods allow policy makers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support,http://arxiv.org/abs/2402.13391v2,2
using highfrequency donation records from a major medical crowdfunding site and careful differenceindifference analysis we demonstrate that the 2020 blm surge decreased the fundraising gap between black and nonblack beneficiaries by around 50 the reduction is largely attributed to nonblack donors those beneficiaries in counties with moderate blm activities were most impacted we construct innovative instrumental variable approaches that utilize weekends and rainfall to identify the global and local effects of blm protests results suggest a broad social movement has a greater influence on charitablegiving behavior than a local event social media significantly magnifies the impact of protests,http://arxiv.org/abs/2310.14590v2,2
purpose the medical imaging and data resource center midrc open data commons was launched to accelerate the development of artificial intelligence ai algorithms to help address the covid19 pandemic the purpose of this study was to quantify longitudinal representativeness of the demographic characteristics of the primary imaging dataset compared to the united states general population us census and covid19 positive case counts from the centers for disease control and prevention cdc approach the jensen shannon distance jsd was used to longitudinally measure the similarity of the distribution of 1 all unique patients in the midrc data to the 2020 us census and 2 all unique covid19 positive patients in the midrc data to the case counts reported by the cdc the distributions were evaluated in the demographic categories of age at index sex race ethnicity and the intersection of race and ethnicity results representativeness the midrc data by ethnicity and the intersection of race and ethnicity was impacted by the percentage of cdc case counts for which data in these categories is not reported the distributions by sex and race have retained their level of representativeness over time conclusion the representativeness of the open medical imaging datasets in the curated public data commons at midrc has evolved over time as both the number of contributing institutions and overall number of subjects has grown the use of metrics such as the jsd support measurement of representativeness one step needed for fair and generalizable ai algorithm development,http://arxiv.org/abs/2303.10501v1,2
using only 34 published tables we reconstruct five variables census block sex age race and ethnicity in the confidential 2010 census person records using the 38bin age variable tabulated at the census block level at most 201 of reconstructed records can differ from their confidential source on even a single value for these five variables using only published data an attacker can verify that all records in 70 of all census blocks 97 million people are perfectly reconstructed the tabular publications in summary file 1 thus have prohibited disclosure risk similar to the unreleased confidential microdata reidentification studies confirm that an attacker can within blocks with perfect reconstruction accuracy correctly infer the actual census response on race and ethnicity for 34 million vulnerable population uniques persons with nonmodal characteristics with 95 accuracy the same precision as the confidential data achieve and far greater than statistical baselines the flaw in the 2010 census framework was the assumption that aggregation prevented accurate microdata reconstruction justifying weaker disclosure limitation methods than were applied to 2010 census public microdata the framework used for 2020 census publications defends against attacks that are based on reconstruction as we also demonstrate here finally we show that alternatives to the 2020 census disclosure avoidance system with similar accuracy enhanced swapping also fail to protect confidentiality and those that partially defend against reconstruction attacks incomplete suppression implementations destroy the primary statutory use case data for redistricting all legislatures in the country in compliance with the 1965 voting rights act,http://arxiv.org/abs/2312.11283v1,2
the ethical implications and social impacts of artificial intelligence have become topics of compelling interest to industry researchers in academia and the public however current analyses of ai in a global context are biased toward perspectives held in the us and limited by a lack of research especially outside the us and western europe this article summarizes the key findings of a literature review of recent social science scholarship on the social impacts of ai and related technologies in five global regions our team of social science researchers reviewed more than 800 academic journal articles and monographs in over a dozen languages our review of the literature suggests that ai is likely to have markedly different social impacts depending on geographical setting likewise perceptions and understandings of ai are likely to be profoundly shaped by local cultural and social context recent research in us settings demonstrates that aidriven technologies have a pattern of entrenching social divides and exacerbating social inequality particularly among historicallymarginalized groups our literature review indicates that this pattern exists on a global scale and suggests that low and middleincome countries may be more vulnerable to the negative social impacts of ai and less likely to benefit from the attendant gains we call for rigorous ethnographic research to better understand the social impacts of ai around the world global ontheground research is particularly critical to identify ai systems that may amplify social inequality in order to mitigate potential harms deeper understanding of the social impacts of ai in diverse social settings is a necessary precursor to the development implementation and monitoring of responsible and beneficial ai technologies and forms the basis for meaningful regulation of these technologies,http://arxiv.org/abs/1907.07892v1,2
comparing how different populations have suffered under covid19 is a core part of ongoing investigations into how public policy and social inequalities influence the number of and severity of covid19 cases but covid19 incidence can vary multifold from one subpopulation to another including between neighborhoods of the same city making comparisons of case rates deceptive at the same time although epidemiological heterogeneities are increasingly wellrepresented in mathematical models of disease spread fitting these models to real data on case numbers presents a tremendous challenge as does interpreting the models to answer questions such as which public health policies achieve the best outcomes which social sacrifices are most worth making here we compare covid19 casecurves between different us states by clustering case surges between march 2020 and march 2021 into groups with similar dynamics we advance the hypothesis that each surge is driven by a subpopulation of covid19 contacting individuals and make detecting the size of that population a step within our clustering algorithm clustering reveals that case trajectories in each state conform to one of a small number 46 of archetypal dynamics our results suggest that while the spread of covid19 in different states is heterogeneous there are underlying universalities in the spread of the disease that may yet be predictable by models with reduced mathematical complexity these universalities also prove to be surprisingly robust to school closures which we choose as a common but high social cost public health measure,http://arxiv.org/abs/2211.09010v1,2
we examine the way race and racial categories are adopted in algorithmic fairness frameworks current methodologies fail to adequately account for the socially constructed nature of race instead adopting a conceptualization of race as a fixed attribute treating race as an attribute rather than a structural institutional and relational phenomenon can serve to minimize the structural aspects of algorithmic unfairness in this work we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research drawing on lessons from public health biomedical research and social survey research we argue that algorithmic fairness researchers need to take into account the multidimensionality of race take seriously the processes of conceptualizing and operationalizing race focus on social processes which produce racial inequality and consider perspectives of those most affected by sociotechnical systems,http://arxiv.org/abs/1912.03593v1,2
in a recent study by ginther et al the probability of receiving a us national institutes of health nih ro1 award was related to the applicants raceethnicity the results indicate blackafricanamerican applicants were 10 less likely than white peers to receive an award after controlling for background and qualifications it has generated a widespread debate regarding the unfairness of the nih grant review process and its correction in this paper the work by ginther et al was augmented by pairing analysis axiomaticallyindividualized productivity and normalized funding success measurement although there are racial differences in r01 grant success rates normalized figures of merit for funding success explain the discrepancy the suggested leverage points for policy intervention are in question and require deeper and more thorough investigations further adjustments in policies to remove racial disparity should be made more systematically for equal opportunity rather than being limited to the nih review process,http://arxiv.org/abs/1112.3944v1,2
feature selection is a prevalent data preprocessing paradigm for various learning tasks due to the expensive cost of acquiring supervision information unsupervised feature selection sparks great interests recently however existing unsupervised feature selection algorithms do not have fairness considerations and suffer from a high risk of amplifying discrimination by selecting features that are over associated with protected attributes such as gender race and ethnicity in this paper we make an initial investigation of the fairnessaware unsupervised feature selection problem and develop a principled framework which leverages kernel alignment to find a subset of highquality features that can best preserve the information in the original feature space while being minimally correlated with protected attributes specifically different from the mainstream inprocessing debiasing methods our proposed framework can be regarded as a modelagnostic debiasing strategy that eliminates biases and discrimination before downstream learning algorithms are involved experimental results on multiple realworld datasets demonstrate that our framework achieves a good tradeoff between utility maximization and fairness promotion,http://arxiv.org/abs/2106.02216v1,2
systemic bias with respect to gender race and ethnicity often unconscious is prevalent in datasets involving choices among individuals consequently society has found it challenging to alleviate bias and achieve diversity in a way that maintains meritocracy in such settings we propose a a novel optimization approach based on optimally flipping outcome labels and training classification models simultaneously to discover changes to be made in the selection process so as to achieve diversity without significantly affecting meritocracy and b a novel implementation tool employing optimal classification trees to provide insights on which attributes of individuals lead to flipping of their labels and to help make changes in the current selection processes in a manner understandable by human decision makers we present case studies on three realworld datasets consisting of parole admissions to the bar and lending decisions and demonstrate that the price of diversity is low and sometimes negative that is we can modify our selection processes in a way that enhances diversity without affecting meritocracy significantly and sometimes improving it,http://arxiv.org/abs/2107.03900v1,2
in recent years monitoring hate speech and offensive language on social media platforms has become paramount due to its widespread usage among all age groups races and ethnicities consequently there have been substantial research efforts towards automated detection of such content using natural language processing nlp while successfully filtering textual data no research has focused on detecting hateful content in multimedia data with increased ease of data storage and the exponential growth of social media platforms multimedia content proliferates the internet as much as text data nevertheless it escapes the automatic filtering systems hate speech and offensiveness can be detected in multimedia primarily via three modalities ie visual acoustic and verbal our preliminary study concluded that the most essential features in classifying hate speech would be the speakers emotional state and its influence on the spoken words therefore limiting our current research to these modalities this paper proposes the first multimodal deep learning framework to combine the auditory features representing emotion and the semantic features to detect hateful content our results demonstrate that incorporating emotional attributes leads to significant improvement over textbased models in detecting hateful multimedia content this paper also presents a new hate speech detection video dataset hsdvd collected for the purpose of multimodal learning as no such dataset exists today,http://arxiv.org/abs/2202.06218v1,2
in the absence of sensitive race and ethnicity data researchers regulators and firms alike turn to proxies in this paper i train a bidirectional long shortterm memory bilstm model on a novel dataset of voter registration data from all 50 us states and create an ensemble that achieves up to 368 higher out of sample oos f1 scores than the best performing machine learning models in the literature additionally i construct the most comprehensive database of first and surname distributions in the us in order to improve the coverage and accuracy of bayesian improved surname geocoding bisg and bayesian improved firstname surname geocoding bifsg finally i provide the first highquality benchmark dataset in order to fairly compare existing models and aid future model developers,http://arxiv.org/abs/2307.08496v2,2
the drug overdose crisis in the united states continues to intensify fatalities have increased fivefold since 1999 reaching a record high of 108000 deaths in 2021 the epidemic has unfolded through distinct waves of different drug types uniquely impacting various age gender race and ethnic groups in specific geographical areas one major challenge in designing effective interventions is the forecasting of agespecific overdose patterns at the local level so that prevention and preparedness can be effectively delivered we develop a forecasting method that assimilates observational data obtained from the cdc wonder database with an agestructured model of addiction and overdose mortality we apply our method nationwide and to three select areas los angeles county cook county and the five boroughs of new york city providing forecasts of drugoverdose mortality and estimates of relevant epidemiological quantities such as mortality and agespecific addiction rates,http://arxiv.org/abs/2309.14452v1,2
we contribute empirical and conceptual insights regarding the roles of digital labor platforms in online freelancing focusing attention to social identities such as gender race ethnicity and occupation findings highlight how digital labor platforms reinforce and exacerbate identitybased stereotypes bias and expectations in online freelance work we focus on online freelancing as this form of working arrangement is becoming more prevalent online freelancing also relies on the marketmaking power of digital platforms to create an online labor market many see this as one likely future of work with less bias others worry that labor platforms market power allows them to embed known biases into new working arrangements a platformization of inequality drawing on data from 108 online freelancers we discuss six findings 1 female freelance work is undervalued 2 gendered occupational expectations 3 gendered treatment 4 shared expectations of differential values 5 racial stereotypes and expectations and 6 race and ethnicity as an asset we discuss the role of design in the platformization and visibility of social identity dimensions and the implications of the reinforced identity perceptions and marginalization in digital labor platforms,http://arxiv.org/abs/2309.16887v1,2
tools models and statistical methods for signal processing and medical image analysis and training deep learning models to create research prototypes for eventual clinical applications are of special interest to the biomedical imaging community but material and optical properties of biological tissues are complex and not easily captured by imaging devices added complexity can be introduced by datasets with underrepresentation of medical images from races and ethnicities for deep learning and limited knowledge about the regulatory framework needed for commercialization and safety of emerging artificial intelligence ai and machine learning ml technologies for medical image analysis this extended version of the workshop paper presented at the special session of the 2022 ieee 19th international symposium on biomedical imaging describes strategy and opportunities by university of california professors engaged in machine learning section i and clinical research section ii the office of science and engineering laboratories osel section iii and officials at the us fda in center for devices radiological health cdrh section iv performance evaluations of aiml models of skin rgb tissue biopsy digital pathology and lungs and kidneys magnetic resonance xray computed tomography medical images for regulatory evaluations and realworld deployment are discussed,http://arxiv.org/abs/2312.13333v1,2
this paper investigates gaps in access to and the cost of housing credit by race and ethnicity using the near universe of us mortgage applications our data contain borrower creditworthiness variables that have historically been absent from industrywide application data and that are likely to affect application approval and loan pricing we find large unconditional disparities in approval and pricing between racial and ethnic groups after conditioning on key elements of observable borrower creditworthiness these disparities are smaller but remain economically meaningful sensitivity analysis indicates that omitted factors as predictive of approvalpricing and raceethnicity as credit score can explain some of the pricing disparities but cannot explain the approval disparities taken together our results suggest that credit score income and down payment requirements significantly contribute to disparities in mortgage access and affordability but that other systemic barriers are also responsible for a large share of disparate outcomes in the mortgage market,http://arxiv.org/abs/2405.00895v1,2
large participatory biomedical studies studies that recruit individuals to join a dataset are gaining popularity and investment especially for analysis by modern ai methods because they purposively recruit participants these studies are uniquely able to address a lack of historical representation an issue that has affected many biomedical datasets in this work we define representativeness as the similarity to a target population distribution of a set of attributes and our goal is to mirror the us population across distributions of age gender race and ethnicity many participatory studies recruit at several institutions so we introduce a computational approach to adaptively allocate recruitment resources among sites to improve representativeness in simulated recruitment of 10000participant cohorts from medical centers in the star clinical research network we show that our approach yields a more representative cohort than existing baselines thus we highlight the value of computational modeling in guiding recruitment efforts,http://arxiv.org/abs/2408.01375v1,2
this study builds on person perception and human ai interaction haii theories to investigate how content and source cues specifically race ethnicity and nationality affect judgments of aigenerated content in a highstakes selfpresentation context college applications results of a preregistered experiment with a nationally representative us sample n 644 show that content heuristics such as linguistic style played a dominant role in ai detection source heuristics such as nationality also emerged as a significant factor with international students more likely to be perceived as using ai especially when their statements included aisounding features interestingly asian and hispanic applicants were more likely to be judged as ai users when labeled as domestic students suggesting interactions between racial stereotypes and ai detection ai attribution led to lower perceptions of personal statement quality and authenticity as well as negative evaluations of the applicants competence sociability morality and future success,http://arxiv.org/abs/2412.18647v1,2
public policy decision making has become more complex and complicated in recent times some authors have attributed this to the fact that public policy decision makers now have more variables to consider in every decision more than ever before others have argued that the rate of civilization globalization and information technology has made the public to be more enlightened and abreast with the activities of government and so can oppose government decisions if they are unfavourable this tends to increase government need for more and better information in order to satisfy the public consequently this paper examined the issue of fuel subsidy removal in nigeria the impact of the policy on the public as well as the country and the role marketing principles would have played if the nigerian government had taken some time to investigate what should be done how it should be done and when it should be done it also proposed a roadmap for future policies that have direct implications for the general public,http://arxiv.org/abs/2404.17551v1,2
where science design business and art meet coins13 looks at the emerging forces behind the phenomena of opensource creative entrepreneurial and social movements coins13 combines a wide range of interdisciplinary fields such as social network analysis group dynamics design and visualization information systems collective action and the psychology and sociality of collaboration the coins13 conference theme is learning from the swarm the papers in this volume explore what is relevant with regard to the innovative powers of creative and civic swarms what are the observable qualities of virtual collaboration and mobilization and how does the quest for global cooperation affect local networks,http://arxiv.org/abs/1308.1028v1,2
social tipping where minorities trigger larger populations to engage in collective action has been suggested as one key aspect in addressing contemporary global challenges here we refine granovetters widely acknowledged theoretical threshold model of collective behavior as a numerical modelling tool for understanding social tipping processes and resolve issues that so far have hindered such applications based on realworld observations and social movement theory we group the population into certain or potential actors such that in contrast to its original formulation the model predicts nontrivial final shares of acting individuals then we use a network cascade model to explain and analytically derive that previously hypothesized broad threshold distributions emerge if individuals become active via social interaction thus through intuitive parameters and low dimensionality our refined model is adaptable to explain the likelihood of engaging in collective behavior where social tipping like processes emerge as saddlenode bifurcations and hysteresis,http://arxiv.org/abs/1911.04126v2,2
there is mounting public concern over the influence that ai based systems has in our society coalitions in all sectors are acting worldwide to resist hamful applications of ai from indigenous people addressing the lack of reliable data to smart city stakeholders to students protesting the academic relationships with sex trafficker and mit donor jeffery epstein the questionable ethics and values of those heavily investing in and profiting from ai are under global scrutiny there are biased wrongful and disturbing assumptions embedded in ai algorithms that could get locked in without intervention our best human judgment is needed to contain ais harmful impact perhaps one of the greatest contributions of ai will be to make us ultimately understand how important human wisdom truly is in life on earth,http://arxiv.org/abs/2107.14052v1,2
fairnessaware learning is a novel framework for classification tasks like regular empirical risk minimization erm it aims to learn a classifier with a low error rate and at the same time for the predictions of the classifier to be independent of sensitive features such as gender religion race and ethnicity existing methods can achieve low dependencies on given samples but this is not guaranteed on unseen samples the existing fairnessaware learning algorithms employ different dependency measures and each algorithm is specifically designed for a particular one such diversity makes it difficult to theoretically analyze and compare them in this paper we propose a general framework for fairnessaware learning that uses fdivergences and that covers most of the dependency measures employed in the existing methods we introduce a way to estimate the fdivergences that allows us to give a unified analysis for the upper bound of the estimation error this bound is tighter than that of the existing convergence rate analysis of the divergence estimation with our divergence estimate we propose a fairnessaware learning algorithm and perform a theoretical analysis of its generalization error our analysis reveals that under mild assumptions and even with enforcement of fairness the generalization error of our method is osqrt1n which is the same as that of the regular erm in addition and more importantly we show that for any fdivergence the upper bound of the estimation error of the divergence is osqrt1n this indicates that our fairnessaware learning algorithm guarantees low dependencies on unseen samples for any dependency measure represented by an fdivergence,http://arxiv.org/abs/1506.07721v1,2
in this article we draw on previous reports from physics science education and womens studies to propose a more nuanced treatment of gender in physics education research per a growing body of per examines gender differences in participation performance and attitudes toward physics we have three critiques of this work 1 it does not question whether the achievements of men are the most appropriate standard 2 individual experiences and student identities are undervalued and 3 the binary model of gender is not questioned driven by these critiques we propose a conception of gender that is more uptodate with other fields and discuss genderasperformance as an extended example we also discuss work on the intersection of identities eg gender with race and ethnicity socioeconomic status lesbian gay bisexual and transgender lgbt status much of which has been conducted outside of physics within per some studies examine the intersection of gender and race and identify the lack of a single identity as a key challenge of belonging in physics acknowledging this complexity enables us to further critique what we term a binary gender deficit model this framework which is implicit in much of the genderbased per casts gender as a fixed binary trait and suggests that women are deficient in characteristics necessary to succeed alternative models of gender allow a greater range and fluidity of gender identities and highlight deficiencies in data that exclude womens experiences we suggest new investigations that diverge from this expanded gender framework in per,http://arxiv.org/abs/1507.05107v1,2
in many application areas predictive models are used to support or make important decisions there is increasing awareness that these models may contain spurious or otherwise undesirable correlations such correlations may arise from a variety of sources including batch effects systematic measurement errors or sampling bias without explicit adjustment machine learning algorithms trained using these data can produce poor outofsample predictions which propagate these undesirable correlations we propose a method to preprocess the training data producing an adjusted dataset that is statistically independent of the nuisance variables with minimum information loss we develop a conceptually simple approach for creating an adjusted dataset in highdimensional settings based on a constrained form of matrix decomposition the resulting dataset can then be used in any predictive algorithm with the guarantee that predictions will be statistically independent of the group variable we develop a scalable algorithm for implementing the method along with theory support in the form of independence guarantees and optimality the method is illustrated on some simulation examples and applied to two case studies removing machinespecific correlations from brain scan data and removing race and ethnicity information from a dataset used to predict recidivism that the motivation for removing undesirable correlations is quite different in the two applications illustrates the broad applicability of our approach,http://arxiv.org/abs/1810.08255v2,2
physics and physics education in the united states suffer from severe and in some cases worsening underrepresentation of black latinao and native people of all genders and women of all races and ethnicities this underrepresentation is a symptom with multiple causes and myriad potential solutions in this paper we describe an approach to addressing the causes of underrepresentation through physics students collective and continued education about racism sexism other dimensions of marginalization as well as models of allyship and social change specifically we focus on the efforts of undergraduate students graduate students and postdocs who are members of a studentrun diversityoriented organization in a physics department at a large selective predominantly white university with high research activity this groups education was accomplished through quarterly diversity workshops here we report on six diversity workshops that were codesigned and facilitated by the authors we describe the context motivation and goals of the workshops the theories underlying their design and implementation and their content in addition we discuss workshop attendance and suggest strategies for maintaining high attendance in the future because the details of our workshops were tailored to the specific needs and interests of a particular student organization our workshop agendas may not be widely applicable beyond our local context therefore we share our model design principles and facilitation strategies in this paper,http://arxiv.org/abs/1607.08184v2,2
this survey article assesses and compares existing critiques of current fairnessenhancing technical interventions into machine learning ml that draw from a range of noncomputing disciplines including philosophy feminist studies critical race and ethnic studies legal studies anthropology and science and technology studies it bridges epistemic divides in order to offer an interdisciplinary understanding of the possibilities and limits of hegemonic computational approaches to ml fairness for producing just outcomes for societys most marginalized the article is organized according to nine major themes of critique wherein these different fields intersect 1 how fairness in ai fairness research gets defined 2 how problems for ai systems to address get formulated 3 the impacts of abstraction on how ai tools function and its propensity to lead to technological solutionism 4 how racial classification operates within ai fairness research 5 the use of ai fairness measures to avoid regulation and engage in ethics washing 6 an absence of participatory design and democratic deliberation in ai fairness considerations 7 data collection practices that entrench bias are nonconsensual and lack transparency 8 the predatory inclusion of marginalized groups into ai systems and 9 a lack of engagement with ais longterm social and ethical outcomes drawing from these critiques the article concludes by imagining future ml fairness research directions that actively disrupt entrenched power dynamics and structural injustices in society,http://arxiv.org/abs/2205.04460v1,2
prediction of individuals race and ethnicity plays an important role in social science and public health research examples include studies of racial disparity in health and voting recently bayesian improved surname geocoding bisg which uses bayes rule to combine information from census surname files with the geocoding of an individuals residence has emerged as a leading methodology for this prediction task unfortunately bisg suffers from two census data problems that contribute to unsatisfactory predictive performance for minorities first the decennial census often contains zero counts for minority racial groups in the census blocks where some members of those groups reside second because the census surname files only include frequent names many surnames especially those of minorities are missing from the list to address the zero counts problem we introduce a fully bayesian improved surname geocoding fbisg methodology that accounts for potential measurement error in census counts by extending the naive bayesian inference of the bisg methodology to full posterior inference to address the missing surname problem we supplement the census surname data with additional data on last first and middle names taken from the voter files of six southern states where selfreported race is available our empirical validation shows that the fbisg methodology and name supplements significantly improve the accuracy of race imputation across all racial groups and especially for asians the proposed methodology together with additional name data is available via the opensource software wru,http://arxiv.org/abs/2205.06129v3,2
despite the great promise that machine learning has offered in many fields of medicine it has also raised concerns about potential biases and poor generalization across genders age distributions races and ethnicities hospitals and data acquisition equipment and protocols in the current study and in the context of three brain diseases we provide evidence which suggests that when properly trained machine learning models can generalize well across diverse conditions and do not necessarily suffer from bias specifically by using multistudy magnetic resonance imaging consortia for diagnosing alzheimers disease schizophrenia and autism spectrum disorder we find that welltrained models have a high areaunderthecurve auc on subjects across different subgroups pertaining to attributes such as gender age racial groups and different clinical studies and are unbiased under multiple fairness metrics such as demographic parity difference equalized odds difference equal opportunity difference etc we find that models that incorporate multisource data from demographic clinical genetic factors and cognitive scores are also unbiased these models have better predictive auc across subgroups than those trained only with imaging features but there are also situations when these additional features do not help,http://arxiv.org/abs/2205.13421v2,2
characterizing the cumulative burden of covid19 by raceethnicity is of the utmost importance for public health researchers and policy makers in order to design effective mitigation measures this analysis is hampered however by surveillance case data with substantial missingness in race and ethnicity covariates worse yet this missingness likely depends on the values of these missing covariates ie they are not missing at random nmar we propose a bayesian parametric model that leverages joint information on spatial variation in the disease and covariate missingness processes and can accommodate both mar and nmar missingness we show that the model is locally identifiable when the spatial distribution of the population covariates is known and observed cases can be associated with a spatial unit of observation we also use a simulation study to investigate the models finitesample performance we compare our models performance on nmar data against completecase analysis and multiple imputation mi both of which are commonly used by public health researchers when confronted with missing categorical covariates finally we model spatial variation in cumulative covid19 incidence in wayne county michigan using data from the michigan department and health and human services the analysis suggests that population relative risk estimates by race during the early part of the covid19 pandemic in michigan were understated for nonwhite residents compared to white residents when cases missing race were dropped or had these values imputed using mi,http://arxiv.org/abs/2206.08161v2,2
critical voices within and beyond the scientific community have pointed to a grave matter of concern regarding who is included in research and who is not subsequent investigations have revealed an extensive form of sampling bias across a broad range of disciplines that conduct human subjects research called weird western educated industrial rich and democratic recent work has indicated that this pattern exists within humancomputer interaction hci research as well how then does humanrobot interaction hri fare and could there be other patterns of sampling bias at play perhaps those especially relevant to this field of study we conducted a systematic review of the premier acmieee international conference on humanrobot interaction 20062022 to discover whether and how weird hri research is importantly we expanded our purview to other factors of representation highlighted by critical work on inclusion and intersectionality as potentially underreported overlooked and even marginalized factors of human diversity findings from 827 studies across 749 papers confirm that participants in hri research also tend to be drawn from weird populations moreover we find evidence of limited obscured and possible misrepresentation in participant sampling and reporting along key axes of diversity sex and gender race and ethnicity age sexuality and family configuration disability body type ideology and domain expertise we discuss methodological and ethical implications for recruitment analysis and reporting as well as the significance for hri as a base of knowledge,http://arxiv.org/abs/2304.08750v1,2
precision medicine aims to create biomedical solutions tailored to specific factors that affect disease risk and treatment responses within the population the success of the genomics era and recent widespread availability of electronic health records ehr has ushered in a new wave of genomic biobanks connected to ehr databases ehrlinked biobanks this perspective aims to discuss how race ethnicity and genetic ancestry are currently utilized to study common disease variation through genetic association studies although genetic ancestry plays a significant role in shaping the genetic landscape underlying disease risk in humans the overall risk of a disease is caused by a complex combination of environmental sociocultural and genetic factors when using ehrlinked biobanks to interrogate underlying disease etiology it is also important to be aware of how the biases associated with commonly used descentassociated concepts such as race and ethnicity can propagate to downstream analyses we intend for this resource to support researchers who perform or analyze genetic association studies in the ehrlinked biobank setting such as those involved in consortiumwide biobanking efforts we provide background on how race ethnicity and genetic ancestry play a role in current association studies highlight considerations where there is no consensus about best practices and provide transparency about the current shortcomings,http://arxiv.org/abs/2402.15696v1,2
measuring average differences in an outcome across racial or ethnic groups is a crucial first step for equity assessments but researchers often lack access to data on individuals races and ethnicities to calculate them a common solution is to impute the missing race or ethnicity labels using proxies then use those imputations to estimate the disparity conventional standard errors mischaracterize the resulting estimates uncertainty because they treat the imputation model as given and fixed instead of as an unknown object that must be estimated with uncertainty we propose a dualbootstrap approach that explicitly accounts for measurement uncertainty and thus enables more accurate statistical inference which we demonstrate via simulation in addition we adapt our approach to the commonly used bayesian improved surname geocoding bisg imputation algorithm where direct bootstrapping is infeasible because the underlying census bureau data are unavailable in simulations we find that measurement uncertainty is generally insignificant for bisg except in particular circumstances bias not variance is likely the predominant source of error we apply our method to quantify the uncertainty of prevalence estimates of common health conditions by race using data from the american family cohort,http://arxiv.org/abs/2403.06238v1,2
concerns about representation in computing within the us have driven numerous activities to broaden participation assessment of the impact of these efforts and indeed a clear assessment of the actual problem being addressed are limited by the nature of the most common data analysis which looks at the representation of each population as a percentage of the number of students graduating with a degree in computing this use of a single metric cannot adequately assess the impact of broadening participation efforts first this approach fails to account for changing demographics of the undergraduate population in terms of overall numbers and relative proportion of the federally designated gender race and ethnicity groupings a second issue is that the majority of literature on broadening participation in computing bpc reports data on gender or on raceethnicity omitting data on students intersectional identities this leads to an incorrect understanding of both the data and the challenges we face as a field in this paper we present several different approaches to tracking the impact of bpc efforts we make three recommendations 1 cohortbased analysis should be used to accurately show student engagement in computing 2 the field as a whole needs to adopt the norm of always reporting intersectional data 3 university demographic context matters when looking at how well a cs department is doing to broaden participation in computing including longitudinal analysis of university demographic shifts that impact the local demographics of computing,http://arxiv.org/abs/2403.14708v1,2
image search and retrieval tasks can perpetuate harmful stereotypes erase cultural identities and amplify social disparities current approaches to mitigate these representational harms balance the number of retrieved items across population groups defined by a small number of often binary attributes however most existing methods overlook intersectional groups determined by combinations of group attributes such as gender race and ethnicity we introduce multigroup proportional representation mpr a novel metric that measures representation across intersectional groups we develop practical methods for estimating mpr provide theoretical guarantees and propose optimization algorithms to ensure mpr in retrieval we demonstrate that existing methods optimizing for equal and proportional representation metrics may fail to promote mpr crucially our work shows that optimizing mpr yields more proportional representation across multiple intersectional groups specified by a rich function class often with minimal compromise in retrieval accuracy,http://arxiv.org/abs/2407.08571v2,2
ai fairness measurements including tests for equal treatment often take the form of disaggregated evaluations of ai systems such measurements are an important part of responsible ai operations these measurements compare system performance across demographic groups or subpopulations and typically require memberlevel demographic signals such as gender race ethnicity and location however sensitive memberlevel demographic attributes like race and ethnicity can be challenging to obtain and use due to platform choices legal constraints and cultural norms in this paper we focus on the task of enabling ai fairness measurements on raceethnicity for emphus linkedin members in a privacypreserving manner we present the privacypreserving probabilistic raceethnicity estimation ppre method for performing this task ppre combines the bayesian improved surname geocoding bisg model a sparse linkedin survey sample of selfreported demographics and privacyenhancing technologies like secure twoparty computation and differential privacy to enable meaningful fairness measurements while preserving member privacy we provide details of the ppre method and its privacy guarantees we then illustrate sample measurement operations we conclude with a review of open research and engineering challenges for expanding our privacypreserving fairness measurement capabilities,http://arxiv.org/abs/2409.04652v2,2
there have been growing concerns around highstake applications that rely on models trained with biased data which consequently produce biased predictions often harming the most vulnerable in particular biased medical data could cause healthrelated applications and recommender systems to create outputs that jeopardize patient care and widen disparities in health outcomes a recent framework titled fairness via ai posits that instead of attempting to correct model biases researchers must focus on their root causes by using ai to debias data inspired by this framework we tackle bias detection in medical curricula using nlp models including llms and evaluate them on a gold standard dataset containing 4105 excerpts annotated by medical experts for bias from a large corpus we build on previous work by coauthors which augments the set of negative samples with nonannotated text containing social identifier terms however some of these terms especially those related to race and ethnicity can carry different meanings eg white matter of spinal cord to address this issue we propose the use of word sense disambiguation models to refine dataset quality by removing irrelevant sentences we then evaluate finetuned variations of bert models as well as gpt models with zero and fewshot prompting we found llms considered sota on many nlp tasks unsuitable for bias detection while finetuned bert models generally perform well across all evaluated metrics,http://arxiv.org/abs/2409.07424v1,2
recent progress in generative ai especially diffusion models has demonstrated significant utility in texttoimage synthesis particularly in healthcare these models offer immense potential in generating synthetic datasets and training medical students however despite these strong performances it remains uncertain if the image generation quality is consistent across different demographic subgroups to address this critical concern we present the first comprehensive study on the fairness of medical texttoimage diffusion models our extensive evaluations of the popular stable diffusion model reveal significant disparities across gender race and ethnicity to mitigate these biases we introduce fairdiffusion an equityaware latent diffusion model that enhances fairness in both image generation quality as well as the semantic correlation of clinical features in addition we also design and curate fairgenmed the first dataset for studying the fairness of medical generative models complementing this effort we further evaluate fairdiffusion on two widelyused external medical datasets ham10000 dermatoscopic images and chexpert chest xrays to demonstrate fairdiffusions effectiveness in addressing fairness concerns across diverse medical imaging modalities together fairdiffusion and fairgenmed significantly advance research in fair generative learning promoting equitable benefits of generative ai in healthcare,http://arxiv.org/abs/2412.20374v1,2
humanitarian challenges including natural disasters food insecurity climate change racial and gender violence environmental crises the covid19 coronavirus pandemic human rights violations and forced displacements disproportionately impact vulnerable communities worldwide according to un ocha 235 million people will require humanitarian assistance in 2021 despite these growing perils there remains a notable paucity of data science research to scientifically inform equitable public policy decisions for improving the livelihood of atrisk populations scattered data science efforts exist to address these challenges but they remain isolated from practice and prone to algorithmic harms concerning lack of privacy fairness interpretability accountability transparency and ethics biases in datadriven methods carry the risk of amplifying inequalities in highstakes policy decisions that impact the livelihood of millions of people consequently proclaimed benefits of datadriven innovations remain inaccessible to policymakers practitioners and marginalized communities at the core of humanitarian actions and global development to help fill this gap we propose the datadriven humanitarian mapping research program which focuses on developing novel data science methodologies that harness humanmachine intelligence for highstakes public policy and resilience planning the proceedings of the 2nd datadriven humanitarian mapping workshop at the 27th acm sigkdd conference on knowledge discovery data mining august 15th 2021,http://arxiv.org/abs/2109.00100v4,2
humanitarian challenges including natural disasters food insecurity climate change racial and gender violence environmental crises the covid19 coronavirus pandemic human rights violations and forced displacements disproportionately impact vulnerable communities worldwide according to un ocha 235 million people will require humanitarian assistance in 2021 despite these growing perils there remains a notable paucity of data science research to scientifically inform equitable public policy decisions for improving the livelihood of atrisk populations scattered data science efforts exist to address these challenges but they remain isolated from practice and prone to algorithmic harms concerning lack of privacy fairness interpretability accountability transparency and ethics biases in datadriven methods carry the risk of amplifying inequalities in highstakes policy decisions that impact the livelihood of millions of people consequently proclaimed benefits of datadriven innovations remain inaccessible to policymakers practitioners and marginalized communities at the core of humanitarian actions and global development to help fill this gap we propose the datadriven humanitarian mapping research program which focuses on developing novel data science methodologies that harness humanmachine intelligence for highstakes public policy and resilience planning the proceedings of the 1st datadriven humanitarian mapping workshop at the 26th acm sigkdd conference on knowledge discovery data mining august 24th 2020,http://arxiv.org/abs/2109.00435v3,2
artificial intelligence ai can potentially transform global health but algorithmic bias can exacerbate social inequities and disparity trustworthy ai entails the intentional design to ensure equity and mitigate potential biases to advance trustworthy ai in global health we convened a workshop on fairness in machine intelligence for global health fairmi4gh the event brought together a global mix of experts from various disciplines community health practitioners policymakers and more topics covered included managing ai bias in sociotechnical systems ais potential impacts on global health and balancing data privacy with transparency panel discussions examined the cultural political and ethical dimensions of ai in global health fairmi4gh aimed to stimulate dialogue facilitate knowledge transfer and spark innovative solutions drawing from nists ai risk management framework it provided suggestions for handling ai risks and biases the need to mitigate data biases from the research design stage adopt a humancentered approach and advocate for ai transparency was recognized challenges such as updating legal frameworks managing crossborder data sharing and motivating developers to reduce bias were acknowledged the event emphasized the necessity of diverse viewpoints and multidimensional dialogue for creating a fair and ethical ai framework for equitable global health,http://arxiv.org/abs/2309.05088v1,2
social media platforms hold valuable insights yet extracting essential information can be challenging traditional topdown approaches often struggle to capture critical signals in rapidly changing events as global events evolve swiftly social media narratives including instances of disinformation become significant sources of insights to address the need for an inductive strategy we explore a niche social media platform gab and an established messaging service telegram to develop methodologies applicable on a broader scale this study investigates narrative evolution on these platforms using quantitative corpusbased discourse analysis techniques our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise allowing for useful and actionable insights the paper details the technical and methodological aspects of gathering and preprocessing gab and telegram data for a keyness log ratio metric analysis identifying crucial nouns and verbs for deeper exploration empirically this approach is applied to a case study of a well defined event that had global impact the 2023 wagner mutiny the main findings are 1 the time line can be deconstructed to provide useful data features allowing for improved interpretation 2 a methodology is applied which provides a basis for generalization the key contribution is an approach that in some cases provides the ability to capture the dynamic narrative shifts over time with elevated confidence the approach can augment nearrealtime assessment of key social movements allowing for informed governance choices this research is important because it lays out a useful methodology for time series relevant infoculling which can enable proactive modes for positive social engagement,http://arxiv.org/abs/2403.07090v1,2
starting with the neobayesian revival of the 1950s many statisticians argued that it was inappropriate to use bayesian methods and in particular subjective bayesian methods in governmental and public policy settings because of their reliance upon prior distributions but the bayesian framework often provides the primary way to respond to questions raised in these settings and the numbers and diversity of bayesian applications have grown dramatically in recent years through a series of examples both historical and recent we argue that bayesian approaches with formal and informal assessments of priors and likelihood functions are well accepted and should become the norm in public settings our examples include censustaking and small area estimation us election night forecasting studies reported to the us food and drug administration assessing global climate change and measuring potential declines in disability among the elderly,http://arxiv.org/abs/1108.2177v1,2
this research analyzes the effects of us science and technology policy on the technological performance of organizations in a global strategic alliance network during the mid1980s the us semiconductor industry appeared to be collapsing industry leaders and policymakers moved to support and protect us firms by creating a program called sematech while many scholars regard sematech as a success how the program succeeded remains unclear this study recontextualizes sematech as a network administrative organization which lowered cooperation costs and enhanced resource combination for innovation at the cutting edge this study combines network analysis and longitudinal regression techniques to test the effects of public policy on organizational network position and technological performance in an unbalanced panel of semiconductor firms between 1986 and 2001 this research suggests governments might achieve policy through interorganizational innovations aimed at the development and administration of robust governance networks,http://arxiv.org/abs/1903.07730v2,2
classifying moral values in usergenerated text from social media is critical in understanding community cultures and interpreting user behaviors of social movements moral values and language usage can change across the social movements however text classifiers are usually trained in source domains of existing social movements and tested in target domains of new social issues without considering the variations in this study we examine domain shifts of moral values and language usage quantify the effects of domain shifts on the morality classification task and propose a neural adaptation framework via instance weighting to improve crossdomain classification tasks the quantification analysis suggests a strong correlation between morality shifts language usage and classification performance we evaluate the neural adaptation framework on a public twitter data across 7 social movements and gain classification improvements up to 121 finally we release a new data of the covid19 vaccine labeled with moral values and evaluate our approach on the new target domain for the case study of the covid19 vaccine our adaptation framework achieves up to 526 improvements over neural baselines,http://arxiv.org/abs/2204.07603v2,2
social movements are dominated by storytelling as narratives play a key role in how communities involved in these movements shape their identities thus recognizing the accepted narratives of different communities is central to understanding social movements in this context journalists face the challenge of making sense of these emerging narratives in social media when they seek to report social protests thus they would benefit from support tools that allow them to identify and explore such narratives in this work we propose a narrative extraction algorithm from social media that incorporates the concept of community acceptance using our method we study the 2021 cuban protests and characterize five relevant communities the extracted narratives differ in both structure and content across communities our work has implications in the study of social movements intelligence analysis computational journalism and misinformation research,http://arxiv.org/abs/2208.04465v1,2
many socioeconomic studies have been carried out to explain the phenomenon of gentrification although results of these works shed light on the process around this phenomenon a perspective which focuses on the relationship between city form and gentrification is still missing with this paper we try to address this gap by studying and comparing through classic methods of mathematical statistics morphological features of five london gentrified neighbourhoods outcomes confirm that areas which have undergone gentrification display similar and recognizable morphological patterns in terms of urban type and geographical location of main and local roads as well as businesses these initial results confirm findings from previous research in urban sociology and highlight the role of urban form in contributing to shape dynamics of nonspatial nature in cities,http://arxiv.org/abs/1411.2984v1,2
the spatial distribution of people exhibits clustering across a wide range of scales from household sim 102 km to continental sim 104 km scales empirical data indicates simple powerlaw scalings for the size distribution of cities known as zipfs law and the population density fluctuations as a function of scale using techniques from random field theory and statistical physics we show that these power laws are fundamentally a consequence of the scalefree spatial clustering of human populations and the fact that humans inhabit a twodimensional surface in this sense the symmetries of scale invariance in two spatial dimensions are intimately connected to urban sociology we test our theory by empirically measuring the power spectrum of population density fluctuations and show that the logarithmic slope alpha 204 pm 009 in excellent agreement with our theoretical prediction alpha 2 the model enables the analytic computation of many new predictions by importing the mathematical formalism of random fields,http://arxiv.org/abs/1501.00738v3,2
in this paper we use analysis on graphs to study quantitative measures of segregation we focus on a classical statistic from the geography and urban sociology literature known as morans i which in our language is a score associated to a realvalued function on a graph computed with respect to a spatial weight matrix such as the adjacency matrix associated to the geographic units that tile a city our results characterizing the extremal behavior of i illustrate the important role of the underlying graph structure especially the degree distribution in interpreting the score in addition to the standard spatial weight matrices encoding unit adjacency we consider the laplacian l and a doublystochastic approximation m these alternatives allow us to connect i to ideas from fourier analysis and random walks we offer illustrations of our theoretical results with a mix of stylized synthetic examples and real geographicdemographic data,http://arxiv.org/abs/2112.10708v2,2
developers of computer vision algorithms outsource some of the labor involved in annotating training data through business process outsourcing companies and crowdsourcing platforms many data annotators are situated in the global south and are considered independent contractors this paper focuses on the experiences of argentinian and venezuelan annotation workers through qualitative methods we explore the discourses encoded in the task instructions that these workers follow to annotate computer vision datasets our preliminary findings indicate that annotation instructions reflect worldviews imposed on workers and through their labor on datasets moreover we observe that forprofit goals drive task instructions and that managers and algorithms make sure annotations are done according to requesters commands this configuration presents a form of commodified labor that perpetuates power asymmetries while reinforcing social inequalities and is compelled to reproduce them into datasets and subsequently in computer vision systems,http://arxiv.org/abs/2105.10990v1,2
in the modern world our cities and societies face several technological and societal challenges such as rapid urbanization global warming climate change the digital divide and social inequalities increasing the need for more sustainable cities and societies addressing these challenges requires a multifaceted approach involving all the stakeholders sustainable planning efficient resource management innovative solutions and modern technologies like other modern technologies social media informatics also plays its part in developing more sustainable and resilient cities and societies despite its limitations social media informatics has proven very effective in various sustainable cities and society applications in this paper we review and analyze the role of social media informatics in sustainable cities and society by providing a detailed overview of its applications associated challenges and potential solutions this work is expected to provide a baseline for future research in the domain,http://arxiv.org/abs/2412.03600v2,2
increasingly there is wellgrounded concern that through perpetual scalingup of computation power and data current deep learning techniques will create highly capable artificial intelligence that could pursue goals in a manner that is not aligned with human values in turn such ai could have the potential of leading to a scenario in which there is serious globalscale damage to human wellbeing against this backdrop a number of researchers and public policy professionals have been developing ideas about how to govern ai in a manner that reduces the chances that it could lead to a global catastrophe the jurisdictional focus of a vast majority of their assessments so far has been the united states china and europe that preference seems to reveal an assumption underlying most of the work in this field that global south countries can only have a marginal role in attempts to govern ai development from a global catastrophic risk focused perspective our paper sets out to undermine this assumption we argue that global south countries like india and singapore and specific coalitions could in fact be fairly consequential in the global catastrophic riskfocused governance of ai we support our position using 4 key claims 3 are constructed out of the current ways in which advanced foundational ai models are built and used while one is constructed on the strategic roles that global south countries and coalitions have historically played in the design and use of multilateral rules and institutions as each claim is elaborated we also suggest some ways through which global south countries can play a positive role in designing strengthening and operationalizing global catastrophic riskfocused ai governance,http://arxiv.org/abs/2312.04616v1,2
public policies that supply public goods especially those involve collaboration by limiting individual liberty always give rise to controversies over governance legitimacy multiagent reinforcement learning marl methods are appropriate for supporting the legitimacy of the public policies that supply public goods at the cost of individual interests among these policies the interregional collaborative pandemic control is a prominent example which has become much more important for an increasingly interconnected world facing a global pandemic like covid19 different patterns of collaborative strategies have been observed among different systems of regions yet it lacks an analytical process to reason for the legitimacy of those strategies in this paper we use the interregional collaboration for pandemic control as an example to demonstrate the necessity of marl in reasoning and thereby legitimizing policies enforcing such interregional collaboration experimental results in an exemplary environment show that our marl approach is able to demonstrate the effectiveness and necessity of restrictions on individual liberty for collaborative supply of public goods different optimal policies are learned by our marl agents under different collaboration levels which change in an interpretable pattern of collaboration that helps to balance the losses suffered by regions of different types and consequently promotes the overall welfare meanwhile policies learned with higher collaboration levels yield higher global rewards which illustrates the benefit of and thus provides a novel justification for the legitimacy of promoting interregional collaboration therefore our method shows the capability of marl in computationally modeling and supporting the theory of calculus of consent developed by nobel prize winner j m buchanan,http://arxiv.org/abs/2111.10627v1,2
objective amyotrophic lateral sclerosis als is a rare disease but is also one of the most common motor neuron diseases and people of all races and ethnic backgrounds are affected there is currently no cure brain computer interfaces bcis can establish a communication channel directly between the brain and an external device by recognizing brain activities that reflect user intent therefore this technology could help als patients in promoting functional independence through bcibased speller systems and motor assistive devices methods in this paper two kinds of erpbased speller systems were tested on 18 als patients to 1 assess performance when they spelled 42 characters online continuously without a break and 2 to compare performance between a matrixbased speller paradigm msp mean visual angle 6 degree and a new speller paradigm that used a larger visual angle called the large visual angle speller paradigm lsp mean visual angle 8 degree results although results showed that there were no significant differences between the two paradigms in accuracy trend over continuous use p005 the fatigue during the lsp condition was significantly lower than that of msp p005 results also showed that continuous use slightly reduced the performance of this erpbased bci conclusion 15 subjects obtained higher than 80 feedback accuracy online output accuracy and 9 subjects obtained higher than 90 feedback accuracy in one of the two paradigms thus validating the bci approaches in this study significance most als subjects in this study could spell effectively after continuous use of an erpbased bci the new lsp display may be easier for subjects to use resulting in lower fatigue,http://arxiv.org/abs/1706.09089v1,2
autoregressive language models which use deep learning to produce humanlike texts have become increasingly widespread such models are powering popular virtual assistants in areas like smart health finance and autonomous driving while the parameters of these large language models are improving concerns persist that these models might not work equally for all subgroups in society despite growing discussions of ai fairness across disciplines there lacks systemic metrics to assess what equity means in dialogue systems and how to engage different populations in the assessment loop grounded in theories of deliberative democracy and science and technology studies this paper proposes an analytical framework for unpacking the meaning of equity in humanai dialogues using this framework we conducted an auditing study to examine how gpt3 responded to different subpopulations on crucial science and social topics climate change and the black lives matter blm movement our corpus consists of over 20000 rounds of dialogues between gpt3 and 3290 individuals who vary in gender race and ethnicity education level english as a first language and opinions toward the issues we found a substantively worse user experience with gpt3 among the opinion and the education minority subpopulations however these two groups achieved the largest knowledge gain changing attitudes toward supporting blm and climate change efforts after the chat we traced these user experience divides to conversational differences and found that gpt3 used more negative expressions when it responded to the education and opinion minority groups compared to its responses to the majority groups we discuss the implications of our findings for a deliberative conversational ai system that centralizes diversity equity and inclusion,http://arxiv.org/abs/2209.13627v2,2
transportation is one of the most pervasive sources of community noise in this study we used a spatiallyresolved model of transportationrelated noise with established transportation noise exposureresponse functions to estimate the population highly annoyed ha due to aviation road and railway traffic sources in the united states additionally we employed the use of the fair share ratio to assess raceethnicity disparities in traffic noise exposures our results estimate that in 2020 78 million 24 individuals were highly annoyed by aviation noise while 52 million 16 and 79 million 24 people were highly annoyed by rail and roadway noise respectively across the us the fair share ratio revealed that nonhispanic asian black nhpi and other and hispanic populations were disproportionally highly annoyed by transportation noise nationwide notably hispanic populations experienced the greatest share of high annoyance from aviation noise 169 times their population share nonhispanic black populations experienced the greatest share of high annoyance from railway noise 148 times their population share nonhispanic asian populations experienced the greatest share of high annoyance from roadway noise 151 times their population share analyses at the state and urban area levels further highlighted varying disparities in transportation noise exposure and annoyance across different race ethnicity groups but still suggested that nonhispanic white populations were less annoyed by all sources of transportation noise compared to nonwhite populations our findings indicate widespread presence of transportation noise annoyance across the us and emphasize the need for targeted sourcespecific noise mitigation strategies and policies to minimize the disproportionate impact of transportation noise in the us,http://arxiv.org/abs/2307.16837v2,2
genderbased violence gbv is a humangenerated crisis existing in various forms including offline via physical and sexual violence and now online via harassment and trolling while studying social media campaigns for different domains such as public health natural crises etc has received attention in the literature such studies for gbv are still in nascent form the dynamics of campaigns responding to curb this crisis could benefit from systematic investigation to our knowledge this is the first study to examine such public campaigns involving social media by organizations operating at the local national and global levels with an eye to answering the following research questions 1 how do members of one campaign community engage with other campaign communities 2 how do demographic variables such as gender effect campaign engagement in light of given regional crime statistics 3 is there any coordination among organizational users for campaigns with similar underlying social causes,http://arxiv.org/abs/1608.01648v1,2
educational disparities within the dominican republic dr have longstanding origins rooted in economic political and social inequity addressing these challenges has necessarily called for capacity building with respect to educational materials highquality instruction and structural resourcing generative ai tools like chatgpt have begun to pique the interest of dominican educators due to their perceived potential to bridge these educational gaps however a substantial body of ai fairness literature has documented ways ai disproportionately reinforces power dynamics reflective of jurisdictions driving ai development and deployment policies collectively termed the ai global north as such indiscriminate adoption of this technology for dr education even in part risks perpetuating forms of digital coloniality therefore this paper centers embracing aifacilitated educational reform by critically examining how aidriven tools like chatgpt in dr education may replicate facets of digital colonialism we provide a concise overview of 20thcentury dominican education reforms following the 1916 us occupation then we employ identified neocolonial aspects historically shaping dominican education to interrogate the perceived advantages of chatgpt for contemporary dominican education as outlined by a dominican scholar this work invites ai global north south developers stakeholders and dominican leaders alike to exercise a relational contextualization of datacentric epistemologies like chatgpt to reap its transformative benefits while remaining vigilant of safeguarding dominican digital sovereignty,http://arxiv.org/abs/2310.17533v2,2
from the urbanists perspective the everyday experience of young people as an underrepresented group in the design of public spaces includes tactics they use to challenge the strategies which rule over urban spaces in this regard youth led social movements are a set of collective tactics which groups of young people use to resist power structures social informational streams have revolutionized the way youth organize and mobilize for social movements throughout the world especially in urban areas however just like public spaces these algorithm based platforms have been developed with a great power imbalance between the developers and users which results in the creation of non inclusive social informational streams for young activists social activism grows agency and confidence in youth which is critical to their development this paper employs a youth centric lens which is used in designing public spaces for designing algorithmic spaces that can improve bottom up youth led movements by reviewing the structure of these spaces and how young people interact with these structures in the different cultural contexts of iran and the us we propose a humanistic approach to designing social informational streams which can enhance youth activism,http://arxiv.org/abs/2303.07541v2,2
to test a hypothesized fasterthanglobal sealevel acceleration along the midatlantic united states i construct a gaussian process model that decomposes tide gauge data into shortterm variability and longerterm trends and into globallycoherent regionallycoherent and local components while tide gauge records indicate a fasterthanglobal increase in the rate of midatlantic us sealevel rise beginning 1975 this acceleration could reflect either the start of a longterm trend or ocean dynamic variability the acceleration will need to continue for 2 decades before the rate of increase of the sealevel gradient between the midatlantic and southeastern us can be judged as very likely unprecedented by 20th century standards however the gradient is correlated with the atlantic multidecadal oscillation north atlantic oscillation and gulf stream north wall indices all of which are currently within the range of past variability,http://arxiv.org/abs/1304.5407v3,2
social movements use social computing systems to complement offline mobilizations but prior literature has focused almost exclusively on movement actors use of social media in this paper we analyze participation and attention to topics connected with the black lives matter movement in the english language version of wikipedia between 2014 and 2016 our results point to the use of wikipedia to 1 intensively document and connect historical and contemporary events 2 collaboratively migrate activity to support coverage of new events and 3 dynamically reappraise preexisting knowledge in the aftermath of new events these findings reveal patterns of behavior that complement theories of collective memory and collective action and help explain how social computing systems can encode and retrieve knowledge about social movements as they unfold,http://arxiv.org/abs/1611.01257v1,2
in recent years social media changed the way individuals participate in social movements while activists demonstrate on the street to fight for a public goal members of specific movements can also act collective online thus different aspects might influence the formation of collective identity and therefore drive collective action on social media this study combines the perspectives of social identity and identity theory in order to examine how members of an opinionbased group contribute to the collective groupsocial identity formation and therefore to collective action to this end we applied automated text classification techniques to instagram communication related to the social movement fridays for future analysing 1137 comments showed that individuals mainly express group cohesion and emotional attachment rather than solidarity by commenting on instagram this study further presents a proposed model of collective groupsocial identity of collective action succeeding research aims at enhancing the classification and testing the model,http://arxiv.org/abs/1912.05123v1,2
social media has emerged as a cornerstone of social movements wielding significant influence in driving societal change simulating the response of the public and forecasting the potential impact has become increasingly important however existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants in this paper we introduce a hybrid framework hisim for social media user simulation wherein users are categorized into two types core users are driven by large language models while numerous ordinary users are modeled by deductive agentbased models we further construct a twitterlike environment to replicate their response dynamics following trigger events subsequently we develop a multifaceted benchmark somosimubench for evaluation and conduct comprehensive experiments across realworld datasets experimental results demonstrate the effectiveness and flexibility of our method,http://arxiv.org/abs/2402.16333v2,2
social media enables activists to directly communicate with the public and provides a space for movement leaders participants bystanders and opponents to collectively construct and contest narratives focusing on twitter messages from social movements surrounding three issues in 20182019 guns immigration and lgbtq rights we create a codebook annotated dataset and computational models to detect diagnostic problem identification and attribution prognostic proposed solutions and tactics and motivational calls to action framing strategies we conduct an indepth unsupervised linguistic analysis of each framing strategy and uncover crossmovement similarities in associations between framing and linguistic features such as pronouns and deontic modal verbs finally we compare framing strategies across issues and other social cultural and interactional contexts for example we show that diagnostic framing is more common in replies than original broadcast posts and that social movement organizations focus much more on prognostic and motivational framing than journalists and ordinary citizens,http://arxiv.org/abs/2406.13820v1,2
current social and activist movements find the opportunity in social media to effectively impact on the agenda of governing bodies and create global perceptions it is often claimed content related to the social and activist movements is online to be accessed supported or disputed and distributed from virtually anywhere at any time in the public sphere of the internet this activity allows the enlargement of social movements and would increase the empowerment in the concerned communities the aim of this explorative study is to assess whether the temporal evolution of the normalised web distance nwd as defined by cilibrasi vitanyi 2007 between identifying terms concerning this activism could be used to measure the progress or decline of social empowerment through the internet the nwd relies on the page count number of single and joint queries which in our study have been registered using a freely available web browser eg google search providing a time search window for temporal query results to explore this metadata technique we introduce the case of a perceived wirikuta online movement which originated in mexico with the aim to protect the huichols sacred land and water resources from open mining projects for silver ore we conducted a small scale internet study relating the key terms wirikuta huichol and wixarika and their cooccurrence with seven positive qualifiers eg sacred land five negative qualifiers eg violence and one neutral qualifier table over time annually from 1994 till 2013 we confirm close semantic clustering over time of traditional indigeneous identity terms of the huichol and observe a slight convergence of key terms to mines and less pronounced to sacred land and a divergence with respect to ancestors indicating a complex image of a tendency of empowerment,http://arxiv.org/abs/1406.5946v1,2
different people and cultures associate different emotional states to different parts and spaces of cities these vary according to individuals their cultures and also to the time of day day of week season special occasions and more recurring patterns may occur in correspondence of the places in which people work study entertain themselves consume relate wait or just take a break what can we learn from these patterns trying to find possible answers to this question passes through the possibility to visualize and represent the configurations of emotional expressions in urban spaces across time geography theme cultures and other dimensions we have developed ways in which it is possible to harvest peoples geolocated or geolocatable emotional expressions from major social networks and to visualize them according to a variety of different modalities in this paper we will present a series of these types of visualizations and the ways in which they can be used to gain better understandings of these emotional patterns as they arise from points of view which derive from anthropology urbanism sociology politics and also arts and poetics the paper will focus on the ways in which the data is harvested from different social networks then categorized and annotated with metadata describing the emotional states the languages in which people express themselves the geographic locations the themes expressed a methodology for representing this information across a variety of domains time space emotion theme will then be presented in detail a reflection on possible usage cases for anthropology urbanism policymaking arts and design will end the contribution as well as the description of series of open issues and the indication of possible nextsteps for research,http://arxiv.org/abs/1412.5583v1,2
collective efficacy the capacity of communities to exert social control toward the realization of their shared goals is a foundational concept in the urban sociology and neighborhood effects literature traditionally empirical studies of collective efficacy use large sample surveys to estimate collective efficacy of different neighborhoods within an urban setting such studies have demonstrated an association between collective efficacy and local variation in community violence educational achievement and health unlike traditional collective efficacy measurement strategies the adolescent health and development in context ahdc study implemented a new approach obtaining spatiallyreferenced placebased ratings of collective efficacy from a representative sample of individuals residing in columbus oh in this paper we introduce a novel nonstationary spatial model for interpolation of the ahdc collective efficacy ratings across the study area which leverages administrative data on land use our constructive model specification strategy involves dimension expansion of a latent spatial process and the use of a filter defined by the landuse partition of the study region to connect the latent multivariate spatial process to the observed ordinal ratings of collective efficacy careful consideration is given to the issues of parameter identifiability computational efficiency of an mcmc algorithm for model fitting and finescale spatial prediction of collective efficacy,http://arxiv.org/abs/2206.04781v2,2
existing open vocabulary detection ovd models exhibit a number of challenges they often struggle with semantic consistency across diverse inputs and are often sensitive to slight variations in input phrasing leading to inconsistent performance the calibration of their predictive confidence especially in complex multilabel scenarios remains suboptimal frequently resulting in overconfident predictions that do not accurately reflect their context understanding to understand these limitations multilabel detection benchmarks are needed a particularly challenging domain for such benchmarking is social activities due to the lack of multilabel benchmarks for social interactions in this work we present elsa evaluating localization of social activities elsa draws on theoretical frameworks in urban sociology and design and uses inthewild streetlevel imagery where the size of groups and the types of activities vary significantly elsa includes more than 900 manually annotated images with more than 4300 multilabeled bounding boxes for individual and group activities we introduce a novel confidence score computation method nlse and a novel dynamic box aggregation dba algorithm to assess semantic consistency in overlapping predictions we report our results on the widelyused sota models grounding dino detic owl and mdetr our evaluation protocol considers semantic stability and localization accuracy and further exposes the limitations of existing approaches,http://arxiv.org/abs/2406.01551v2,2
modern societies have adopted governmentissued fiat currencies many of which exist today mainly in the form of digits in credit and bank accounts fiat currencies are controlled by central banks for economic stimulation and stabilization boomandbust cycles are created the volatility of the cycle has become increasingly extreme social inequality due to the concentration of wealth is prevalent worldwide as such restoring sound money which provides stored value over time has become a pressing issue currently cryptocurrencies such as bitcoin are in their infancy and may someday qualify as sound money bitcoin today is considered as a digital asset for storing value but bitcoin has problems the first issue of the current bitcoin network is its high energy consumption consensus mechanism the second is the cryptographic primitives which are unsafe against postquantum pq attacks we aim to propose green bitcoin which addresses both issues to save energy in consensus mechanism we introduce a postquantum secure selfelection verifiable cointoss function and novel pq secure proofofcomputation primitives it is expected to reduce the rate of energy consumption more than 90 percent of the current bitcoin network the elliptic curve cryptography will be replaced with pqsafe versions the green bitcoin protocol will help bitcoin evolve into a postquantum secure network,http://arxiv.org/abs/2212.13986v1,2
it is arguable whether history is made by great men and women or vice versa but undoubtably social connections shape history analysing wikipedia a global collective memory place we aim to understand how social links are recorded across cultures starting with the set of biographies in the english wikipedia we focus on the networks of links between these biographical articles on the 15 largest language wikipedias we detect the most central characters in these networks and point out culturerelated peculiarities furthermore we reveal remarkable similarities between distinct groups of language wikipedias and highlight the shared knowledge about connections between persons across cultures,http://arxiv.org/abs/1204.3799v2,2
minority groups have been using social media to organize social movements that create profound social impacts black lives matter blm and stop asian hate sah are two successful social movements that have spread on twitter that promote protests and activities against racism and increase the publics awareness of other social challenges that minority groups face however previous studies have mostly conducted qualitative analyses of tweets or interviews with users which may not comprehensively and validly represent all tweets very few studies have explored the twitter topics within blm and sah dialogs in a rigorous quantified and datacentered approach therefore in this research we adopted a mixedmethods approach to comprehensively analyze blm and sah twitter topics we implemented 1 the latent dirichlet allocation model to understand the top highlevel words and topics and 2 opencoding analysis to identify specific themes across the tweets we collected more than one million tweets with the blacklivesmatter and stopasianhate hashtags and compared their topics our findings revealed that the tweets discussed a variety of influential topics in depth and social justice social movements and emotional sentiments were common topics in both movements though with unique subtopics for each movement our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of ai ethics and society in general,http://arxiv.org/abs/2205.14725v2,2
with our knowledge of the universe we have sent men to the moon we know microscopic details of objects around us and within us and yet we know relatively little about how our society works and how it reacts to changes brought upon it humankind is now facing serious crises for which we must develop new ways to tackle the global challenges of humanity in the 21st century with connectivity between people rapidly increasing we are now able to exploit information and communication technologies to achieve major breakthroughs that go beyond the stepwise improvements in other areas the need of a socioeconomic knowledge collider was first pointed out in the oecd global science forum on applications of complexity science for public policy in erice from october 5 to 7 2008 since then many scientists have called for a largescale ictbased research initiative on technosocialeconomic environmental issues sometimes phrased as a manhattan apollo or cernlike project to study the way our living planet works in a social dimension due to the connotations we use the term knowledge accelerator here,http://arxiv.org/abs/1004.4969v2,2
covid19 pandemic represents an unprecedented global health crisis in the last 100 years its economic social and health impact continues to grow and is likely to end up as one of the worst global disasters since the 1918 pandemic and the world wars mathematical models have played an important role in the ongoing crisis they have been used to inform public policies and have been instrumental in many of the social distancing measures that were instituted worldwide in this article we review some of the important mathematical models used to support the ongoing planning and response efforts these models differ in their use their mathematical form and their scope,http://arxiv.org/abs/2009.10014v1,2
to realize the potential benefits and mitigate potential risks of ai it is necessary to develop a framework of governance that conforms to ethics and fundamental human values although several organizations have issued guidelines and ethical frameworks for trustworthy ai without a mediating governance structure these ethical principles will not translate into practice in this paper we propose a multilevel governance approach that involves three groups of interdependent stakeholders governments corporations and citizens we examine their interrelationships through dimensions of trust such as competence integrity and benevolence the levels of governance combined with the dimensions of trust in ai provide practical insights that can be used to further enhance user experiences and inform public policy related to ai,http://arxiv.org/abs/2307.03198v2,2
this paper focuses on explaining changes over time in globallysourced annual temporal data with the specific objective of identifying pivotal factors that contribute to these temporal shifts leveraging such analytical frameworks can yield transformative impacts including the informed refinement of public policy and the identification of key drivers affecting a countrys economic evolution we employ local interpretable modelagnostic explanations lime to shed light on national happiness indices economic freedom and population metrics spanning variable time frames acknowledging the presence of missing values we employ three imputation approaches to generate robust multivariate timeseries datasets apt for limes input requirements our methodologys efficacy is substantiated through a series of empirical evaluations involving multiple datasets these evaluations include comparative analyses against random feature selection correlation with realworld events as elucidated by lime and validation through individual conditional expectation ice plots a stateoftheart technique proficient in feature importance detection,http://arxiv.org/abs/2404.11874v1,2
terrorism instills fear in the minds of people and takes away the freedom of individuals to act as they will terrorism has turned out to be an international menace today here we study the terrorist attack incidents which occurred in the last halfcentury across the globe from the open source global terrorism database and develop a view on their spatiotemporal dynamics we construct a complex network of global terrorism and study its growth dynamics along with the statistical properties of the antisocial network which are quite intriguing normally each nation pursues its own vision of international security based upon its mandate and particular notions of politics and its policies to counter the threat of terrorism that could naturally include the use of tactical measures and strategic negotiations or even physical power we study the network resilience against targeted attacks and random failures which could guide the counterterrorist outfits in designing strategies to fight terrorism we then use a disparity filter method to isolate backbone of the network and identify the terror hubs and vulnerable motifs of global terrorism we also examine evolution of the hubs and motifs in a few exemplary cases like afghanistan colombia india israel pakistan and the united kingdom the dynamics of the terror hubs and the vulnerable motifs that we discover in the network backbone turn out to be very significant and may provide deep insight on their formations and spreading and thereby help in contending terrorism or framing public policies that can check their spread,http://arxiv.org/abs/1802.01147v2,2
the past century ended with an unexpected explosion of information and communication technologies ict both in planningmanaging public policies and in exchanging knowledge however the extent to which ictbased tools increase the level of public knowledge or help decision makers is still uncertain although indirectly the overload of unfiltered webbased information seems able to hamper the knowledge growth of people particularly in some developing communities whereas decision support systems dss and geographical information systems gis prove to be ineffective if managed by unskilled planning bodies given such warns this paper outlines how the different social and cultural awareness of local communities can affect the outcomes of ictbased tools it further explores the impacts of ictbased tools on community development and spatial planning emphasizing the role of proper literacy and education for effective management,http://arxiv.org/abs/cs/0312021v1,2
the covid19 pandemic presented an unprecedented global public health emergency and concomitantly an unparalleled opportunity to investigate public responses to adverse social conditions the widespread ability to post messages to social media platforms provided an invaluable outlet for such an outpouring of public sentiment including not only expressions of social solidarity but also the spread of misinformation and misconceptions around the effect and potential risks of the pandemic this archive of message content therefore represents a key resource in understanding public responses to health crises analysis of which could help to inform public policy interventions to better respond to similar events in future we present a benchmark database of public social media postings from the united kingdom related to the covid19 pandemic for academic research purposes along with some initial analysis including a taxonomy of key themes organised by keyword this release supports the findings of a research study funded by the scottish government chief scientists office that aims to investigate social sentiment in order to understand the response to public health measures implemented during the pandemic,http://arxiv.org/abs/2103.16446v2,2
governments have long standing interests in preventing market failures and enhancing innovation in strategic industries public policy regarding domestic technology is critical to both national security and economic prosperity governments often seek to enhance their global competitiveness by promoting private sector cooperative activity at the interorganizational level research on network governance has illuminated the structure of boundaryspanning collaboration mainly for programs with immediate public or nonprofit objectives far less research has examined how governments might accelerate private sector cooperation to prevent market failures or to enhance innovation the theoretical contribution of this research is to suggest that government programs might catalyze cooperative activity by accelerating the preferential attachment mechanism inherent in social networks we analyze the longterm effects of a government program on the strategic alliance network of 451 organizations in the hightech semiconductor industry between 1987 and 1999 using stochastic network analysis methods for longitudinal social networks,http://arxiv.org/abs/1907.13087v2,2
scholars have long hypothesized that democratic forms of government are more compatible with scientific advancement however empirical analysis testing the democracyscience compatibility hypothesis remains underdeveloped this article explores the effect of democratic governance on scientific performance using panel data on 124 countries between 2007 and 2017 we find evidence supporting the democracyscience hypothesis further using both internal and external measures of complexity we estimate the effects of complexity as a moderating factor between the democracyscience connection the results show differential main effects of economic complexity globalization and international collaboration on scientific performance as well as significant interaction effects that moderate the effect of democracy on scientific performance the findings show the significance of democratic governance and complex systems in national scientific performance,http://arxiv.org/abs/1909.04468v4,2
the covid19 pandemic has emerged as a global public health crisis to make decisions about mitigation strategies and to understand the disease dynamics policy makers and epidemiologists must know how the disease is spreading in their communities we analyze confirmed infections and deaths over multiple geographic scales to show that covid19s impact is highly unequal many subregions have nearly zero infections and others are hot spots we attribute the effect to a reedhugheslike mechanism in which disease arrives at different times and grows exponentially hot spots however appear to grow faster than neighboring subregions and dominate spatially aggregated statistics thereby amplifying growth rates the staggered spread of covid19 can also make aggregated growth rates appear higher even when subregions grow at the same rate public policy economic analysis and epidemic modeling need to account for potential distortions introduced by spatial aggregation,http://arxiv.org/abs/2004.12994v2,2
precipitated by rapid globalization rising inequality population growth and longevity gains social protection programs have been on the rise in low and middleincome countries lmics in the last three decades however the introduction of public benefits could displace informal mechanisms for riskprotection which are especially prevalent in lmics if the displacement of private transfers is considerably large the expansion of social protection programs could even lead to social welfare loss in this paper we critically survey the recent empirical literature on crowdout effects in response to public policies specifically in the context of lmics we review and synthesize patterns from the behavioral response to various types of social protection programs furthermore we specifically examine for heterogeneous treatment effects by important socioeconomic characteristics we conclude by drawing on lessons from our synthesis of studies if poverty reduction objectives are considered along with careful program targeting that accounts for potential crowdout effects there may well be a net social gain,http://arxiv.org/abs/2006.00737v2,2
based on a conceptual framework that integrates three dimensions of digital transformation dt namely the nature of the product client interaction and the level of coordination with industry players this paper aims to explain the level of influence that contextual crisis factors may have played in organizational digitalization choices in search for resilience as part of adaptation strategies in particular this investigation would analyze digitalization choices as survival strategies for covid19 crisis in the case of mexican enterprises the selected country is of particular interest as research target in the global south in that public policy has offered little support to keep business organizations up and running leaving entrepreneurs with no other option but to implement bottomup resilience strategies including digitalization qualitative comparative analysis qca has been proposed to identify combinations of conditions to explain the role played by covid crisisrelated contextual factors that may have led to particular forms of digitalization semistructured interviews with industry associations are also proposed to gain knowledge about group responses to the crisis,http://arxiv.org/abs/2108.09802v1,2
accurately estimating the snowpack in key mountainous basins is critical for water resource managers to make decisions that impact local and global economies wildlife and public policy currently this estimation requires multiple lidarequipped plane flights or in situ measurements both of which are expensive sparse and biased towards accessible regions in this paper we demonstrate that fusing spatial and temporal information from multiple openlyavailable satellite and weather data sources enables estimation of snowpack in key mountainous regions our multisource model outperforms singlesource estimation by 50 inches rmse as well as outperforms sparse in situ measurements by 12 inches rmse,http://arxiv.org/abs/2208.04246v1,2
there is growing interest in understanding how interactions between systemwide objectives and local community decisionmaking will impact the clean energy transition the concept of energysheds has gained traction in the areas of public policy and social science as a way to study these relationships however development of technical definitions of energysheds that permit system analysis are still largely missing in this work we propose a mathematical definition for energysheds and introduce an analytical framework for studying energyshed concepts within the context of future electric power system operations this framework is used to develop insights into the factors that impact a communitys ability to achieve energyshed policy incentives within a larger connected power grid as well as the tradeoffs associated with different spatial policy requirements we also propose an optimizationbased energyshed policy design problem and show that it can be solved to global optimality within arbitrary precision by employing concepts from quasiconvex optimization finally we investigate how interconnected energysheds can cooperatively achieve their objectives in bulk power system operations,http://arxiv.org/abs/2311.16300v2,2
urban forests provide ecosystem services that are public goods with local shade to global carbon sequestration benefits and occur on both public and private lands thus incentives for private tree owners to invest in tree care may fall short of those of a public forest manager aiming to optimize ecosystem service benefits for society the management of a forest pest provides a salient focus area because pests threaten public goods provision and pest management generates feedback that mitigates future risks to forests we use a game theoretic model to determine optimal pest treatment subsidies for a focal privately owned tree and use an optimization approach to guide targeted public treatment of a representative public tree we find that optimal public subsidies for private tree treatment depend on assessed tree health and on the prevalence of the pest in the community considerations absent from many existing programs next by applying our pest treatment policies to a communityscale model of emerald ash borer forest pest dynamics we predict ash mortality under a range of treatment scenarios over a 50year time horizon our results highlight how designing policies that consider the public goods benefits of private actions can contribute to sustainable land management,http://arxiv.org/abs/2312.05403v2,2
the land matrix initiative and its global observatory aim to provide reliable data on largescale land acquisitions to inform debates and actions in sectors such as agriculture extraction or energy in low and middleincome countries although these data are recognized in the academic world they remain underutilized in public policy mainly due to the complexity of access and exploitation which requires technical expertise and a good understanding of the database schema the objective of this work is to simplify access to data from different database systems the methods proposed in this article are evaluated using data from the land matrix this work presents various comparisons of large language models llms as well as combinations of llm adaptations prompt engineering rag agents to query different database systems graphql and rest queries the experiments are reproducible and a demonstration is available online,http://arxiv.org/abs/2412.12961v1,2
background an advantaged socioeconomic position sep and satisfying social support during pregnancy ssp have been found to be protective factors of maternal postpartum depression pdd an advantaged sep is also associated with satisfying ssp making ssp a potential mediator of social inequalities in ppd sep ssp and ppd are associated with migrant status the aim of this study was to quantify the mediating role of ssp in social inequalities in ppd regarding mothers migrant status methods a subsample of 15000 mothers from the french nationallyrepresentative elfe cohort study was used for the present analyses sep was constructed as a latent variable measured with educational attainment occupational grade employment financial difficulties and household income ssp was characterized as perceived support from partner good relation satisfying support and paternal leave and actual support from midwives psychosocial risk factors assessment and antenatal education mediation analyses with multiple mediators stratified by migrant status were conducted results study population included 76 of nonmigrant women 12 of second and 12 of first generation migrant sep was positively associated with support from partner regardless of migrant status satisfying partner support was associated with a 8 nonmigrant women to 11 first generation migrant women reduction in ppd score limitations history of depression was not reportedconclusions partner support could reduce social inequalities in ppd this work supports the need of interventions longitudinal and qualitative studies including fathers and adapted to women at risk of ppd to better understand the role of ssp in social inequalities in ppd,http://arxiv.org/abs/2004.11244v1,2
inward foreign direct investment ifdi into europe and asian developing countries like bangladesh is experimentally examined in this study ifdi in emerging markets has been boosted by global investment and inflow influenced by resource availability and public policy the economic policy uncertainty on ifdi in 13 countries is explored at a time when the crisis between russia and ukraine war is having a global impact microeconomic factors affected gross domestic product gdp growth inflation interest rates and the currency rate fluctuated with ifdi which mostly shocked during covid19 and the russiaukraine war with data from the world bank and the united nations conference on trade and development unctad database we compile a panel dataset covering 20182022 the researchers used a mixture of panel and linear regression analysis using a random effect model our findings show that the impact of global rates hurts ifdi in 13 selected countries there is a correlation between a countrys ability to enforce contracts and the amount of inward fdi it receives using the top 13 hosts of incoming fdi flows covid19 and russiaukraine wartime series analysis gives valuable information for policymakers in the remaining countries chosen to attract ifdi inflows,http://arxiv.org/abs/2401.03096v1,2
social movements rely in large measure on networked communication technologies to organize and disseminate information relating to the movements objectives in this work we seek to understand how the goals and needs of a protest movement are reflected in the geographic patterns of its communication network and how these patterns differ from those of stable political communication to this end we examine an online communication network reconstructed from over 600000 tweets from a thirtysix week period covering the birth and maturation of the american anticapitalist movement occupy wall street we find that compared to a network of stable domestic political communication the occupy wall street network exhibits higher levels of locality and a hub and spoke structure in which the majority of nonlocal attention is allocated to highprofile locations such as new york california and washington dc moreover we observe that information flows across state boundaries are more likely to contain framing language and references to the media while communication among individuals in the same state is more likely to reference protest action and specific places and and times tying these results to social movement theory we propose that these features reflect the movements efforts to mobilize resources at the local level and to develop narrative frames that reinforce collective purpose at the national level,http://arxiv.org/abs/1306.5473v1,2
existing endtoendencrypted e2ee email systems mainly pgp have long been evaluated in controlled lab settings while these studies have exposed usability obstacles for the average user and offer design improvements there exist users with an immediate need for private communication who must cope with existing software and its limitations we seek to understand whether individuals motivated by concrete privacy threats such as those vulnerable to state surveillance can overcome usability issues to adopt complex e2ee tools for longterm use we surveyed regional activists as surveillance of social movements is welldocumented our study group includes individuals from 9 social movement groups in the us who had elected to participate in a workshop on using thunderbirdenigmail for email encryption these workshops tool place prior to mid2017 via a partnership with a nonprofit which supports social movement groups six to 40 months after their pgp email encryption training more than half of the study participants were continuing to use pgp email encryption despite intervening widespread deployment of simple e2ee messaging apps such as signal we study the interplay of usability with social factors such as motivation and the risks that individuals undertake through their activism we find that while usability is an important factor it is not enough to explain long term use for example we find that riskiness of ones activism is negatively correlated with longterm pgp use this study represents the first longterm study and the first inthewild study of pgp email encryption adoption,http://arxiv.org/abs/2104.04478v1,2
public policy must confront emergencies that evolve in real time and in uncertain directions yet little is known about the nature of policy response here we take the coronavirus pandemic as a global and extraordinarily consequential case and study the global policy response by analyzing a novel dataset recording policy documents published by government agencies think tanks and intergovernmental organizations igos across 114 countries 37725 policy documents from jan 2nd through may 26th 2020 our analyses reveal four primary findings 1 global policy attention to covid19 follows a remarkably similar trajectory as the total confirmed cases of covid19 yet with evolving policy focus from public health to broader social issues 2 the covid19 policy frontier disproportionately draws on the latest peerreviewed and highimpact scientific insights moreover policy documents that cite science appear especially impactful within the policy domain 3 the global policy frontier is primarily interconnected through igos such as the who which produce policy documents that are central to the covid19 policy network and draw especially strongly on scientific literature removing igos contributions fundamentally alters the global policy landscape with the policy citation network among government agencies increasingly fragmented into many isolated clusters 4 countries exhibit highly heterogeneous policy attention to covid19 most strikingly a countrys early policy attention to covid19 shows a surprising degree of predictability for the countrys subsequent deaths overall these results uncover fundamental patterns of policy interactions and given the consequential nature of emergent threats and the paucity of quantitative approaches to understand them open up novel dimensions for assessing and effectively coordinating global and local responses to covid19 and beyond,http://arxiv.org/abs/2006.13853v1,2
nonlinear dynamics is probably much more common in the epigenetic dynamics of living beings than hitherto recognized here we report a case of global bifurcation triggered by gender that affects higher cognitive functions in humans we report a crosscultural study showing deviations in time perception as assessed by estimating the duration of brief sounds according to their durations and to the gender of the perciver results show that the duration of sounds lasting less than 10 s were on average overestimated whereas those lasting longer were underestimated estimates of sounds shorter than 1 s were extremely inaccurate females consistently gave longer estimates than males accuracy in time estimation was correlated to academic performance in disciplines requiring mathematical or scientific skills in male but not in female students this difference in correlation however had nothing to do with overall skills in mathematics both sexes scored similarly in scientific and technical disciplines but females had higher grades than males in languages and lower ones in physical education our results confirm existing evidence for gender differences in cognitive processing hinting to the existence of different mathematical intelligences with different nonlinear relationships between natural or biological mathematical intuition and time perception,http://arxiv.org/abs/1203.3954v2,2
perception of offensiveness is inherently subjective shaped by the lived experiences and sociocultural values of the perceivers recent years have seen substantial efforts to build aibased tools that can detect offensive language at scale as a means to moderate social media platforms and to ensure safety of conversational ai technologies such as chatgpt and bard however existing approaches treat this task as a technical endeavor built on top of data annotated for offensiveness by a global crowd workforce without any attention to the crowd workers provenance or the values their perceptions reflect we argue that cultural and psychological factors play a vital role in the cognitive processing of offensiveness which is critical to consider in this context we reframe the task of determining offensiveness as essentially a matter of moral judgment deciding the boundaries of ethically wrong vs right language within an implied set of sociocultural norms through a largescale crosscultural study based on 4309 participants from 21 countries across 8 cultural regions we demonstrate substantial crosscultural differences in perceptions of offensiveness more importantly we find that individual moral values play a crucial role in shaping these variations moral concerns about care and purity are significant mediating factors driving crosscultural differences these insights are of crucial importance as we build ai models for the pluralistic world where the values they espouse should aim to respect and account for moral values in diverse geocultural contexts,http://arxiv.org/abs/2312.06861v1,2
we show that the divergent acoustic energy release rate in a quasistatically compressed nanoporous material can be used as a precursor to failure in such materials a quantification of the inequality of the energy release rate using social inequality measure indices help constructing a warning signal for large bursts of energy release we also verify similar behavior for simulations of viscoelastic fiber bundle models that mimic the strainhardening dynamics of the samples the results demonstrate experimental applicability of the precursory signal formulation for any diverging response function near a transition point using social inequality indices,http://arxiv.org/abs/2406.06200v1,2
public policy also represent a special subdiscipline within political science within political science they are given increasing importance and importance in the context of scientific research and scientific approaches public policy as a discipline of political science have their own special subject and method of research a particularly important aspect of the scientific approach to public policy is the aspect of applying research methods as one of the stages and phases of designing scientific research in this sense the goal of this research is to present the application of scientific research methods in the field of public policy those methods are based on scientific achievements developed within the framework of modern methodology of social sciences scientific research methods represent an important functional part of the research project as a model of the scientific research system predominantly of an empirical character which is applicable to all types of research this is precisely what imposes the need to develop a project as a prerequisite for applying scientific methods and conducting scientific research and therefore for a more complete understanding of public policy the conclusions that will be reached point to the fact that scientific research of public policy can not be carried out without the creation of a scientific research project as a complex scientific and operational document and the application of appropriate methods and techniques developed within the framework of scientific achievements of modern social science methodology,http://arxiv.org/abs/2309.15973v2,2
enum effectively bridges the telephone and internet worlds by placing telephone numbers from the itu rec e164 public telecommunication numbering plan into the internet domain name system dns as domain names enum potentially presents significant public policy issues at both the domestic and international levels ultimately it should not matter whether enum is approached as a telecommunications issue or an internet issue because 1 they are becoming the same thing technically and 2 they engage the same global public interests for the same reasons as apply to traditional telecommunications and even to the internet itself public oversight of enum naming numbering and addressing resources is justified both by technical necessity and the interests of consumer protection particularly personal privacy and competition at higher service layers a single coordinated global dns domain for at least tier 0 the international level of the enum names hierarchy should be designated by public authorities many of the technical characteristics and policy considerations relevant at the enum tier 0 and 1 zones are also directly applicable to the internets ip address space and dns root or tier 0 zone key shared elements of the internets logical infrastructure despite the fundamentally international nature of the internets logical infrastructure layer and the purported privatization of administration of its ip address space and the dns internet governance is not yet truly international the enum policy debate illustrates the need for authoritative international public oversight of public communications network logical infrastructure including that of traditional telecommunications the internet and enum,http://arxiv.org/abs/cs/0109091v2,2
quantitative metrics that measure the global economys equilibrium have strong and interdependent relationships with the agricultural supply chain and international trade flows sudden shocks in these processes caused by outlier events such as trade wars pandemics or weather can have complex effects on the global economy in this paper we propose a novel framework namely deepag that employs econometrics and measures the effects of outlier events detection using deep learning dl to determine relationships between commonplace financial indices such as the dowjones and the production values of agricultural commodities such as cheese and milk we employed a dl technique called long shortterm memory lstm networks successfully to predict commodity production with high accuracy and also present five popular models regression and boosting as baselines to measure the effects of outlier events the results indicate that deepag with outliers considerations using isolation forests outperforms baseline models as well as the same model without outliers detection outlier events make a considerable impact when predicting commodity production with respect to financial indices moreover we present the implications of deepag on public policy provide insights for policymakers and farmers and for operational decisions in the agricultural ecosystem data are collected models developed and the results are recorded and presented,http://arxiv.org/abs/2110.12062v2,2
the pepenaactivity is part of the informal sector characterized by precariousness and social invisibility however it is a key element in the recycling production chain providing an economic income for many people in the city of chihuahua this activity is carried out on a significant scale therefore this study analyzes and dignifies the recycling of urban solid waste from the perspective of scavengers also explaining their relationship with other social actors at the citys final disposal site the study starts from the premise that waste picking is in the current era of global environmental crisis an effective way to conserve resources and reduce environmental impacts methodologically participatory action research was used as a strategy to raise awareness among scavengers about the importance of their work and to present their community organization as an example to other urban waste picking groups in different locations finally we found issues in these groups such as informality lack of legal supportspecific health risks and more broadly the absence of public policies to recognize this activity as valuable in addressing the global environmental crisis,http://arxiv.org/abs/2410.05283v1,2
social inequality is a topic of interest since ages and has attracted researchers across disciplines to ponder over it origin manifestation characteristics consequences and finally the question of how to cope with it it is manifested across different strata of human existence and is quantified in several ways in this review we discuss the origins of social inequality the historical and commonly used nonentropic measures such as lorenz curve gini index and the recently introduced k index we also discuss some analytical tools that aid in understanding and characterizing them finally we argue how statistical physics modeling helps in reproducing the results and interpreting them,http://arxiv.org/abs/1507.02445v1,2
based on some analytic structural properties of the gini and kolkata indices for social inequality as obtained from a generic form of the lorenz function we make a conjecture that the limiting effective saturation value of the abovementioned indices is about 0865 this together with some more new observations on the citation statistics of individual authors including nobel laureates suggests that about 14 of people or papers or social conflicts tend to earn or attract or cause about 86 of wealth or citations or deaths respectively in very competitive situations in markets universities or wars this is a modified form of the more than a century old 8020 law of pareto in economy not visible today because of various welfare and other strategies and gives an universal value 086 of social inequality constant or number,http://arxiv.org/abs/2102.01527v5,2
the present study examines to what extent cultural background determines sensorimotor synchronization in humans,http://arxiv.org/abs/2107.03971v1,2
with the growth of social media usage social activists try to leverage this platform to raise the awareness related to a social issue and engage the public worldwide the broad use of social media platforms in recent years made it easier for the people to stay uptodate on the news related to regional and worldwide events while social media namely twitter assists social movements to connect with more people and mobilize the movement traditional media such as news articles help in spreading the news related to the events in a broader aspect in this study we analyze linguistic features and cues such as individualism vs pluralism sentiment and emotion to examine the relationship between the medium and discourse over time we conduct this work in a specific application context the black lives matter blm movement and compare discussions related to this event in social media vs news articles,http://arxiv.org/abs/1808.01742v1,2
messaging platforms especially those with a mobile focus have become increasingly ubiquitous in society these mobile messaging platforms can have deceivingly large user bases and in addition to being a way for people to stay in touch are often used to organize social movements as well as a place for extremists and other neerdowell to congregate in this paper we present a dataset from one such mobile messaging platform telegram our dataset is made up of over 278k channels and 317m messages from 22m unique users to the best of our knowledge our dataset comprises the largest and most complete of its kind in addition to the raw data we also provide the source code used to collect it allowing researchers to run their own data collection instance we believe the pushshift telegram dataset can help researchers from a variety of disciplines interested in studying online social movements protests political extremism and disinformation,http://arxiv.org/abs/2001.08438v1,2
the social structure of an animal population can often influence movement and inform researchers on a species behavioral tendencies animal social networks can be studied through movement data however modern sources of data can have identification issues that result in multiplylabeled individuals since all available social movement models rely on unique labels we extend an existing bayesian hierarchical movement model in a way that makes use of a latent social network and accommodates multiplylabeled movement data mlmd we apply our model to dronemeasured movement data from rissos dolphins grampus griseus and estimate the effects of sonar exposure on the dolphins social structure our proposed framework can be applied to mlmd for various social movement applications,http://arxiv.org/abs/2204.00542v2,2
social media enables the rapid spread of many kinds of information from memes to social movements however little is known about how information crosses linguistic boundaries we apply causal inference techniques on the european twitter network to quantify multilingual users structural role and communication influence in crosslingual information exchange overall multilinguals play an essential role posting in multiple languages increases betweenness centrality by 13 and having a multilingual network neighbor increases monolinguals odds of sharing domains and hashtags from another language 16fold and 4fold respectively we further show that multilinguals have a greater impact on diffusing information less accessible to their monolingual compatriots such as information from faraway countries and content about regional politics nascent social movements and job opportunities by highlighting information exchange across borders this work sheds light on a crucial component of how information and ideas spread around the world,http://arxiv.org/abs/2304.03797v1,2
encompassing a diverse population of developers nontechnical users organizations and many other stakeholders open source software oss development has expanded to broader social movements from the initial product development aims ideology as a coherent system of ideas offers value commitments and normative implications for any social movement so does oss ideology for the open source movement however the literature on open source ideology is often fragile or lacking in empirical evidence in this paper we sought to develop a comprehensive empirical theory of ideologies in open source software movement following a grounded theory procedure we collected and analyzed data from 22 semistructured interviews and 41 video recordings of open source initiative osi board members public speeches an empirical theory of oss ideology emerged in our analysis with six key categories membership normsvalues goals activities resources and positionsgroup relations each consists of a number of themes and subthemes we discussed a subset of carefully selected themes and subthemes in detail based on their theoretical significance with this ideological lens we examined the implications and insights into open source development and shed light on the research into open source as a socialcultural construction in the future,http://arxiv.org/abs/2306.05548v1,2
large language and vision models have transformed how social movements scholars identify protest and extract key protest attributes from multimodal data such as texts images and videos this article documents how we finetuned two large pretrained transformer models including longformer and swintransformer v2 to infer potential protests in news articles using textual and imagery data first the longformer model was finetuned using the dynamic of collective action doca corpus we matched the new york times articles with the doca database to obtain a training dataset for downstream tasks second the swintransformer v2 models was trained on uclaprotest imagery data uclaprotest project contains labeled imagery data with information such as protest violence and sign both finetuned models will be available via url we release this short technical report for social movement scholars who are interested in using llvms to infer protests in textual and imagery data,http://arxiv.org/abs/2311.18241v1,2
the cyberspace and development of intelligent systems using artificial intelligence ai creates new challenges to computer professionals data scientists regulators and policy makers for example selfdriving cars raise new technical ethical legal and public policy issues this paper proposes a course named computers ethics law and public policy and suggests a curriculum for such a course this paper presents ethical legal and public policy issues relevant to building and using intelligent systems,http://arxiv.org/abs/1904.12470v5,2
the potential threat that domestic animals pose to the health of human populations tends to be overlooked we posit that positive steps forward can be made in this area via suitable statewide public policy in this paper we describe the data collection process that took place in casilda a city in argentina in the context of a canine census we outline preliminary findings emerging from the data based on a number of perspectives along with implications of these findings in terms of informing public policy,http://arxiv.org/abs/2012.07475v1,2
promoting and increasing energy efficiency is a promising method of reducing co2 emissions and avoiding the potentially devastating effects of climate change the question is how do we induce a cultural or behavioural change whereby people nationally and globally adopt more energy efficient lifestyles we propose a new family of mathematical models based on a statistical mechanics extension of discrete choice theory that offer a set of formal tools to systematically analyse and quantify this problem an application example could be to predict the percentage of people choosing to buy new energy efficient light bulbs instead of the traditional incandescent versions through statistical evaluation of survey responses the models can identify the key driving factors in the decisionmaking process for example the extent to which people imitate each other these models allow us to incorporate the effect of social interactions could help us identify tipping points at a societal level this knowledge could be used to trigger structural changes in our society the results may provide tangible and deliverable evidencebased policy options to decisionmakers we believe that these models offer an opportunity for the research communityin both the social and physical sciencesand decisionmakers in the private and public sectors to work together towards preventing the potentially devastating social economic and environmental effects of climate change,http://arxiv.org/abs/0804.3319v3,2
the quantum decision theory developed recently by the authors is applied to clarify the role of risk and uncertainty in decision making and in particular in relation to the phenomenon of dynamic inconsistency by formulating this notion in precise mathematical terms we distinguish three types of inconsistency time inconsistency planning paradox and inconsistency occurring in some discounting effects while time inconsistency is well accounted for in classical decision theory the planning paradox is in contradiction with classical utility theory it finds a natural explanation in the frame of the quantum decision theory different types of discounting effects are analyzed and shown to enjoy a straightforward explanation within the suggested theory we also introduce a general methodology based on selfsimilar approximation theory for deriving the evolution equations for the probabilities of future prospects this provides a novel classification of possible discount factors which include the previously known cases exponential or hyperbolic discounting but also predicts a novel class of discount factors that decay to a strictly positive constant for very large future time horizons this class may be useful to deal with very longterm discounting situations associated with intergenerational public policy choices encompassing issues such as global warming and nuclear waste disposal,http://arxiv.org/abs/0812.2388v2,2
the functioning of the cryptocurrency bitcoin relies on the open availability of the entire history of its transactions this makes it a particularly interesting socioeconomic system to analyse from the point of view of network science here we analyse the evolution of the network of bitcoin transactions between users we achieve this by using the complete transaction history from december 5th 2011 to december 23rd 2013 this period includes three bubbles experienced by the bitcoin price in particular we focus on the global and local structural properties of the user network and their variation in relation to the different period of price surge and decline by analysing the temporal variation of the heterogeneity of the connectivity patterns we gain insights on the different mechanisms that take place during bubbles and find that hubs ie the most connected nodes had a fundamental role in triggering the burst of the second bubble finally we examine the local topological structures of interactions between users we discover that the relative frequency of triadic interactions experiences a strong change before during and after a bubble and suggest that the importance of the hubs grows during the bubble these results provide further evidence that the behaviour of the hubs during bubbles significantly increases the systemic risk of the bitcoin network and discuss the implications on public policy interventions,http://arxiv.org/abs/1805.04460v1,2
reducing and redressing the effects of deforestation is a complex public policy challenge and evaluating the efficacy of such policy efforts is crucial for policy learning and adaptation deforestation in highincome nations can contribute substantially to global forest loss despite the presence of strong institutions and high policy capacity in queensland australia over 5 million hectares of native forest has been lost since 1988 successive regulatory policies have aimed to reduce deforestation in queensland though debate exists over their effect given the influence of other drivers of forest loss using a hierarchical bayesian statistical framework we combine satellite imagery of forest loss with macroeconomic land tenure biophysical and climatic variables to collectively model deforestation for 50 local government areas lgas across queensland we apply the spatially explicit bentcable regression model to detect trend change that may signal a regulatory policy effect we find that annual growth in gdp was the only clear driver of lgaspecific deforestation after adjusting for other covariate effects our model shows strong evidence of spatial contagion in deforestation across queensland and this effect is influenced by the dominant land tenure type within each lga we find our model exhibits a bend mostly between 2000 and 2007 consistent with expectations but the signal is not particularly strong due extreme variation in deforestation trends between and within lgas our results demonstrate that the bentcable model is a promising technique for detecting system changes in response to policy interventions but future work should be conducted at a national scale to provide more data points and incorporate more lgaspecific data to improve model goodnessoffit,http://arxiv.org/abs/1906.09365v1,2
intervention policies against covid19 have caused largescale disruptions globally and led to a series of pattern changes in the power system operation analyzing these pandemicinduced patterns is imperative to identify the potential risks and impacts of this extreme event with this purpose we developed an openaccess data hub covidemda an opensource toolbox covemda and a few evaluation methods to explore what the us power systems are experiencing during covid19 these resources could be broadly used for research public policy and educational purposes technically our data hub harmonizes a variety of raw data such as generation mix demand profiles electricity price weather observations mobility confirmed cases and deaths typical methods are reformulated and standardized in our toolbox including baseline estimation regression analysis and scientific visualization here the fluctuation index and probabilistic baseline are proposed for the first time to consider data fluctuation and estimation uncertainty based on these we conduct three empirical studies on the us power systems and share new solutions and unexpected findings to address the issues of public concerns this conveys a more complete picture of the pandemics impacts and also opens up several attractive topics for future work python matlab source codes and user manuals are all publicly shared on a github repository,http://arxiv.org/abs/2112.05320v2,2
background the covid19 pandemic has increased mental distress globally the proportion of people reporting anxiety is 26 and depression is 34 points disentangling associational and causal contributions of behavior covid19 cases and economic distress on mental distress will dictate different mitigation strategies to reduce longterm pandemicrelated mental distress methods we use the household pulse survey hps april 2020 to february 2021 data to examine mental distress among us citizens attributable to covid19 we combined hps survey data with publicly available statelevel weekly covid19 case and death data from the centers for disease control public policies and apple and google mobility data finally we constructed economic and mental distress measures to estimate structural models with lag dependent variables to tease out public health policies associational and causal path coefficients on economic and mental distress findings from april 2020 to february 2021 we found that anxiety and depression had steadily climbed in the us by design mobility restrictions primarily affected public health policies where businesses and restaurants absorbed the biggest hit period t1 covid19 cases increased job loss by 41 and economic distress by 63 points in the same period jobloss and housing insecurity in t1 increased period t mental distress by 291 and 327 respectively however t1 food insecurity decreased mental distress by 49 in time t the pandemicrelated potential causal path coefficient of period t1 economic distress on period t depression is 578 and anxiety is 559 thus we show that period t1 covid19 case information behavior and economic distress may be causally associated with pandemic related period t mental distress,http://arxiv.org/abs/2112.11564v1,2
objective the novel coronavirus covid19 outbreak rapidly evolved into pandemic global research efforts focus on this topic and with the collaboration of the scientific journals publication industry produced more than 16000 related published articles in pubmed within five months from the onset of the outbreak herein a comparison of the covid19 citations in pubmed and web of science was performed with sarscov merscov ebola zika avian and swine influenza epidemics methods the citations were searched and collected using the disease terms and the date of publication restriction the total number of pubmed citations and the hiv associated papers during the same chronological periods were examined in parallel the journal category and country information of the publications were gathered from web of science the collected data were statistically analyzed and compared results significant correlations were found between covid19 and mers cc0988 p0003 q0006 ebola cc0987 p0003 q0011 and sars cc0964 p0015 q0028 epidemics fivemonth pick of novel citations in pubmed however covid19 publications were accumulated earlier and in larger numbers than any other 21st century major communicable disease outbreak conclusion the acceleration and the total number of covid19 publications represent an unprecedented landmark event in the medical library history the immediate adoption of the fasttrack peerreviewing and publishing as well as the open access publication policies by the journal publishers are significant contributors to this bibliographic phenomenon,http://arxiv.org/abs/2006.05366v1,2
longterm monitoring of the evolution of the artificial night sky brightness is a key tool for developing scienceinformed public policies and assessing the efficacy of light pollution mitigation measures detecting the underlying artificial brightness trend is a challenging task since the typical night sky brightness signal shows a large variability with characteristic time scales ranging from seconds to years in order to effectively isolate the weak signature of the effect of interest determining the potential long term drifts of the radiance sensing systems is crucial if these drifts can be adequately characterized the raw measurements could be easily corrected for them and transformed to a consistent scale in this short note we report on the progressive darkening of the signal recorded by sqm detectors belonging to several monitoring networks permanently installed outdoors for periods ranging from several months to several years the sensitivity drifts were estimated by means of parallel measurements made at the beginning and at the end of the evaluation periods using reference detectors of the same kind that were little or no exposed to weathering in the intervening time our preliminary results suggest that sqm detectors installed outdoors steadily increase their readings at an average rate of 0034 magsqmarcsec2 per mwhm2 of exposure to solar horizontal global irradiation that for our locations translates into approximately 005 to 006 magsqmarcsec2 per year,http://arxiv.org/abs/2011.15044v2,2
tropical forests represent the home of many species on the planet for flora and fauna retaining billions of tons of carbon footprint promoting clouds and rain formation implying a crucial role in the global ecosystem besides representing the home to countless indigenous peoples unfortunately millions of hectares of tropical forests are lost every year due to deforestation or degradation to mitigate that fact monitoring and deforestation detection programs are in use in addition to public policies for the prevention and punishment of criminals these monitoringdetection programs generally use remote sensing images image processing techniques machine learning methods and expert photointerpretation to analyze identify and quantify possible changes in forest cover several projects have proposed different computational approaches tools and models to efficiently identify recent deforestation areas improving deforestation monitoring programs in tropical forests in this sense this paper proposes the use of pattern classifiers based on neuroevolution technique neat in tropical forest deforestation detection tasks furthermore a novel framework called eneat has been created and achieved classification results above 90 for balanced accuracy measure in the target application using an extremely reduced and limited training set for learning the classification models these results represent a relative gain of 62 over the best baseline ensemble method compared in this paper,http://arxiv.org/abs/2208.11058v1,2
emotion arcs capture how an individual or a population feels over time they are widely used in industry and research however there is little work on evaluating the automatically generated arcs this is because of the difficulty of establishing the true gold emotion arc our work for the first time systematically and quantitatively evaluates automatically generated emotion arcs we also compare two common ways of generating emotion arcs machinelearning ml models and lexicononly lexo methods by running experiments on 18 diverse datasets in 9 languages we show that despite being markedly poor at instance level emotion classification lexo methods are highly accurate at generating emotion arcs when aggregating information from hundreds of instances we also show through experiments on six indigenous african languages as well as arabic and spanish that automatic translations of english emotion lexicons can be used to generate highquality emotion arcs in lessresource languages this opens up avenues for work on emotions in languages from around the world which is crucial for commerce public policy and health research in service of speakers often left behind code and resources,http://arxiv.org/abs/2306.02213v3,2
the abundance of social media data has presented opportunities for accurately determining public and groupspecific stances around policy proposals or controversial topics in contrast with sentiment analysis which focuses on identifying prevailing emotions stance detection identifies precise positions ie supportive opposing neutral relative to a welldefined topic such as perceptions toward specific global health interventions during the covid19 pandemic traditional stance detection models while effective within their specific domain eg attitudes towards masking protocols during covid19 often lag in performance when applied to new domains and topics due to changes in data distribution this limitation is compounded by the scarcity of domainspecific labeled datasets which are expensive and laborintensive to create a solution we present in this paper combines counterfactual data augmentation with contrastive learning to enhance the robustness of stance detection across domains and topics of interest we evaluate the performance of current stateoftheart stance detection models including a promptoptimized large language model relative to our proposed framework succinctly called stancec3 domainadaptive crosstarget stance detection via contrastive learning and counterfactual generation empirical evaluations demonstrate stancec3s consistent improvements over the baseline models with respect to accuracy across domains and varying focal topics despite the increasing prevalence of generalpurpose models such as generative ai specialized models such as stancec3 provide utility in safetycritical domains wherein precision is highly valued especially when a nuanced understanding of the concerns of different population segments could result in crafting more impactful public policies,http://arxiv.org/abs/2309.15176v2,2
causal machine learning tools are beginning to see use in realworld policy evaluation tasks to flexibly estimate treatment effects one issue with these methods is that the machine learning models used are generally black boxes ie there is no globally interpretable way to understand how a model makes estimates this is a clear problem in policy evaluation applications particularly in government because it is difficult to understand whether such models are functioning in ways that are fair based on the correct interpretation of evidence and transparent enough to allow for accountability if things go wrong however there has been little discussion of transparency problems in the causal machine learning literature and how these might be overcome this paper explores why transparency issues are a problem for causal machine learning in public policy evaluation applications and considers ways these problems might be addressed through explainable ai tools and by simplifying models in line with interpretable ai principles it then applies these ideas to a casestudy using a causal forest model to estimate conditional average treatment effects for a hypothetical change in the school leaving age in australia it shows that existing tools for understanding blackbox predictive models are poorly suited to causal machine learning and that simplifying the model to make it interpretable leads to an unacceptable increase in error in this application it concludes that new tools are needed to properly understand causal machine learning models and the algorithms that fit them,http://arxiv.org/abs/2310.13240v2,2
discussion of bayesian models and methods in public policy and government settings by s e fienberg arxiv11082177,http://arxiv.org/abs/1108.3912v1,2
we study a behavioral sir model with timevarying costs of distancing the two main causes of the variation in the cost of distancing we explore are distancing fatigue and public policies lockdowns we show that for a second wave of an epidemic to arise a steep increase in distancing cost is necessary distancing fatigue cannot increase the distancing cost sufficiently fast to create a second wave however public policies that discontinuously affect the distancing cost can create a second wave with that in mind we characterize the largest change in the distancing cost due to for example lifting a public policy that will not cause a second wave finally we provide a numerical analysis of public policies under distancing fatigue and show that a strict lockdown at the beginning of an epidemic as for example recently in china can lead to unintended adverse consequences when the policy is lifted the disease spreads very fast due to the accumulated distancing fatigue of the individuals causing high prevalence levels,http://arxiv.org/abs/2206.03847v2,2
research software is increasingly recognized as a vital component of the scholarly record journals offer authors the opportunity to publish research software papers but often have different requirements for how these publications should be structured and how code should be verified in this short case study we gather data from 20 physical science journals to trace the frequency quality control and publishing criteria for software papers our goal with the case study is to provide a proofofconcept for doing descriptive empirical work with software publication policies across numerous domains of science and engineering in the narrative we therefore provide descriptive statistics showing how these journals differ in criteria required for archiving linking verifying and documenting software as part of a formal publication the contribution of this preliminary work is twofold 1 we provide case study of physical science research software publications over time 2 we demonstrate the use of a new survey method for analyzing research software publication policies in our conclusion we describe how comparative research into software publication policies can provide better criteria and requirements for an emerging software publication landscape,http://arxiv.org/abs/2206.05367v1,2
while outdoor advertisements are common features within towns and cities they may reinforce social inequalities in health vulnerable populations in deprived areas may have greater exposure to fast food gambling and alcohol advertisements encouraging their consumption understanding who is exposed and evaluating potential policy restrictions requires a substantial manual data collection effort to address this problem we develop a deep learning workflow to automatically extract and classify unhealthy advertisements from streetlevel images we introduce the liverpool 360 street view liv360sv dataset for evaluating our workflow the dataset contains 25349 360 degree streetlevel images collected via cycling with a gopro fusion camera recorded jan 14th 18th 2020 10106 advertisements were identified and classified as food 1335 alcohol 217 gambling 149 and other 8405 eg cars and broadband we find evidence of social inequalities with a larger proportion of food advertisements located within deprived areas and those frequented by students our project presents a novel implementation for the incidental classification of street view images for identifying unhealthy advertisements providing a means through which to identify areas that can benefit from tougher advertisement restriction policies for tackling social inequalities,http://arxiv.org/abs/2007.04611v2,2
in the 60s of the last century the few courses of history of physics in physics degree were held by scholars who apart from a few exceptions did not have a specific research background in the field some activities books social movements in the civil society allowed in the 70s the entry among physics courses of teachings in history of physics held by scholars specifically trained for that job since their degree a second change happened in the 90s when many difficulties forced physicists to allocate fewer and fewer resources to the history of their discipline ill outline the features of the two periods and the efforts of historians to find a proper space in physics departments,http://arxiv.org/abs/2302.02438v2,2
the model of bonabeau explains the emergence of social hierarchies from the memory of fights in an initially egalitarian society introducing a feedback from the social inequality into the probability to win a fight we find a sharp transition between egalitarian society at low population density and hierarchical society at high population density,http://arxiv.org/abs/cond-mat/0208388v1,2
high resolution crop type maps are an important tool for improving food security and remote sensing is increasingly used to create such maps in regions that possess ground truth labels for model training however these labels are absent in many regions and models trained in other regions on typical satellite features such as those from optical sensors often exhibit low performance when transferred here we explore the use of nasas global ecosystem dynamics investigation gedi spaceborne lidar instrument combined with sentinel2 optical data for crop type mapping using data from three major cropped regions in china france and the united states we first demonstrate that gedi energy profiles are capable of reliably distinguishing maize a crop typically above 2m in height from crops like rice and soybean that are shorter we further show that these gedi profiles provide much more invariant features across geographies compared to spectral and phenological features detected by passive optical sensors gedi is able to distinguish maize from other crops within each region with accuracies higher than 84 and able to transfer across regions with accuracies higher than 82 compared to 64 for transfer of optical features finally we show that gedi profiles can be used to generate training labels for models based on optical imagery from sentinel2 thereby enabling the creation of 10m walltowall maps of tall versus short crops in labelscarce regions as maize is the second most widely grown crop in the world and often the only tall crop grown within a landscape we conclude that gedi offers great promise for improving global crop type maps,http://arxiv.org/abs/2109.06972v1,2
chinas pledge to reach carbon neutrality before 2060 is an ambitious goal and could provide the world with muchneeded leadership on how to limit warming to 15c warming above preindustrial levels by the end of the century but the pathways that would achieve net zero by 2060 are still unclear including the role of negative emissions technologies we use the global change analysis model to simulate how negative emissions technologies in general and direct air capture dac in particular could contribute to chinas meeting this target our results show that negative emissions could play a large role offsetting on the order of 3 gtco2 per year from difficulttomitigate sectors such as freight transportation and heavy industry this includes up to a 16 gtco2 per year contribution from dac constituting up to 60 of total projected negative emissions in china but dac like bioenergy with carbon capture and storage and afforestation has not yet been demonstrated at anywhere approaching the scales required to meaningfully contribute to climate mitigation deploying nets at these scales will have widespread impacts on financial systems and natural resources such as water land and energy in china,http://arxiv.org/abs/2010.06723v2,2
the media frequently describes the 2017 charlottesville unite the right rally as a turning point for the altright and white supremacist movements social movement theory suggests that the media attention and public discourse concerning the rally may have influenced the altright but this has yet to be empirically tested the current study investigates whether there are differences in language use between 7142 altright and progressive youtube channels in addition to measuring possible changes as a result of the rally to do so we create structural topic models and measure bigram proportions in video transcripts spanning eight weeks before to eight weeks after the rally we observe differences in topics between the two groups with the alternative influencers for example discussing topics related to race and free speech to an increasing and larger extent than progressive channels we also observe structural breakpoints in the use of bigrams at the time of the rally suggesting there are changes in language use within the two groups as a result of the rally while most changes relate to mentions of the rally itself the alternative group also shows an increase in promotion of their youtube channels results are discussed in light of social movement theory followed by a discussion of potential implications for understanding the altright and their language use on youtube,http://arxiv.org/abs/1908.11599v2,2
since the fatal shooting of 17year old black teenager trayvon martin in february 2012 by a white neighborhood watchman george zimmerman in sanford florida there has been a significant increase in digital activism addressing policebrutality related and raciallymotivated incidents in the united states in this work we administer an innovative study of digital activism by exploiting social media as an authoritative tool to examine and analyze the linguistic cues and thematic relationships in these three mediums we conduct a multilevel text analysis on 36984559 tweets to investigate users behaviors to examine the language used and understand the impact of digital activism on social media within each social movement on a sentencelevel wordlevel and topiclevel our results show that excessive use of raciallyrelated or prejudicial hashtags were used by the counter protests which portray potential discriminatory tendencies consequently our findings highlight that social activism done by black lives matter activists does not diverge from the social issues and topics involving policebrutality related and raciallymotivated killings of black individuals due to the shape of its topical graph that topics and conversations encircling the largest component directly relate to the topic of black lives matter finally we see that both blue lives matter and all lives matter movements depict a different directive as the topics of blue lives matter or all lives matter do not reside in the center these findings suggest that topics and conversations within each social movement are skewed random or possessed raciallyrelated undertones and thus deviating from the prominent social injustice issues,http://arxiv.org/abs/2109.12192v1,2
to achieve the ambitious aims of the paris climate agreement the majority of fossilfuel reserves needs to remain underground as current national government commitments to mitigate greenhouse gas emissions are insufficient by far actors such as institutional and private investors and the social movement on divestment from fossil fuels could play an important role in putting pressure on national governments on the road to decarbonization using a stochastic agentbased model of coevolving financial market and investors beliefs about future climate policy on an adaptive social network here we find that the dynamics of divestment from fossil fuels shows potential for social tipping away from a fossilfuel based economy our results further suggest that socially responsible investors have leverage a small share of 1020 of such moral investors is sufficient to initiate the burst of the carbon bubble consistent with the pareto principle these findings demonstrate that divestment has potential for contributing to decarbonization alongside other social movements and policy instruments particularly given the credible imminence of strong international climate policy our analysis also indicates the possible existence of a carbon bubble with potentially destabilizing effects to the economy,http://arxiv.org/abs/1902.07481v1,2
as the unicultural studies of website usability have matured the paucity of crosscultural studies of usability become increasingly apparent moving toward these crosscultural studies will require the development of a new tool to assess website usability in the context of cultural dimensions this paper introduces the preliminary results from the first phase of this project and then presents the proposed method for the research in progress that specifically is directed to the development and quantitative evaluation of a measurement scale of a culture sensitive measurement of website usability the recognition of the need to develop this scale resulted from the identification of culturerelated shortcomings of previous measurement tools that have been used widely within the management of information systems mis literature,http://arxiv.org/abs/1803.04074v1,2
in this paper we analyze the effects of commuting and social inequalities for the epidemic development of the novel coronavirus covid19 with this aim we consider a seird susceptible exposed infected recovered and dead by disease model without vital dynamics in a population divided into patches that have different economic resources and in which the individuals can commute from one patch to another bilaterally in the modeling we choose the social and commuting parameters arbitrarily we calculate the basic reproductive number r0 with the next generation approach and analyze the sensitivity of r0 with respect to the parameters furthermore we run numerical simulations considering a population divided into two patches to bring some conclusions on the number of total infected individuals and cumulative deaths for our model considering heterogeneous populations,http://arxiv.org/abs/2008.06718v1,2
we have studied few social inequality measures associated with the subcritical dynamical features measured in terms of the avalanche size distributions of four selforganized critical models while the corresponding systems approach their respective stationary critical states it has been observed that these inequality measures specifically the gini and kolkata indices exhibit nearly universal values though the models studied here are widely different namely the baktangwiesenfeld sandpile the manna sandpile and the quenched edwardswilkinson interface and the fiber bundle interface these observations suggest that the selforganized critical systems have broad similarity in terms of these inequality measures a comparison with similar earlier observations in the data of socioeconomic systems with unrestricted competitions suggest the emergent inequality as a result of the possible proximity to the selforganized critical states,http://arxiv.org/abs/2111.11230v3,2
the importance of the working document is that it allows the analysis of information and cases associated with sarscov2 covid19 based on the daily information generated by the government of mexico through the secretariat of health responsible for the epidemiological surveillance system for viral respiratory diseases sveerv the information in the sveerv is disseminated as open data and the level of information is displayed at the municipal state and national levels on the other hand the monitoring of the genomic surveillance of sarscov2 covid19 through the identification of variants and mutations is registered in the database of the information system of the global initiative on sharing all influenza data gisaid based in germany these two sources of information sveerv and gisaid provide the information for the analysis of the impact of sarscov2 covid19 on the population in mexico the first data source identifies information at the national level on patients according to age sex comorbidities and covid19 presence sarscov2 among other characteristics the data analysis is carried out by means of the design of an algorithm applying data mining techniques and methodology to estimate the case fatality rate positivity index and identify a typology according to the severity of the infection identified in patients who present a positive result for sarscov2 covid19 from the second data source information is obtained worldwide on the new variants and mutations of covid19 sarscov2 providing valuable information for timely genomic surveillance this study analyzes the impact of sarscov2 covid19 on the indigenous languagespeaking population it allows us to provide information quickly and in a timely manner to support the design of public policy on health,http://arxiv.org/abs/2112.01276v1,2
the covid19 pandemic sarscov2 virus is the defying global health crisis of our time the absence of mass testing and the relevant presence of asymptomatic individuals causes the available data of the covid19 pandemic in brazil to be largely underreported regarding the number of infected individuals and deaths we propose an adapted susceptibleinfectedrecovered sir model which explicitly incorporates the underreporting and the response of the population to public policies such as confinement measures widespread use of masks etc to cast shortterm and longterm predictions large amounts of uncertainty could provide misleading models and predictions in this paper we discuss the role of uncertainty in these prediction which is illustrated regarding three key aspects first assuming that the number of infected individuals is underreported we demonstrate an anticipation regarding the peak of infection furthermore while a model with a single class of infected individuals yields forecasts with increased peaks a model that considers both symptomatic and asymptomatic infected individuals suggests a decrease of the peak of symptomatic second considering that the actual amount of deaths is larger than what is being register then demonstrate the increase of the mortality rates third when consider generally underreported data we demonstrate how the transmission and recovery rate model parameters change qualitatively and quantitatively we also investigate the effect of the covid19 underreporting tripod ie the underreporting in terms of infected individuals of deaths and the true mortality rate if two of these factors are known the remainder can be inferred as long as proportions are kept constant the proposed approach allows one to determine the margins of uncertainty by assessments on the observed and true mortality rates,http://arxiv.org/abs/2006.15268v1,2
quantifying variable importance is essential for answering highstakes questions in fields like genetics public policy and medicine current methods generally calculate variable importance for a given model trained on a given dataset however for a given dataset there may be many models that explain the target outcome equally well without accounting for all possible explanations different researchers may arrive at many conflicting yet equally valid conclusions given the same data additionally even when accounting for all possible explanations for a given dataset these insights may not generalize because not all good explanations are stable across reasonable data perturbations we propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics we demonstrate through experiments that our framework recovers variable importance rankings for complex simulation setups where other methods fail further we show that our framework accurately estimates the true importance of a variable for the underlying data distribution we provide theoretical guarantees on the consistency and finite sample error rates for our estimator finally we demonstrate its utility with a realworld case study exploring which genes are important for predicting hiv load in persons with hiv highlighting an important gene that has not previously been studied in connection with hiv code is available at,http://arxiv.org/abs/2309.13775v4,2
samerace mentorship preference refers to mentors or mentees forming connections significantly influenced by a shared race although racial diversity in science has been wellstudied and linked to favorable outcomes the extent and effects of samerace mentorship preferences remain largely underexplored here we analyze 465355 mentormentee pairs from more than 60 research areas over the last 70 years to investigate the effect of samerace mentorship preferences on mentees academic performance and survival we use causal inference and statistical matching to measure samerace mentorship preferences while accounting for racial demographic variations across institutions time periods and research fields our findings reveal a pervasive samerace mentorship propensity across races fields and universities of varying research intensity we observe an increase in samerace mentorship propensity over the years further reinforced intergenerationally within a mentorship lineage this propensity is more pronounced for minorities asians blacks and hispanics our results reveal that mentees under the supervision of mentors with high samerace propensity experience significantly lower productivity impact and collaboration reach during and after training ultimately leading to a 276 reduced likelihood of remaining in academia in contrast a mentorship approach devoid of racial propensity appears to offer the best prospects for academic performance and persistence these findings underscore the importance of mentorship diversity for academic success and shed light on factors contributing to minority underrepresentation in science,http://arxiv.org/abs/2310.09453v2,2
the design of coherent and efficient policies to address infectious diseases and their consequences requires to model not only epidemics dynamics but also individual behaviors as the latter has a strong influence on the former in our work we provide a theoretical model for this problem taking into account the social structure of a population this model is based on a mean field game version of a sir compartmental model in which individuals are grouped by their age class and interact together in different settings this social heterogeneity allows to reproduce realistic situations while remaining usable in practice in our game theoretical approach individuals can choose to limit their contacts by making a tradeoff between the risks incurred by infection and the cost of being confined the aggregation of all these individual choices and optimizations forms a nash equilibrium through a system of coupled equations that we derive and solve numerically the global cost born by the population within this scenario is then compared to its societal optimum counterpart ie the optimal cost from the society viewpoint and we investigate how the gap between these two costs can be partially bridged within a constrained nash equilibrium for which a governmental institution would impose lockdowns finally we consider the consequences of the finiteness of the population size n or of a time t at which an external event would end the epidemic and show that the variation of these parameters could lead to first order phase transitions in the choice of optimal strategies in this paper all the strategies considered to mitigate epidemics correspond to nonpharmaceutical interventions npi and we provide here a theoretical framework within which guidelines for public policies depending on the characteristics of an epidemic and on the cost of restrictions on the society could be assessed,http://arxiv.org/abs/2404.08758v1,2
public policy involves proposing changes to existing practices alternatives new habits citizens and institutions react accordingly accepting refuting or adapting agentbased modeling is a tool that can enrich the policy analysis package explicitly considering dynamics space and individuallevel interactions this paper presents a modeling platform called policyspace that models public policies within an empirical spatial environment using data from 46 metropolitan regions in brazil we describe the basics of the model its agents and markets the tax scheme the parametrization and how to run the model finally we validate the model and demonstrate an application of the fiscal analysis besides providing the basics of the platform our results indicate the relevance of the rules of taxes transfer for cities quality of life,http://arxiv.org/abs/1801.00259v1,2
in this study we performed an initial investigation and evaluation of altmetrics and their relationship with public policy citation of research papers we examined methods for using altmetrics and other data to predict whether a research paper is cited in public policy and applied receiver operating characteristic curve on various feature groups in order to evaluate their potential usefulness from the methods we tested classifying based on tweet count provided the best results achieving an area under the roc curve of 091,http://arxiv.org/abs/1708.01658v1,2
this paper has been withdrawn by the authors due to the violation of atlas experiment publication policy,http://arxiv.org/abs/0705.2001v2,2
rejoinder of bayesian models and methods in public policy and government settings by s e fienberg arxiv11082177,http://arxiv.org/abs/1108.3914v1,2
explainability is highlydesired in machine learning ml systems supporting highstakes policy decisions in areas such as health criminal justice education and employment while the field of explainable ml has expanded in recent years much of this work has not taken realworld needs into account a majority of proposed methods are designed with textitgeneric explainability goals without welldefined usecases or intended endusers and evaluated on simplified tasks benchmark problemsdatasets or with proxy users eg amt we argue that these simplified evaluation settings do not capture the nuances and complexities of realworld applications as a result the applicability and effectiveness of this large body of theoretical and methodological work in realworld applications are unclear in this work we take steps toward addressing this gap for the domain of public policy first we identify the primary usecases of explainable ml within public policy problems for each use case we define the endusers of explanations and the specific goals the explanations have to fulfill finally we map existing work in explainable ml to these usecases identify gaps in established capabilities and propose research directions to fill those gaps to have a practical societal impact through ml the contribution is 1 a methodology for explainable ml researchers to identify use cases and develop methods targeted at them and 2 using that methodology for the domain of public policy and giving an example for the researchers on developing explainable ml methods that result in realworld impact,http://arxiv.org/abs/2010.14374v3,2
objective the aim of this research is to demonstrate how the use of hierarchical cluster analysis on 366 municipalities and other minor entities parishes of venezuela could be useful to consider regional differences and similarities between territorial entities when designing national public policies of water sanitation and hygiene wash based on evidence methods and results consider data from various sources to characterize the population of venezuela through their territorial entities select variables at the level of the territorial entities to cover demographic characteristics mortality and nutrition coverage of reliable water and sanitation services access to education and access to information and communication technologies classify the territorial entities into a limited number of mutually exclusive groups using hierarchical clustering techniques and based on proximity in the multidimensional space adjust of assignments reallocating some entities into a different group based on the specialists opinion about its hierarchy in the cities regional system and its geographic location define an indicator to verify the consistency of the groups built conduct a statistical analysis to confirm separation of the groups demonstrate the utility of the results with some examples of common analysis when building a sanitary public policy using seven distinct groups of recommendations depending on each cluster conclusions cluster analysis can be a useful method to analyse relevant differences between territorial entities when designing national public policies based on evidence,http://arxiv.org/abs/2301.12604v1,2
this position paper offers a framework to think about how to better involve human influence in algorithmic decisionmaking of contentious public policy issues drawing from insights in communication literature we introduce a publicsintheloop approach and enumerates three features that are central to this approach publics as plural political entities collective decisionmaking through deliberation and the construction of publics it explores how these features might advance our understanding of stakeholder participation in ai design in contentious public policy domains such as recidivism prediction finally it sketches out part of a research agenda for the hci community to support this work,http://arxiv.org/abs/2204.10814v1,2
this document has been prepared as a snowmass contributed paper by the public policy government engagement topical group cef06 within the community engagement frontier the charge of cef06 is to review all aspects of how the high energy physics hep community engages with government at all levels and how public policy impacts members of the community and the community at large and to assess and raise awareness within the community of direct communitydriven engagement of the us federal government textitie advocacy the focus of this paper is the advocacy undertaken by the hep community that pertains directly to the funding of the field by the us federal government,http://arxiv.org/abs/2207.00122v2,2
this document has been prepared as a snowmass contributed paper by the public policy government engagement topical group cef06 within the community engagement frontier the charge of cef06 is to review all aspects of how the high energy physics hep community engages with government at all levels and how public policy impacts members of the community and the community at large and to assess and raise awareness within the community of direct communitydriven engagement of the us federal government textitie advocacy the focus of this paper is the potential for hep community advocacy on topics other than funding for basic research,http://arxiv.org/abs/2207.00124v2,2
this document has been prepared as a snowmass contributed paper by the public policy government engagement topical group cef06 within the community engagement frontier the charge of cef06 is to review all aspects of how the high energy physics hep community engages with government at all levels and how public policy impacts members of the community and the community at large and to assess and raise awareness within the community of direct communitydriven engagement of the us federal government ie advocacy the focus of this paper is hep community engagement of government entities other than the us federal legislature ie congress,http://arxiv.org/abs/2207.00125v2,2
blackbox optimization bbo has become increasingly relevant for tackling complex decisionmaking problems especially in public policy domains such as police redistricting however its broader application in public policymaking is hindered by the complexity of defining feasible regions and the highdimensionality of decisions this paper introduces a novel bbo framework termed as the conditional and generative blackbox optimization cagebo this approach leverages a conditional variational autoencoder to learn the distribution of feasible decisions enabling a twoway mapping between the original decision space and a simplified constraintfree latent space the cagebo efficiently handles the implicit constraints often found in public policy applications allowing for optimization in the latent space while evaluating objectives in the original space we validate our method through a case study on largescale police redistricting problems in atlanta georgia our results reveal that our cagebo offers notable improvements in performance and efficiency compared to the baselines,http://arxiv.org/abs/2310.18449v4,2
journalists must find stories in huge amounts of textual data eg leaks bills press releases as part of their jobs determining when and why text becomes news can help us understand coverage patterns and help us build assistive tools yet this is challenging because very few labelled links exist language use between corpora is very different and text may be covered for a variety of reasons in this work we focus on news coverage of local public policy in the san francisco bay area by the san francisco chronicle first we gather news articles public policy documents and meeting recordings and link them using probabilistic relational modeling which we show is a lowannotation linking methodology that outperforms other retrievalbased baselines second we define a new task newsworthiness prediction to predict if a policy item will get covered we show that different aspects of public policy discussion yield different newsworthiness signals finally we perform human evaluation with expert journalists and show our systems identify policies they consider newsworthy with 68 f1 and our coverage recommendations are helpful with an 84 winrate,http://arxiv.org/abs/2311.09734v1,2
this study investigates the impact of disinformation on public policies using 28 sets of keywords in eight databases a systematic review was carried out following the prisma 2020 model page et al 2021 after applying filters and inclusion and exclusion criteria to 4128 articles and materials found 46 publications were analyzed resulting in 23 disinformation impact categories these categories were organized into two main axes state and society and actors and dynamics covering impacts on state actors society actors state dynamics and society dynamics the results indicate that disinformation affects public decisions adherence to policies prestige of institutions perception of reality consumption public health and other aspects furthermore this study suggests that disinformation should be treated as a public problem and incorporated into the public policy research agenda contributing to the development of strategies to mitigate its effects on government actions,http://arxiv.org/abs/2406.00951v1,2
the implementation of public policies is crucial in controlling the spread of covid19 however the effectiveness of different policies can vary across different aspects of epidemic containment identifying the most effective policies is essential for providing informed recommendations for pandemic control this paper examines the relationship between various public policy responses and their impact on covid19 containment using the propensity score matchingdifference in differences psmdid model to address endogeneity we analyze the causal significance of each policy on epidemic control our analysis reveals that that policies related to vaccine delivery debt relief and the cancellation of public events are the most effective measures these findings provide key insights for policymakers highlighting the importance of focusing on specific highimpact measures in managing public health crises,http://arxiv.org/abs/2408.14108v1,2
this article surveys the use of algorithmic systems to support decisionmaking in the public sector governments adopt procure and use algorithmic systems to support their functions within several contexts including criminal justice education and benefits provision with important consequences for accountability privacy social inequity and public participation in decisionmaking we explore the social implications of municipal algorithmic systems across a variety of stages including problem formulation technology acquisition deployment and evaluation we highlight several open questions that require further empirical research,http://arxiv.org/abs/2106.03673v2,2
this study analyzes the role of meritocracy media influence and scheduled theory from multiple perspectives as mechanisms that maintain inequality in social classes social inequality exists in complex forms in the educational media and political spheres the study focuses on how inequality in society is structured and reproduced and how the theory of scheduled harmony justifies this,http://arxiv.org/abs/2311.02445v2,2
this paper is a critical reflection on the epistemic culture of contemporary computational linguistics framed in the context of its growing obsession with tables with numbers we argue against tables with numbers on the basis of their epistemic irrelevance their environmental impact their role in enabling and exacerbating social inequalities and their deep ties to commercial applications and profitdriven research we substantiate our arguments with empirical evidence drawn from a metaanalysis of computational linguistics research over the last decade,http://arxiv.org/abs/2408.06062v3,2
the book argues ict are part of the set of goods and services that determine quality of life social inequality and the chances for economic development therefore understanding the digital divide demands a broader discussion of the place of ict within each society and in the international system the author argues against the perspectives that either isolates ict from other basic social goods in particular education and employment as well as those that argue that new technologies are luxury of a consumer society though the author accepts that new technologies are not a panacea for the problems of inequality access to them become a condition of full integration of social life using examples mainly from latin america the work presents some general policy proposals on the fight against the digital divide which take in consideration other dimensions of social inequality and access to public goods bernardo sorj was born in montevideo uruguay he is a naturalized brazilian living in brazil since 1976 he studied anthropology and philosophy in uruguay and holds a ba and an ma in history and sociology from haifa university israel he received his phd in sociology from the university of manchester in england sorj was a professor at the department of political science at the federal university of minas gerais and at the institute for international relations pucrj the author of 20 books and more than 100 articles was visiting professor and chair at many european and north american universities,http://arxiv.org/abs/0807.2743v1,2
social inequality manifested across different strata of human existence can be quantified in several ways here we compute nonentropic measures of inequality such as lorenz curve gini index and the recently introduced k index analytically from known distribution functions we characterize the distribution functions of different quantities such as votes journal citations city size etc with suitable fits compute their inequality measures and compare with the analytical results a single analytic function is often not sufficient to fit the entire range of the probability distribution of the empirical data and fit better to two distinct functions with a single crossover point here we provide general formulas to calculate these inequality measures for the above cases we attempt to specify the crossover point by minimizing the gap between empirical and analytical evaluations of measures regarding the k index as an extra dimension both the lower and upper bounds of the gini index are obtained as a function of the k index this type of inequality relations among inequality indices might help us to check the validity of empirical and analytical evaluations of those indices,http://arxiv.org/abs/1406.2874v1,2
controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness these debates rarely engage with how racial identity is embedded in our social experience making for sociological and psychological complexity this complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes racial identity is not simply a personal subjective quality for people labeled black it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation in the united states racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the negroblack category as stigmatized social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state corporate and civic institutions and practices this creates a dilemma for society and designers be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality or be conscious of racial categories in a way that itself reifies race we propose a third option by preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation machine learning systems can mitigate the root cause of social disparities social segregation and stratification without further anchoring status categories of disadvantage,http://arxiv.org/abs/1811.11668v1,2
human deaths caused by individual manmade conflicts eg wars armedconflicts terroristattacks etc occur unequally across the events conflicts and such inequality in deaths have been studied here using lorenz curve and values of the inequality indices gini g and kolkata k have been estimated from it the data are taken from various wellknown databases maintained by some universities and peace research institutes the inequality measures for manmade conflicts are found to have very high values g 082 pm 002 k 084 pm 002 which is rarely seen in economic income or wealth inequality measures across the world g leq 04 k leq 06 presumably because of various welfare measures we also investigated the inequalities in human deaths from natural disasters like earthquakes floods etc interestingly we observe that the social inequality measures g and k values from manmade conflicts compare well with those of academic centers inequality in citations found in earlier studies of different institutions of the world while those for natural disasters can be even higher we discuss about the similarity classes of social inequality similar higher values of g and k indices for manmade competitive societies like academic institutions and manmade social conflicts and connect our observations with that of the growing recent trend of economic inequality across the world with rapid disappearance of welfare strategies,http://arxiv.org/abs/1812.05709v8,2
social inequalities are ubiquitous and here we show that the values of the gini g and kolkata k indices two generic inequality indices approach each other starting from g 0 and k 05 for equality as the competitions grow in various social institutions like markets universities elections etc it is further showed that these two indices become equal and stabilize at a value at g k simeq 087 under unrestricted competitions we propose to view this coincidence of inequality indices as a generalized version of the more than a century old 8020 law of pareto furthermore the coincidence of the inequality indices noted here is very similar to the ones seen before for selforganized critical soc systems the observations here therefore stand as a quantitative support towards viewing interacting socioeconomic systems in the framework of soc an idea conjectured for years,http://arxiv.org/abs/2111.07516v5,2
statistical physicists and social scientists both study extensively some characteristic features of the unequal distributions of energy cluster or avalanche sizes and of income wealth etc among the particles or sites and population respectively while physicists concentrate on the selfsimilar fractal structure and the characteristic exponents of the largest percolating cluster or avalanche social scientists study the inequality indices like gini and kolkata etc given by the nonlinearity of the lorenz function representing the cumulative fraction of the wealth possessed by different fraction of the population we review here using results from earlier publications and some new numerical as well as analytical results how the abovementioned social inequality indices when extracted from the unequal distributions of energy in kinetic exchange models cluster sizes in percolation models or avalanche sizes in selforganized critical or fiber bundle models can help in a major way in providing precursor signals for an approaching critical point or imminent failure point extensive numerical and some analytical results have been discussed,http://arxiv.org/abs/2207.04276v3,2
visualization research often focuses on perceptual accuracy or helping readers interpret key messages however we know very little about how chart designs might influence readers perceptions of the people behind the data specifically could designs interact with readers social cognitive biases in ways that perpetuate harmful stereotypes for example when analyzing social inequality bar charts are a popular choice to present outcome disparities between race gender or other groups but bar charts may encourage deficit thinking the perception that outcome disparities are caused by groups personal strengths or deficiencies rather than external factors these faulty personal attributions can then reinforce stereotypes about the groups being visualized we conducted four experiments examining design choices that influence attribution biases and therefore deficit thinking crowdworkers viewed visualizations depicting social outcomes that either mask variability in data such as bar charts or dot plots or emphasize variability in data such as jitter plots or prediction intervals they reported their agreement with both personal and external explanations for the visualized disparities overall when participants saw visualizations that hide withingroup variability they agreed more with personal explanations when they saw visualizations that emphasize withingroup variability they agreed less with personal explanations these results demonstrate that data visualizations about social inequity can be misinterpreted in harmful ways and lead to stereotyping design choices can influence these biases hiding variability tends to increase stereotyping while emphasizing variability reduces it,http://arxiv.org/abs/2208.04440v2,2
this twopart paper presents a new approach to predictive analysis for social processes part i identifies a class of social processes called positive externality processes which are both important and difficult to predict and introduces a multiscale stochastic hybrid system modeling framework for these systems in part ii of the paper we develop a systems theorybased computationally tractable approach to predictive analysis for these systems among other capabilities this analytic methodology enables assessment of process predictability identification of measurables which have predictive power discovery of reliable early indicators for events of interest and robust scalable prediction the potential of the proposed approach is illustrated through case studies involving online markets social movements and protest behavior,http://arxiv.org/abs/0907.2313v1,2
the spread of ideas across a social network can be studied using complex contagion models in which agents are activated by contact with multiple activated neighbors the investigation of complex contagions can provide crucial insights into social influence and behavioradoption cascades on networks in this paper we introduce a model of a multistage complex contagion on networks agents at different stages which could for example represent differing levels of support for a social movement or differing levels of commitment to a certain product or idea exert different amounts of influence on their neighbors we demonstrate that the presence of even one additional stage introduces novel dynamical behavior including interplay between multiple cascades that cannot occur in singlestage contagion models we find that cascades and hence collective action can be driven not only by highstage influencers but also by lowstage influencers,http://arxiv.org/abs/1111.1596v2,2
using social media archives of the 2011 chilean student unrest and dynamic social network analysis we study how leaders and participants use social media such as twitter and the web to selforganize and communicate with each other and thus generate one of the biggest smart movements in the history of chile in this paper we i describe the basic network topology of the 2011 studentled social movement in chile ii explore how the student leaders are connected to and how are they seen by a political leaders and b university authorities iii hypothesize about key success factors and risk variables for the student network movements organization process and sustainability over time we contend that this social media enabled massive movement is yet another manifestation of the network era which leverages agents sociotechnical networks and thus accelerates how agents coordinate mobilize resources and enact collective intelligence,http://arxiv.org/abs/1204.3939v1,2
we examine the temporal evolution of digital communication activity relating to the american anticapitalist movement occupy wall street using a highvolume sample from the microblogging site twitter we investigate changes in occupy participant engagement interests and social connectivity over a fifteen month period starting three months prior to the movements first protest action the results of this analysis indicate that on twitter the occupy movement tended to elicit participation from a set of highly interconnected users with preexisting interests in domestic politics and foreign social movements these users while highly vocal in the months immediately following the birth of the movement appear to have lost interest in occupy related communication over the remainder of the study period,http://arxiv.org/abs/1306.5474v1,2
how does political discourse spread in digital networks can we empirically test if certain conceptual frames of social movements have a correlate on their online discussion networks through an analysis of the twitter data from the occupy movement this paper describes the formation of political discourse over time building on a previous set of concepts derived from theoretical discussions about the movement and its roots we analyse the data to observe when those concepts start to appear within the networks who are those twitter users responsible for them and what are the patterns through which those concepts spread preliminary evidence shows that although there are some signs of opportunistic behaviour among activists most of them are central nodes from the onset of the network and shape the discussions across time these central activists do not only start the conversations around given frames but also sustain over time and become key members of the network from here we aim to provide a thorough account of the travel of political discourse and the correlate of online conversational networks with theoretical accounts of the movement,http://arxiv.org/abs/1308.1176v1,2
searching for influential spreaders in complex networks is an issue of great significance for applications across various domains ranging from the epidemic control innovation diffusion viral marketing social movement to idea propagation in this paper we first display some of the most important theoretical models that describe spreading processes and then discuss the problem of locating both the individual and multiple influential spreaders respectively recent approaches in these two topics are presented for the identification of privileged single spreaders we summarize several widely used centralities such as degree betweenness centrality pagerank kshell etc we investigate the empirical diffusion data in a large scale online social community livejournal with this extensive dataset we find that various measures can convey very distinct information of nodes of all the users in livejournal social network only a small fraction of them involve in spreading for the spreading processes in livejournal while degree can locate nodes participating in information diffusion with higher probability kshell is more effective in finding nodes with large influence our results should provide useful information for designing efficient spreading strategies in reality,http://arxiv.org/abs/1312.6335v1,2
peer production projects like wikipedia have inspired voluntary associations collectives social movements and scholars to embrace open online collaboration as a model of democratic organization however many peer production projects exhibit entrenched leadership and deep inequalities suggesting that they may not fulfill democratic ideals instead peer production projects may conform to robert michels iron law of oligarchy which proposes that democratic membership organizations become increasingly oligarchic as they grow using exhaustive data of internal processes from a sample of 683 wikis we construct empirical measures of participation and test for increases in oligarchy associated with growth in contrast to previous studies we find support for michels iron law and conclude that peer production entails oligarchic organizational forms,http://arxiv.org/abs/1407.0323v1,2
the 5th annual international conference on collaborative innovation networks conference coins takes place at keio university from march 12 to 14 2015 coins15 brings together practitioners researchers and students of the emerging science of collaboration to share their work learn from each other and get inspired through creative new ideas where science design business and art meet coins15 looks at the emerging forces behind the phenomena of opensource creative entrepreneurial and social movements through interactive workshops professional presentations and fascinating keynotes coins15 combines a wide range of interdisciplinary fields such as social network analysis group dynamics design and visualization information systems collective action and the psychology and sociality of collaboration,http://arxiv.org/abs/1502.01142v1,2
social media has emerged to be a popular platform for people to express their viewpoints on political protests like the arab spring millions of people use social media to communicate and mobilize their viewpoints on protests hence it is a valuable tool for organizing social movements however the mechanisms by which protest affects the population is not known making it difficult to estimate the number of protestors in this paper we are inspired by sociological theories of protest participation and propose a framework to predict from the users past status messages and interactions whether the next post of the user will be a declaration of protest drawing concepts from these theories we model the interplay between the users status messages and messages interacting with him over time and predict whether the next post of the user will be a declaration of protest we evaluate the framework using data from the social media platform twitter on protests during the recent nigerian elections and demonstrate that it can effectively predict whether the next post of a user is a declaration of protest,http://arxiv.org/abs/1512.02968v1,2
given the centrality of regions in social movements politics and public administration we aim to quantitatively study inter and intraregional communication for the first time this work uses social media posts to first identify contiguous geographical regions with a shared social identity and then investigate patterns of communication within and between them our case study uses over 150 days of located twitter data from england and wales in contrast to other approaches eg phone call data records or online friendship networks we have the message contents as well as the social connection this allows us to investigate not only the volume of communication but also the sentiment and vocabulary we find that the southeast and northwest regions are the most talked about regions tend to be more positive about themselves than about others people talk politics much more between regions than within this methodology gives researchers a powerful tool to study identity and interaction within and between socialgeographic regions,http://arxiv.org/abs/1807.04107v1,2
advancements in information and communication technologies have enhanced our possibilities to communicate worldwide eliminating borders and making it possible to interact with people coming from other cultures like never happened before such powerful tools have brought us to reconsider our concept of privacy and social involvement in order to make them fit into this wider environment it is possible to claim that the ict revolution is changing our world and is having a core role as a mediating factor for social movements eg arab spring and political decisions eg brexit shaping the world in a faster and shared brand new way it is then interesting to explore how the perception of this brand new environment in terms of social engagement privacy perception and sense of belonging to a community differs even in similar cultures separated by recent historical reasons as for example in italian and turkish cultures,http://arxiv.org/abs/1607.06721v1,2
in this paper we examine the most influential resources shared on twitter about the metoo movement we also examine whether a small proportion of domain names and urls eg 20 appear in a large number of tweets eg 80 that contain metoo known as the 8020 rule or pareto principle r and python were used to analyze the data results demonstrated that the most frequently shared domains were twittercom 4720 nytimescom 442 and youtubecom 369 the most frequently shared content was a recent poll which indicated men are afraid to mentor women after the metoo movement in accordance with the pareto principle 8 of domain names accounted for 80 of the shared content on twitter that contained metoo this study provides a base for researchers who are interested in understanding what online resources people rely on when sharing information about online social movements eg metoo,http://arxiv.org/abs/1906.12321v2,2
in this paper we present a dataset containing 9973 tweets related to the metoo movement that were manually annotated for five different linguistic aspects relevance stance hate speech sarcasm and dialogue acts we present a detailed account of the data collection and annotation processes the annotations have a very high interannotator agreement 079 to 093 kalpha due to the domain expertise of the annotators and clear annotation instructions we analyze the data in terms of geographical distribution label correlations and keywords lastly we present some potential use cases of this dataset we expect this dataset would be of great interest to psycholinguists sociolinguists and computational linguists to study the discursive space of digitally mobilized social movements on sensitive issues like sexual harassment,http://arxiv.org/abs/1912.06927v2,2
this paper investigates the contribution of business model innovations in improvement of food supply chains through a systematic literature review the notable business model innovations in the food industry are identified surveyed and evaluated findings reveal that the innovations in value proposition value creation processes and value delivery processes of business models are the successful strategies proposed in food industry it is further disclosed that rural female entrepreneurs social movements and also urban conditions are the most important driving forces inducing the farmers to reconsider their business models in addition the new technologies and environmental factors are the secondary contributors in business model innovation for the food processors it is concluded that digitalization has disruptively changed the food distributors models ecommerce models and internet of things are reported as the essential factors imposing the retailers to innovate their business models furthermore the consumption demand and the product quality are two main factors affecting the business models of all the firms operating in the food supply chain regardless of their positions in the chain the findings of the current study provide an insight into the food industry to design a sustainable business model to bridge the gap between food supply and food demand,http://arxiv.org/abs/2001.03982v1,2
extracting moral sentiment from text is a vital component in understanding public opinion social movements and policy decisions the moral foundation theory identifies five moral foundations each associated with a positive and negative polarity however moral sentiment is often motivated by its targets which can correspond to individuals or collective entities in this paper we introduce morality frames a representation framework for organizing moral attitudes directed at different entities and come up with a novel and highquality annotated dataset of tweets written by us politicians then we propose a relational learning model to predict moral attitudes towards entities and moral foundations jointly we do qualitative and quantitative evaluations showing that moral sentiment towards entities differs highly across political ideologies,http://arxiv.org/abs/2109.04535v1,2
this paper investigates how hate speech varies in systematic ways according to the identities it targets across multiple hate speech datasets annotated for targeted identities we find that classifiers trained on hate speech targeting specific identity groups struggle to generalize to other targeted identities this provides empirical evidence for differences in hate speech by target identity we then investigate which patterns structure this variation we find that the targeted demographic category eg gendersexuality or raceethnicity appears to have a greater effect on the language of hate speech than does the relative social power of the targeted identity group we also find that words associated with hate speech targeting specific identities often relate to stereotypes histories of oppression current social movements and other social contexts specific to identities these experiments suggest the importance of considering targeted identity as well as the social contexts associated with these identities in automated hate speech classification,http://arxiv.org/abs/2210.10839v2,2
in this paper we use mixed methods to study a controversial internet site the kotaku in action kia subreddit members of kia are part of gamergate a distributed social movement we present an emic account of what takes place on kia who are they what are their goals and beliefs and what rules do they follow members of gamergate in general and kia in particular have often been accused of harassment however kia site policies explicitly prohibit such behavior and members insist that they have been falsely accused underlying the controversy over whether kia supports harassment is a complex disagreement about what harassment is and where to draw the line between freedom of expression and censorship we propose a model that characterizes perceptions of controversial speech dividing it into four categories criticism insult public shaming and harassment we also discuss design solutions that address the challenges of moderating harassment without impinging on free speech and communicating across different ideologies,http://arxiv.org/abs/1712.05851v2,2
users on twitter are identified with the help of their profile attributes that consists of username display name profile image to name a few the profile attributes that users adopt can reflect their interests belief or thematic inclinations literature has proposed the implications and significance of profile attribute change for a random population of users however the use of profile attribute for endorsements and to start a movement have been underexplored in this work we consider loksabhaelections2019 as a movement and perform a largescale study of the profile of users who actively made changes to profile attributes centered around loksabhaelections2019 we collect the profile metadata for 494m users for a period of 2 months from april 5 2019 to june 5 2019 amid loksabhaelections2019 we investigate how the profile changes vary for the influential leaders and their followers over the social movement we further differentiate the organic and inorganic ways to show the political inclination from the prism of profile changes we report how the addition of election campaign related keywords lead to spread of behavior contagion and further investigate it with respect to chowkidar movement in detail,http://arxiv.org/abs/1909.10012v1,2
3d human dance motion is a cooperative and elegant social movement unlike regular simple locomotion it is challenging to synthesize artistic dance motions due to the irregularity kinematic complexity and diversity it requires the synthesized dance is realistic diverse and controllable in this paper we propose a novel generative motion model based on temporal convolution and lstmtclstm to synthesize realistic and diverse dance motion we introduce a unique control signal dance melody line to heighten controllability hence our model and its switch for control signals promote a variety of applications random dance synthesis musictodance user control and more our experiments demonstrate that our model can synthesize artistic dance motion in various dance types compared with existing methods our method achieved startoftheart results,http://arxiv.org/abs/2006.05743v1,2
open source development to a great extent is a type of social movement in which shared ideologies play critical roles for participants of open source development ideology determines how they make sense of things shapes their thoughts actions and interactions enables rich social dynamics in their projects and communities and hereby realizes profound impacts at both individual and organizational levels while software engineering researchers have been increasingly recognizing ideologys importance in open source development the notion of ideology has shown significant ambiguity and vagueness and resulted in theoretical and empirical confusion in this article we first examine the historical development of ideologys conceptualization and its theories in multiple disciplines then we review the extant software engineering literature related to ideology we further argue the imperatives of developing an empirical theory of ideology in open source development and propose a research agenda for developing such a theory how such a theory could be applied is also discussed,http://arxiv.org/abs/2104.12732v1,2
who actually expresses an intent to buy gamestop shares on reddit what convinces people to buy stocks are people convinced to support a coordinated plan to adversely impact wall street investors existing literature on understanding intent has mainly relied on surveys and self reporting however there are limitations to these methodologies hence in this paper we develop an annotated dataset of communications centered on the gamestop phenomenon to analyze the subscriber intentions behaviors within the rwallstreetbets community to buy or not buy stocks likewise we curate a dataset to better understand how intent interacts with a users general support towards the coordinated actions of the community for gamestop overall our dataset can provide insight to social scientists on the persuasive power to buy into social movements online by adopting common language and narrative warning this paper contains offensive language that commonly appears on reddits rwallstreetbets subreddit,http://arxiv.org/abs/2203.08694v1,2
the covid19 pandemic has intensified numerous social issues that warrant academic investigation although information dissemination has been extensively studied the silenced voices and censored content also merit attention due to their role in mobilizing social movements in this paper we provide empirical evidence to explore the relationships among covid19 regulations censorship and protest through a series of social incidents occurred in china during 2022 we analyze the similarities and differences between censored articles and discussions on rchinairl the most popular chinesespeaking subreddit and scrutinize the temporal dynamics of government censorship activities and their impact on user engagement within the subreddit furthermore we examine users linguistic patterns under the influence of a censorshipdriven environment our findings reveal patterns in topic recurrence the complex interplay between censorship activities user subscription and collective commenting behavior as well as potential linguistic adaptation strategies to circumvent censorship these insights hold significant implications for researchers interested in understanding the survival mechanisms of marginalized groups within censored information ecosystems,http://arxiv.org/abs/2304.02800v1,2
during the 1950s scandinavian design caught international attention with its minimalism simplicity functionalism and sophistication several factors rested at its heart functionality democracy and affordability aesthetic styles connected to international minimalist modernist and functionalist movements which were also symbolically connected to education political and social movements and the nordic welfare model studies have shown how social and political values from this period connect with nordic interaction design from the past three decades how these are represented in contemporary interaction design discourse and design form and expression is a perspective under represented this paper presents the results of a threetiered content analysis of the proceedings of nordichi years 20002014 categorization of titles according to emphasis content analysis of scandinavian value constructs overall and thematic connection of the results to conference theme and site results are then discussed with reflection on form and process in the nordic interaction design industry,http://arxiv.org/abs/2304.06820v1,2
climate change is the defining issue of our time and we are at a defining moment various interest groups social movement organizations and individuals engage in collective action on this issue on social media in addition issue advocacy campaigns on social media often arise in response to ongoing societal concerns especially those faced by energy industries our goal in this paper is to analyze how those industries their advocacy group and climate advocacy group use social media to influence the narrative on climate change in this work we propose a minimally supervised model soup 57 approach combined with messaging themes to identify the stances of climate ads on facebook finally we release our stance dataset model and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances,http://arxiv.org/abs/2305.06174v2,2
the formation mechanisms and cyclical conditions of collective action have become open issues in research involving public choice social movements and more for this reason on the basis of rational decisionmaking and social assimilation this paper proposes an action model that combines bayesian game and social network dynamics and incorporates exogenous cycles into it for this model this paper proves the spontaneous action theorem and action cycle theorem of collective action and based on numerical simulation and empirical calibration further confirms the theoretical mechanism involving elements such as riskriskfree incentives and the number of social ties based on such conclusions and evidence this paper proposes a theory of spontaneous cycles as an integrative answer to the open question of collective action formationcycles,http://arxiv.org/abs/2308.01791v1,2
social media has become a major driver of social change by facilitating the formation of online social movements automatically understanding the perspectives driving the movement and the voices opposing it is a challenging task as annotated data is difficult to obtain we propose a weakly supervised graphbased approach that explicitly models perspectives in backlivesmatterrelated tweets our proposed approach utilizes a sociallinguistic representation of the data we convert the text to a graph by breaking it into structured elements and connect it with the social network of authors then structured prediction is done over the elements for identifying perspectives our approach uses a small seed set of labeled examples we experiment with large language models for generating artificial training examples compare them to manual annotation and find that it achieves comparable performance we perform quantitative and qualitative analyses using a humanannotated test set our model outperforms multitask baselines by a large margin successfully characterizing the perspectives supporting and opposing blm,http://arxiv.org/abs/2310.07155v2,2
online memes have emerged as powerful digital cultural artifacts in the age of social media offering not only humor but also platforms for political discourse social critique and information dissemination their extensive reach and influence in shaping online communities sentiments make them invaluable tools for campaigning and promoting ideologies despite the development of several memegeneration tools there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies addressing this we introduce memecraft an innovative meme generator that leverages large language models llms and visual language models vlms to produce memes advocating specific social movements memecraft presents an endtoend pipeline transforming user prompts into compelling multimodal memes without manual intervention conscious of the misuse potential in creating divisive content an intrinsic safety mechanism is embedded to curb hateful meme production,http://arxiv.org/abs/2403.14652v1,2
understanding the mechanisms behind opinion formation is crucial for gaining insight into the processes that shape political beliefs cultural attitudes consumer choices and social movements this work aims to explore a nuanced model that captures the intricacies of realworld opinion dynamics by synthesizing principles from cognitive science and employing social network analysis the proposed model is a hybrid continuousdiscrete extension of the wellknown naming game opinion model the added latent continuous layer of opinion strength follows cognitive processes in the human brain akin to memory imprints the discrete layer allows for the conversion of intrinsic continuous opinion into discrete form which often occurs when we publicly verbalize our opinions we evaluated our model using real data as ground truth and demonstrated that the proposed mechanism outperforms the classic naming game model in many cases reflecting that our model is closer to the real process of opinion formation,http://arxiv.org/abs/2406.19204v1,2
coordination is a fundamental aspect of life the advent of social media has made it integral also to online human interactions such as those that characterize thriving online communities and social movements at the same time coordination is also core to effective disinformation manipulation and hate campaigns this survey collects categorizes and critically discusses the body of work produced as a result of the growing interest on coordinated online behavior we reconcile industry and academic definitions propose a comprehensive framework to study coordinated online behavior and review and critically discuss the existing detection and characterization methods our analysis identifies open challenges and promising directions of research serving as a guide for scholars practitioners and policymakers in understanding and addressing the complexities inherent to online coordination,http://arxiv.org/abs/2408.01257v1,2
we are living in an age of protest although we have an excellent understanding of the factors that predict participation in protest we understand little about the conditions that foster a sustained versus transient movement how do interactions between supporters and authorities combine to influence whether and how people engage ie using conventional or radical tactics this paper introduces a novel theoreticallyfounded and empiricallyinformed agentbased model dimesim to address these questions we model the complex interactions between the psychological attributes of the protester agents the authority to whom the protests are targeted and the environment that allows protesters to coordinate with each other over time and at a population scale where an authority is responsive and failure is contested a modest sized conventional movement endured where authorities repeatedly and incontrovertibly fail the movement the population disengaged from action but evidenced an ongoing commitment to radicalism latent radicalism,http://arxiv.org/abs/2408.12795v1,2
the constant shifts in social and political contexts driven by emerging social movements and political events lead to new forms of hate content and previously unrecognized hate patterns that machine learning models may not have captured some recent literature proposes the data augmentationbased techniques to enrich existing hate datasets by incorporating samples that reveal new implicit hate patterns this approach aims to improve the models performance on outofdomain implicit hate instances it is observed that further addition of more samples for augmentation results in the decrease of the performance of the model in this work we propose a knowledge transferdriven concept refinement method that distills and refines the concepts related to implicit hate samples through novel prototype alignment and concept losses alongside data augmentation based on concept activation vectors experiments with several publicly available datasets show that incorporating additional implicit samples reflecting new hate patterns through concept refinement enhances the models performance surpassing baseline results while maintaining crossdataset generalization capabilitiesfootnotedisclaimer this paper contains explicit statements that are potentially offensive,http://arxiv.org/abs/2410.15314v1,2
the current covid19 pandemic has proven that proper control and prevention of infectious disease require creating and enforcing the appropriate public policies one critical policy imposed by the policymakers is encouraging the population to practice social distancing ie controlling the contact rate among the population here we pose a meanfield game model of individuals each choosing a dynamic strategy of making contacts given the tradeoff of gaining utility but also risking infection from additional contacts we compute and compare the meanfield equilibrium mfe strategy which assumes each individual acting selfishly to maximize its own utility to the socially optimal strategy which maximizes the total utility of the population we prove that the optimal decision of the infected is always to make more contacts than the level at which it would be socially optimal which reinforces the important role of public policy to reduce contacts of the infected eg quarantining sick paid leave additionally we include cost to incentivize people to change strategies when computing the socially optimal strategies we find that with this cost policies reducing contacts of the infected should be further enforced after the peak of the epidemic has passed lastly we compute the price of anarchy poa of this system to understand the conditions under which large discrepancies between the mfe and socially optimal strategies arise which is when intervening public policy would be most effective,http://arxiv.org/abs/2005.06758v2,2
artificial intelligence ai has an increasing impact on all areas of peoples livelihoods a detailed look at existing interdisciplinary and transdisciplinary metrics frameworks could bring new insights and enable practitioners to navigate the challenge of understanding and assessing the impact of autonomous and intelligent systems ais there has been emerging consensus on fundamental ethical and rightsbased ai principles proposed by scholars governments civil rights organizations and technology companies in order to move from principles to realworld implementation we adopt a lens motivated by regulatory impact assessments and the wellbeing movement in public policy similar to public policy interventions outcomes of ai systems implementation may have farreaching complex impacts in public policy indicators are only part of a broader toolbox as metrics inherently lead to gaming and dissolution of incentives and objectives similarly in the case of ais theres a need for a larger toolbox that allows for the iterative assessment of identified impacts inclusion of new impacts in the analysis and identification of emerging tradeoffs in this paper we propose the practical application of an enhanced wellbeing impact assessment framework for ais that could be employed to address ethical and rightsbased normative principles in ai this process could enable a humancentered algorithmicallysupported approach to the understanding of the impacts of ai systems finally we propose a new testing infrastructure which would allow for governments civil rights organizations and others to engage in cooperating with ais developers towards implementation of enhanced wellbeing impact assessments,http://arxiv.org/abs/2007.14826v2,2
network data is increasingly being used in quantitative datadriven public policy research these are typically very rich datasets that contain complex correlations and interdependencies this richness both promises to be quite useful for policy research while at the same time posing a challenge for the useful extraction of information from these datasets a challenge which calls for new data analysis methods in this report we formulate a research agenda of key methodological problems whose solutions would enable new advances across many areas of policy research we then review recent advances in applying deep learning to network data and show how these methods may be used to address many of the methodological problems we identified we particularly emphasize deep generative methods which can be used to generate realistic synthetic networks useful for microsimulation and agentbased models capable of informing key public policy questions we extend these recent advances by developing a new generative framework which applies to large social contact networks commonly used in epidemiological modeling for context we also compare and contrast these recent neural networkbased approaches with the more traditional exponential random graph models lastly we discuss some open problems where more progress is needed,http://arxiv.org/abs/2010.07870v2,2
governments are increasingly turning to algorithmic risk assessments when making important decisions such as whether to release criminal defendants before trial policymakers assert that providing public servants with algorithmic advice will improve human risk predictions and thereby lead to better eg fairer decisions yet because many policy decisions require balancing riskreduction with competing goals improving the accuracy of predictions may not necessarily improve the quality of decisions if risk assessments make people more attentive to reducing risk at the expense of other values these algorithms would diminish the implementation of public policy even as they lead to more accurate predictions through an experiment with 2140 lay participants simulating two highstakes government contexts we provide the first direct evidence that risk assessments can systematically alter how people factor risk into their decisions these shifts counteracted the potential benefits of improved prediction accuracy in the pretrial setting of our experiment the risk assessment made participants more sensitive to increases in perceived risk this shift increased the racial disparity in pretrial detention by 19 in the government loans setting of our experiment the risk assessment made participants more riskaverse this shift reduced government aid by 83 these results demonstrate the potential limits and harms of attempts to improve public policy by incorporating predictive algorithms into multifaceted policy decisions if these observed behaviors occur in practice presenting risk assessments to public servants would generate unexpected and unjust shifts in public policy without being subject to democratic deliberation or oversight,http://arxiv.org/abs/2012.05370v2,2
policymakers decide on alternative policies facing restricted budgets and uncertain everchanging future designing public policies is further difficult due to the need to decide on priorities and handle effects across policies housing policies specifically involve heterogeneous characteristics of properties themselves and the intricacy of housing markets and the spatial context of cities we propose policyspace2 ps2 as an adapted and extended version of the open source policyspace agentbased model ps2 is a computer simulation that relies on empirically detailed spatial data to model real estate along with labor credit and goods and services markets interaction among workers firms a bank households and municipalities follow the literature benchmarks to integrate economic spatial and transport scholarship ps2 is applied to a comparison among three competing public policies aimed at reducing inequality and alleviating poverty a house acquisition by the government and distribution to lower income households b rental vouchers and c monetary aid within the model context the monetary aid that is smaller amounts of help for a larger number of households makes the economy perform better in terms of production consumption reduction of inequality and maintenance of financial duties ps2 as such is also a framework that may be further adapted to a number of related research questions,http://arxiv.org/abs/2102.11929v4,2
the public policy cycle requires increasingly the use of evidence by policy makers evidence gap maps egms are a relatively new methodology that helps identify process and visualize the vast amounts of studies representing a rich source of evidence for better policy making this document performs a methodological review of egms and presents the development of a working integrated system that automates several critical steps of egm creation by means of applied computational and statistical methods above all the proposed system encompasses all major steps of egm creation in one place namely inclusion criteria determination processing of information analysis and userfriendly communication of synthesized relevant evidence this tool represents a critical milestone in the efforts of implementing cuttingedge computational methods in usable systems the contribution of the document is twofold first it presents the critical importance of egms in the public policy cycle second it justifies and explains the development of a usable tool that encompasses the methodological phases of creation of egms while automating most timeconsuming stages of the process the overarching goal is the better and faster information communication to relevant actors like policy makers thus promoting wellbeing through better and more efficient interventions based on more evidencedriven policy making,http://arxiv.org/abs/2304.10576v1,2
accessing data collected by federal statistical agencies is essential for public policy research and improving evidencebased decision making such as evaluating the effectiveness of social programs understanding demographic shifts or addressing public health challenges differentially private interactive systems or validation servers can form a crucial part of the datasharing infrastructure they may allow researchers to query targeted statistics providing flexible efficient access to specific insights reducing the need for broad data releases and supporting timely focused research however they have not yet been practically implemented while substantial theoretical work has been conducted on the privacy and accuracy guarantees of differentially private mechanisms prior efforts have not considered usability as an explicit goal of interactive systems this work outlines and considers the barriers to developing differentially private interactive systems for informing public policy and offers an alternative way forward we propose balancing three design considerations privacy assurance statistical utility and system usability we develop recommendations for making differentially private interactive systems work in practice we present an example architecture based on these recommendations and we provide an outline of how to conduct the necessary usertesting our work seeks to move the practical development of differentially private interactive systems forward to better aid public policy making and spark future research,http://arxiv.org/abs/2412.11794v1,2
this paper has been withdrawn by the authors in order to comply with the atlas publication policy and is now only available via the cern cdsweb,http://arxiv.org/abs/0704.2307v3,2
the paper proposes a new model of spin dynamics which can be treated as a model of sociological coupling between individuals our approach takes into account two different human features gregariousness and individuality we will show how they affect a psychological distance between individuals and how the distance changes the opinion formation in a social group apart from its sociological aplications the model displays the variety of other interesting phenomena like selforganizing ferromagnetic state or a second order phase transition and can be studied from different points of view eg as a model of ferromagnetic fluid complex evolving network or multiplicative random process,http://arxiv.org/abs/physics/0603248v1,2
we explore the promises and challenges of employing sequential decisionmaking algorithms such as bandits reinforcement learning and active learning in law and public policy while such algorithms have wellcharacterized performance in the private sector eg online advertising the tendency to naively apply algorithms motivated by one domain often online advertisements can be called the advertisement fallacy our main thesis is that law and public policy pose distinct methodological challenges that the machine learning community has not yet addressed machine learning will need to address these methodological problems to move beyond ads public law for instance can pose multiple objectives necessitate batched and delayed feedback and require systems to learn rational causal decisionmaking policies each of which presents novel questions at the research frontier we discuss a wide range of potential applications of sequential decisionmaking algorithms in regulation and governance including public health environmental protection tax administration occupational safety and benefits adjudication we use these examples to highlight research needed to render sequential decision making policycompliant adaptable and effective in the public sector we also note the potential risks of such deployments and describe how sequential decision systems can also facilitate the discovery of harms we hope our work inspires more investigation of sequential decision making in law and public policy which provide unique challenges for machine learning researchers with potential for significant social benefit,http://arxiv.org/abs/2112.06833v3,2
this collection of big bayes stories could be partitioned into two groups one relating to the sciences cosmology in particular and the other relating to public policy that is health fisheries management and demographics,http://arxiv.org/abs/1405.4982v1,2
fienberg convincingly demonstrates that bayesian models and methods represent a powerful approach to squeezing illumination from data in public policy settings however no school of inference is without its weaknesses and in the face of the ambiguities uncertainties and poorly posed questions of the real world perhaps we should not expect to find a formally correct inferential strategy which can be universally applied whatever the nature of the question we should not expect to be able to identify a norm approach an analogy is made between george boxs no models are right but some are useful and inferential systems arxiv11082177,http://arxiv.org/abs/1108.3657v1,2
social security and other public policies can be viewed as a series of cash in and outflows that depend on parameters such as the age distribution of the population and the retirement age given forecasts of these parameters policies can be designed to be financially stable ie to terminate with a zero balance if reality deviates from the forecasts policies normally terminate with a surplus or a deficit we derive constraints on the cash flows of robust policies that terminate with zero balance even in the presence of forecasting errors social security and most similar policies are not robust we show that nontrivial robust policies exist and provide a recipe for constructing robust extensions of nonrobust policies an example illustrates our results,http://arxiv.org/abs/1201.6340v1,2
increasing integration and availability of data on large groups of persons has been accompanied by proliferation of statistical and other algorithmic prediction tools in banking insurance marketing medicine and other fields see eg steyerberg 2009ab controversy may ensue when such tools are introduced to fields traditionally reliant on individual clinical evaluations such controversy has arisen about actuarial assessments of violence recidivism risk ie the probability that someone found to have committed a violent act will commit another during a specified period recently hart et al 2007a and subsequent papers from these authors in several reputable journals have claimed to demonstrate that statistical assessments of such risks are inherently too imprecise to be useful using arguments that would seem to apply to statistical risk prediction quite broadly this commentary examines these arguments from a technical statistical perspective and finds them seriously mistaken in many particulars they should play no role in reasoned discussions of violence recidivism risk assessment,http://arxiv.org/abs/1503.03666v1,2
in this article we consider modeling ranked responses from a heterogeneous population specifically we analyze data from the eurobarometer 341 survey regarding public policy preferences towards drugs alcohol and aids such policy preferences are likely to exhibit substantial differences within as well as across european nations reflecting a wide variety of cultures political affiliations ideological perspectives and common practices we use a mixed membership model to account for multiple subgroups with differing preferences and to allow each individual to possess partial membership in more than one subgroup previous methods for fitting mixed membership models to rank data in a univariate setting have utilized an mcmc approach and do not estimate the relative frequency of each subgroup we propose a variational em approach for fitting mixed membership models with multivariate rank data our method allows for fast approximate inference and explicitly estimates the subgroup sizes analyzing the eurobarometer 341 data we find interpretable subgroups which generally agree with the left vs right classification of political ideologies,http://arxiv.org/abs/1512.08731v3,2
throughout the covid19 pandemic a significant amount of effort had been put into developing techniques that predict the number of infections under various assumptions about the public policy and nonpharmaceutical interventions while both the available data and the sophistication of the ai models and available computing power exceed what was available in previous years the overall success of prediction approaches was very limited in this paper we start from prediction algorithms proposed for xprize pandemic response challenge and consider several directions that might allow their improvement then we investigate their performance over mediumterm predictions extending over several months we find that augmenting the algorithms with additional information about the culture of the modeled region incorporating traditional compartmental models and uptodate deep learning architectures can improve the performance for short term predictions the accuracy of mediumterm predictions is still very low and a significant amount of future research is needed to make such models a reliable component of a public policy toolbox,http://arxiv.org/abs/2112.11187v1,2
the availability of open government data ogd provides means for citizens to understand and follow governmental policies and decisions showing evidence of how the latter have contributed to both the place they live in and their lives in such a scenario one of the proposals is the use of visualizations to support the process of data analysis and interpretation herein we present the use of three different visualization tools a commercial one and two academic ones applied to two specific brazilian cases the implementation of the drink driving law and the construction of a new overpass in an important city avenue our focus was on the analysis of how visualization could help in the identification of the effects of such traffic public policies as our main contributions we present details on the effects of the observed policies as well as new cases showing how visualization tools can assist users to interpret ogd,http://arxiv.org/abs/2102.07621v1,2
how should we think of the preferences of citizens whereas selfoptimal policy is relatively straightforward to produce socially optimal policy often requires a more detailed examination in this paper we identify an issue that has received far too little attention in welfarist modelling of public policy which we name the hidden assumptions problem hidden assumptions can be deceptive because they are not expressed explicitly and the social planner eg a policy maker a regulator a legislator may not give them the critical attention they need we argue that ethical expertise has a direct role to play in public discourse because it is hard to adopt a position on major issues like public health policy or healthcare prioritisation without making contentious assumptions about population ethics we then postulate that ethicists are best situated to critically evaluate these hidden assumptions and can therefore play a vital role in public policy debates,http://arxiv.org/abs/2205.01957v1,2
this document is the snowmass summary report for the public policy government engagement topical group within the community engagement frontier this report discusses how the us high energy physics hep community currently engages with government at all levels and provides recommendations for how the execution of these actives can be improved and for how the scope of existing activities can be expanded this includes the current hep congressional government engagement dc trip materials produced for communication with government officials government engagement in areas other than hep funding and interactions with the funding agencies executive office of the president and state and local governments,http://arxiv.org/abs/2209.09067v2,2
mainstream food delivery platforms like doordash and uber eats have been the locus of fierce policy debates about their unfair business and labor practices at the same time hundreds of independent food delivery services provide alternative opportunities to many communities across the us we surveyed operators of independent food delivery platforms to learn about their perception of the role of public policy we found conflicting opinions on whether and how policy should interact with their businesses ranging from not wanting policymakers to interfere to articulating specific policies that would curtail mainstream platforms business practices we provide insights for technologists and policymakers interested in the sociotechnical challenges of local marketplaces,http://arxiv.org/abs/2303.15415v2,2
while we typically focus on data visualization as a tool for facilitating cognitive tasks eg learning facts making decisions we know relatively little about their secondorder impacts on our opinions attitudes and values for example could design or framing choices interact with viewers social cognitive biases in ways that promote political polarization when reporting on us attitudes toward public policies it is popular to highlight the gap between democrats and republicans eg with blue vs red connected dot plots but these charts may encourage socialnormative conformity influencing viewers attitudes to match the divided opinions shown in the visualization we conducted three experiments examining visualization framing in the context of social conformity and polarization crowdworkers viewed charts showing simulated polling results for public policy proposals we varied framing aggregating data as nonpartisan all us adults or partisan democrat and republican and the visualized groups support levels participants then reported their own support for each policy we found that participants attitudes biased significantly toward the group attitudes shown in the stimuli and this can increase interparty attitude divergence these results demonstrate that data visualizations can induce social conformity and accelerate political polarization choosing to visualize partisan divisions can divide us further,http://arxiv.org/abs/2309.00690v1,2
in order to effectively analyze information regarding ongoing events that impact local communities across language and country borders researchers often need to perform multilingual data analysis this analysis can be particularly challenging due to the rapidly evolving eventcentric data and the language barrier in this abstract we present preliminary results of a case study with the goal to better understand how researchers interact with multilingual eventcentric information in the context of crosscultural studies and which methods and features they use,http://arxiv.org/abs/1801.07011v1,2
from the perspective of cultural studies and critical theory luhmanns communicationtheoretical approach in sociology can still be read as a metabiology while biologists take the development of life as a given luhmann tends to treat the development of meaning as a cultural given meaning is no longer considered as constructed in communication but meaning processing precedes and controls communication as an independent variable habermas appreciates luhmanns distinction between psychic and social systems but he challenges us to bring the critique of metaphysical issues of providing meaning to events in a dialectics back into this metabiological perspective that processes meaning without intentionality that is as a scientistic objectivation,http://arxiv.org/abs/0911.2718v1,2
the paper discusses the potential of large visionlanguage models as objects of interest for empirical cultural studies focusing on the comparative analysis of outputs from two popular texttoimage synthesis models dalle 2 and stable diffusion the paper tries to tackle the pros and cons of striving towards culturally agnostic vs culturally specific ai models the paper discusses several examples of memorization and bias in generated outputs which showcase the tradeoff between risk mitigation and cultural specificity as well as the overall impossibility of developing culturally agnostic models,http://arxiv.org/abs/2211.15271v2,2
according to physicist david mermin the science wars was a series of exchanges between scientists and sociologists historians and literary critics whom the scientists thought to be ludicrously ignorant of science making all kinds of nonsensical pronouncements the science wars peaked in 1996 with the publication of a hoax article by physicist alan sokal in a cultural studies journal and then seemed mercifully to die away by the end of the 1990s i recently noticed however that the kerfuffle has persisted this motivated me to pen this essay and point out the silliness of the entire affair,http://arxiv.org/abs/2303.11980v1,2
inequalities are abundant in a society with a number of agents competing for a limited amount of resource statistics of such social inequalities are usually represented by the lorenz function lp where p fraction of the population possesses lp fraction of the total wealth when the population is arranged in the ascending order of their wealth similarly in scientometrics such inequalities can be represented by a plot of the citation count against the respective number of papers by a scientist again arranged in the ascending order of their citation counts quantitatively these inequalities are captured by the corresponding inequality indices namely the kolkata k and the hirsch h indices given by the fixed points of these nonlinear functions in statistical physics of criticality the fixed points of the renormalization group generator functions are studied in their selfsimilar limit where its fractal structure converges to a unique form the statistical indices in the social science however correspond to the fixed points where the values of the generator function wealth or citation sizes are commensurately abundant in fractions or numbers of persons or papers we introduce and study these indices for the inequalities of prefailure avalanches given by their size distributions in the fiber bundle models fbm of nonbrittle materials we show how a prior knowledge of the terminal and almost universal value of the k index for a wide range of disorder parameter can help in predicting an imminent catastrophic breakdown in the model this observation has also been complemented by noting a similar but not identical behavior of the hirsch index h redefined for such avalanche statistics,http://arxiv.org/abs/2106.14294v5,2
this paper assesses the equity impacts of forhire autonomous vehicles avs and investigates regulatory policies that promote spatial and social equity in future autonomous mobility ecosystems to this end we consider a multimodal transportation network where a ridehailing platform operates a fleet of avs to offer mobilityondemand services in competition with a public transit agency that offers transit services on a transportation network a gametheoretic model is developed to characterize the intimate interactions between the ridehailing platform the transit agency and multiclass passengers with distinct income levels an algorithm is proposed to compute the nash equilibrium of the game and conduct an expost evaluation of the performance of the obtained solution based on the proposed framework we evaluate the spatial and social equity in transport accessibility using the theil index and find that although the proliferation of forhire avs in the ridehailing network improves overall accessibility the benefits are not fairly distributed among distinct locations or population groups implying that the deployment of avs will enlarge the existing spatial and social inequity gaps in the transportation network if no regulatory intervention is in place to address this concern we investigate two regulatory policies that can improve transport equity a a minimum servicelevel requirement on ridehailing services which improves the spatial equity in the transport network b a subsidy on transit services by taxing ridehailing services which promotes the use of public transit and improves the spatial and social equity of the transport network we show that the minimum servicelevel requirement entails a tradeoff as a higher minimum service level is imposed the spatial inequity reduces but the social inequity will be exacerbated on the other hand,http://arxiv.org/abs/2301.05798v2,2
social inequalities are ubiquitous and evolve towards a universal limit herein we extensively review the values of inequality measures namely the gini g index and the kolkata k index two standard measures of inequality used in the analysis of various social sectors through data analysis the kolkata index denoted as k indicates the proportion of the wealth owned by 1k fraction of the people our findings suggest that both the gini index and the kolkata index tend to converge to similar values around gk approx 087 starting from the point of perfect equality where g0 and k05 as competition increases in different social institutions such as markets movies elections universities prize winning battle fields sports olympics etc under conditions of unrestricted competition no social welfare or support mechanism in this review we present the concept of a generalized form of paretos 8020 law k080 where the coincidence of inequality indices is observed the observation of this coincidence is consistent with the precursor values of the g and k indices for the selforganized critical soc state in selftuned physical systems such as sand piles these results provide quantitative support for the view that interacting socioeconomic systems can be understood within the framework of soc which has been hypothesized for many years these findings suggest that the soc model can be extended to capture the dynamics of complex socioeconomic systems and help us better understand their behavior,http://arxiv.org/abs/2303.03795v3,2
mathematical modelling has served a central role in studying how infectious disease transmission manifests at the population level these models have demonstrated the importance of populationlevel factors like social network heterogeneity on structuring epidemic risk and are now routinely used in public health for decision support one barrier to broader utility of mathematical models is that the existing canon does not readily accommodate the social determinants of health as distinct formal drivers of transmission dynamics given the decades of empirical support for the organizational effect of social determinants on health burden more generally and infectious disease risk more specially addressing this modelling gap is of critical importance in this study we build on prior efforts to integrate social forces into mathematical epidemiology by introducing several new metrics principally structural causal influence sci here sci leverages causal analysis to provide a measure of the relative vulnerability of subgroups within a susceptible population which are crafted by differences in healthcare exposure to disease and other determinants we develop our metrics using a general case and apply it to specific one of public health importance hepatitis c virus in a population of persons who inject drugs our use of the sci reveals that under specific parameters in a multicommunity model the less vulnerable community may sustain a basic reproduction number below one when isolated ensuring disease extinction however even minimal transmission between less and more vulnerable communities can elevate this number leading to sustained epidemics within both communities summarizing we reflect on our findings in light of conversations surrounding the importance of social inequalities and how their consideration can influence the study and practice of mathematical epidemiology,http://arxiv.org/abs/2409.09096v1,2
since culture influences expectations perceptions and satisfaction a crossculture study is necessary to understand the differences between japans biggest tourist populations chinese and western tourists however with everincreasing customer populations this is hard to accomplish without extensive customer base studies there is a need for an automated method for identifying these expectations at a large scale for this we used a datadriven approach to our analysis our study analyzed their satisfaction factors comparing soft attributes such as service with hard attributes such as location and facilities and studied different price ranges we collected hotel reviews and extracted keywords to classify the sentiment of sentences with an svc we then used dependency parsing and partofspeech tagging to extract nouns tied to positive adjectives we found that chinese tourists consider room quality more than hospitality whereas westerners are delighted more by staff behavior furthermore the lack of a chinesefriendly environment for chinese customers and cigarette smell for western ones can be disappointing factors of their stay as one of the first studies in the tourism field to use the highstandard japanese hospitality environment for this analysis our crosscultural study contributes to both the theoretical understanding of satisfaction and suggests practical applications and strategies for hotel managers,http://arxiv.org/abs/2107.14681v1,2
social movements neurons in the brain or even industrial suppliers are best described by agents evolving on networks with basic interaction rules in these real systems the connectivity between agents corresponds to the a critical state of the system related to the noise of the system the new idea is that connectivity adjusts itself because of two opposite tendencies on the one hand informations percolation is better when the network connectivity is small but all agents have rapidely the same state and the dynamics stops on the other hand when agents have a large connectivity the state of a node opinion of a person state of a neuron tends to freeze agents find always a minority among their neighbours to support their state the model introduced here captures this essential feature showing a clear transition between the two tendencies at some critical connectivity depending on the noise the dynamics of the system can only take place at a precise critical connectivity since away from this critical point the system remains in a static phase when the noise is very small the critical connectivity becomes very large and highly connected networks are obtained like the airports network and the internet this model may be used as a starting point for understanding the evolution of agents living on networks,http://arxiv.org/abs/physics/0509074v2,2
the number of people using online social networks in their everyday life is continuously growing at a pace never saw before this new kind of communication has an enormous impact on opinions cultural trends information spreading and even in the commercial success of new products more importantly social online networks have revealed as a fundamental organizing mechanism in recent countrywide social movements in this paper we provide a quantitative analysis of the structural and dynamical patterns emerging from the activity of an online social network around the ongoing may 15th 15m movement in spain our network is made up by users that exchanged tweets in a time period of one month which includes the birth and stabilization of the 15m movement we characterize in depth the growth of such dynamical network and find that it is scalefree with communities at the mesoscale we also find that its dynamics exhibits typical features of critical systems such as robustness and powerlaw distributions for several quantities remarkably we report that the patterns characterizing the spreading dynamics are asymmetric giving rise to a clear distinction between information sources and sinks our study represent a first step towards the use of data from online social media to comprehend modern societal dynamics,http://arxiv.org/abs/1107.1750v1,2
an increasing fraction of today social interactions occur using online social media as communication channels recent worldwide events such as social movements in spain or revolts in the middle east highlight their capacity to boost people coordination online networks display in general a rich internal structure where users can choose among different types and intensity of interactions despite of this there are still open questions regarding the social value of online interactions for example the existence of users with millions of online friends sheds doubts on the relevance of these relations in this work we focus on twitter one of the most popular online social networks and find that the network formed by the basic type of connections is organized in groups the activity of the users conforms to the landscape determined by such groups furthermore twitters distinction between different types of interactions allows us to establish a parallelism between online and offline social networks personal interactions are more likely to occur on internal links to the groups the weakness of strong ties events transmitting new information go preferentially through links connecting different groups the strength of weak ties or even more through links connecting to users belonging to several groups that act as brokers the strength of intermediary ties,http://arxiv.org/abs/1107.4009v2,2
this is an introduction to the special issue titled collective behavior and evolutionary games that is in the making at chaos solitons fractals the term collective behavior covers many different phenomena in nature and society from bird flocks and fish swarms to social movements and herding effects it is the lack of a central planner that makes the spontaneous emergence of sometimes beautifully ordered and seemingly meticulously designed behavior all the more sensational and intriguing the goal of the special issue is to attract submissions that identify unifying principles that describe the essential aspects of collective behavior and which thus allow for a better interpretation and foster the understanding of the complexity arising in such systems as the title of the special issue suggests the later may come from the realm of evolutionary games but this is certainly not a necessity neither for this special issue and certainly not in general interdisciplinary work on all aspects of collective behavior regardless of background and motivation and including synchronization and human cognition is very welcome,http://arxiv.org/abs/1306.2296v2,2
recent grassroots movements have suggested that online social networks might play a key role in their organization as adherents have a fast manytomany communication channel to help coordinate their mobilization the structure and dynamics of the networks constructed from the digital traces of protesters have been analyzed to some extent recently however less effort has been devoted to the analysis of the semantic content of messages exchanged during the protest using the data obtained from a microblogging service during the brewing and active phases of the 15m movement in spain we perform the first large scale test of theories on collective emotions and social interaction in collective actions our findings show that activity and information cascades in the movement are larger in the presence of negative collective emotions and when users express themselves in terms related to social content at the level of individual participants our results show that their social integration in the movement as measured through social network metrics increases with their level of engagement and of expression of negativity our findings show that nonrational factors play a role in the formation and activity of social movements through online media having important consequences for viral spreading,http://arxiv.org/abs/1505.03776v1,2
we analyse a large sample of the twitter activity developed around the social movement occupy wall street to study the complex interactions between the human communication activity and the semantic content of a discussion we use a network approach based on the analysis of the bipartite graph usershashtags and of its projections the semantic network whose nodes are hashtags and the users interest network whose nodes are users in the first instance we find out that discussion topics hashtags present a high heterogeneity with the distinct role of the communication hubs where most the opinion traffic passes through in the second case the selforganization process of users activity leads to the emergence of two classes of communicators the professionals and the amateurs moreover the network presents a strong community structure based on the differentiation of the semantic topics and a high level of structural robustness when a certain set of topics are censored andor accounts are removed analysing the characteristics the usershashtags network we can distinguish three phases of the discussion about the movement each phase corresponds to specific moment of the movement from declaration of intent organisation and development and the final phase of political reactions each phase is characterised by the presence of specific hashtags in the discussion,http://arxiv.org/abs/1412.4639v1,2
social media has become an important venue for diverse groups to share information discuss political issues and organize social movements recent scholarship has shown that the social media ecosystem can affect political thinking and expression individuals and groups across the political spectrum have engaged in the use of these platforms extensively even creating their own forums with varying approaches to content moderation in pursuit of freer standards of speech the gab social media platform arose in this context gab is a social media platform for the socalled alt right and much of the popular press has opined about the thematic content of discourses on gab and platforms like it but little research has examined the content itself using a publicly available dataset of all gab posts from august 2016 until july 2019 the current paper explores a five percent random sample of this dataset to explore thematic content on the platform we run multiple structural topic models using standard procedures to arrive at an optimal k number of topics the final model specifies 85 topics for 403469 documents we include as prevalence variables whether the source account has been flagged as a bot and the number of followers for the source account results suggest the most nodal topics in the dataset pertain to the authenticity of the holocaust the meaning of red pill and the journalistic merit of mainstream media we conclude by discussing the implications of our findings for work in ethical content moderation online community development political polarization and avenues for future research,http://arxiv.org/abs/2007.09685v1,2
black lives matter blm is a decentralized social movement protesting violence against black individuals and communities with a focus on police brutality the movement gained significant attention following the killings of ahmaud arbery breonna taylor and george floyd in 2020 the blacklivesmatter social media hashtag has come to represent the grassroots movement with similar hashtags counter protesting the blm movement such as alllivesmatter and bluelivesmatter we introduce a data set of 639 million tweets from 130 million users from over 100 countries which contain one of the following keywords blacklivesmatter alllivesmatter and bluelivesmatter this data set contains all currently available tweets from the beginning of the blm movement in 2013 to 2021 we summarize the data set and show temporal trends in use of both the blacklivesmatter keyword and keywords associated with counter movements additionally for each keyword we create and release a set of latent dirichlet allocation lda topics ie automatically clustered groups of semantically cooccuring words to aid researchers in identifying linguistic patterns across the three keywords,http://arxiv.org/abs/2009.00596v3,2
diffusion of information spread of rumors and infectious diseases are all instances of stochastic processes that occur over the edges of an underlying network many times networks over which contagions spread are unobserved and such networks are often dynamic and change over time in this paper we investigate the problem of inferring dynamic networks based on information diffusion data we assume there is an unobserved dynamic network that changes over time while we observe the results of a dynamic process spreading over the edges of the network the task then is to infer the edges and the dynamics of the underlying network we develop an online algorithm that relies on stochastic convex optimization to efficiently solve the dynamic network inference problem we apply our algorithm to information diffusion among 33 million mainstream media and blog sites and experiment with more than 179 million different pieces of information spreading over the network in a one year period we study the evolution of information pathways in the online media space and find interesting insights information pathways for general recurrent topics are more stable across time than for ongoing news events clusters of news media sites and blogs often emerge and vanish in matter of days for ongoing news events major social movements and events involving civil population such as the libyans civil war or syrias uprise lead to an increased amount of information pathways among blogs as well as in the overall increase in the network centrality of blogs and social media sites,http://arxiv.org/abs/1212.1464v1,2
inspired by the recent social movement of metoo we are building a chatbot to assist survivors of sexual harassment cases designed for the city of maastricht but can easily be extended the motivation behind this work is twofold properly assist survivors of such events by directing them to appropriate institutions that can offer them help and increase the incident documentation so as to gather more data about harassment cases which are currently under reported we break down the problem into three data sciencemachine learning components harassment type identification treated as a classification problem spatiotemporal information extraction treated as named entity recognition problem and dialogue with the users treated as a slotfilling based chatbot we are able to achieve a success rate of more than 98 for the identification of a harassmentornot case and around 80 for the specific type harassment identification locations and dates are identified with more than 90 accuracy and time occurrences prove more challenging with almost 80 finally initial validation of the chatbot shows great potential for the further development and deployment of such a beneficial for the whole society tool,http://arxiv.org/abs/1909.02809v1,2
social media provides the means by which extremist social movements such as white supremacy and anti lgbtq thrive online yet we know little about the roles played by the participants of such movements in this paper we investigate these participants to characterize their roles their role dynamics and their influence in spreading online extremism our participants online extremist accounts are 4876 public facebook pages or groups that have shared information from the websites of 289 southern poverty law center designated extremist groups by clustering the quantitative features followed by qualitative expert validation we identify five roles surrounding extremist activism educators solicitors flamers motivators sympathizers for example solicitors use links from extremist websites to attract donations and participation in extremist issues whereas flamers share inflammatory extremist content inciting anger we further investigate role dynamics such as how stable these roles are over time and how likely will extremist accounts transition from one role into another we find that roles core to the movement educators and solicitors are more stable while flamers and motivators can transition to sympathizers with high probability we further find that educators and solicitors exert the most influence in triggering extremist link posts whereas flamers are influential in triggering the spread of information from fake news sources our results help in situating various roles on the trajectory of deeper engagement into the extremist movements and understanding the potential effect of various counter extremism interventions our findings have implications for understanding how online extremist movements flourish through participatory activism and how they gain a spectrum of allies for mobilizing extremism online,http://arxiv.org/abs/2105.08827v1,2
instagram infographics are a digital activism tool that have redefined action frames for technologyfacilitated social movements from the 1960s through the 1980s united states ethnic movements practiced collective action ideologically unified resourceintensive traditional activism today technologically enabled movements have been categorized as practicing connective action individualized lowresource online activism yet we argue that instagram infographics are both connective and collective this paper juxtaposes the insights of past and present us ethnic movement activists and analyzes black lives matter instagram data over the course of 7 years 20142020 we find that instagram infographic activism bridges connective and collective action in three ways 1 scope for education visually enticing and digestible infographics reduce the friction of information dissemination facilitating collective movement education while preserving customizability 2 reconciliation for credibility activists use connective features to combat infographic misinformation and resolve internal differences creating a trusted collective movement front 3 highresource efforts for transformative change instagram infographic activism has been paired with boots on the ground and actionoriented content curating a connectivetocollective pipeline that expends movement resources our work unveils the vitality of evaluating digital activism action frames at the movement integration level exemplifies the powerful coexistence of connective and collective action and offers meaningful design implications for activists seeking to leverage this novel tool,http://arxiv.org/abs/2111.00714v1,2
science and society inevitably interact with each other and evolve together studying the trend of science helps recognize leading topics significant for research and establish better policies to allocate funds efficiently scholarly societies such as the korean physics society kps also play an important role in the history of science figuring out the role of these scholarly societies motivate our research related with our society since societies pay attention to improve our society although several studies try to capture the trend of science leveraging scientific documents such as paper or patents but these studies limited their research scope only to the academic world neglecting the interaction with society here we try to understand the trend of science along with society using a public magazine named physics and high technology published by the korean physics society kps we build keyword cooccurrence networks for each time period and applied community detection to capture the keyword structure and tracked the structures evolution in the networks a researchrelated cluster is consistently dominant over time and subclusters of the researchrelated cluster divide into various fields of physics implying specialization of the physics discipline also we found that education and policy clusters appear consistently revealing the kpss contribution to science and society furthermore we applied pagerank algorithm to selected keywords semiconductor woman evading to investigate the temporal change of the importance of keywords in the network for example the importance of the keyword woman increases as time goes by indicating that academia also pays attention to gender issues reflecting the social movement in recent years,http://arxiv.org/abs/2205.09969v2,2
online social networks osns play a crucial role in todays world on the one hand they allow free speech information sharing and socialmovements organization to cite a few on the other hand they are the tool of choice to spread disinformation hate speech and to support propaganda for these reasons osns data mining and analysis aimed at detecting disinformation campaigns that may arm the society and more in general poison the democratic posture of states are essential activities during key events such as elections pandemics and conflicts in this paper we studied the 2022 russoukrainian conflict on twitter one of the most used osns we quantitatively and qualitatively analyze a dataset of more than 55 million tweets related to the subject generated by 18 million unique users by leveraging statistical analysis techniques and aspectbased sentiment analysis absa we discover hidden insights in the collected data and abnormal patterns in the users sentiment that in some cases confirm while in other cases disprove common beliefs on the conflict in particular based on our findings and contrary to what suggested in some mainstream media there is no evidence of massive disinformation campaigns however we have identified several anomalies in the behavior of particular accounts and in the sentiment trend for some subjects that represent a starting point for further analysis in the field the adopted techniques the availability of the data the replicability of the experiments and the preliminary findings other than being interesting on their own also pave the way to further research in the domain,http://arxiv.org/abs/2208.04903v1,2
the metoo movement has catalyzed widespread public discourse surrounding sexual harassment and assault empowering survivors to share their stories and holding perpetrators accountable while the movement has had a substantial and largely positive influence this study aims to examine the potential negative consequences in the form of increased hostility against women and men on the social media platform twitter by analyzing tweets shared between october 2017 and january 2020 by more than 471k individuals who had either disclosed their own sexual abuse experiences on twitter or engaged in discussions about the movement we identify the overall increase in genderbased hostility towards both women and men since the start of the movement we also monitor 16 pivotal reallife events that shaped the metoo movement to identify how these events may have amplified negative discussions targeting the opposite gender on twitter furthermore we conduct a thematic content analysis of a subset of genderbased hostile tweets which helps us identify recurring themes and underlying motivations driving the expressions of anger and resentment from both men and women concerning the metoo movement this study highlights the need for a nuanced understanding of the impact of social movements on online discourse and underscores the importance of addressing genderbased hostility in the digital sphere,http://arxiv.org/abs/2308.13076v1,2
narratives can be powerful tools for inspiring action on pressing societal issues such as climate change while social science theories offer frameworks for understanding the narratives that arise within collective movements these are rarely applied to the vast data available from social media platforms which play a significant role in shaping public opinion and mobilizing collective action this gap in the empirical evaluation of online narratives limits our understanding of their relationship with public response in this study we focus on plantbased diets as a form of proenvironmental action and employ natural language processing to operationalize a theoretical framework of moral narratives specific to the vegan movement we apply this framework to narratives found in youtube videos promoting environmental initiatives such as veganuary meatless march and no meat may our analysis reveals that several narrative types as defined by the theory are empirically present in the data to identify narratives with the potential to elicit positive public engagement we used text processing to estimate the proportion of comments supporting collective action across narrative types video narratives advocating social fight whether through protest or through efforts to convert others to the cause are associated with a stronger sense of collective action in the respective comments these narrative types also demonstrate increased semantic coherence and alignment between the message and public response markers typically associated with successful collective action our work offers new insights into the complex factors that influence the emergence of collective action thereby informing the development of effective communication strategies within social movements,http://arxiv.org/abs/2401.09210v2,2
as more algorithmic systems have come under scrutiny for their potential to inflict societal harms an increasing number of organizations that hold power over harmful algorithms have chosen or were required under the law to abandon them while social movements and calls to abandon harmful algorithms have emerged across application domains little academic attention has been paid to studying abandonment as a means to mitigate algorithmic harms in this paper we take a first step towards conceptualizing algorithm abandonment as an organizations decision to stop designing developing or using an algorithmic system due to its potential harms we conduct a thematic analysis of realworld cases of algorithm abandonment to characterize the dynamics leading to this outcome our analysis of 40 cases reveals that campaigns to abandon an algorithm follow a common process of six iterative phases discovery diagnosis dissemination dialogue decision and death which we term the 6 ds of abandonment in addition we highlight key factors that facilitate or prohibit abandonment which include characteristics of both the technical and social systems that the algorithm is embedded within we discuss implications for several stakeholders including proprietors and technologists who have the power to influence an algorithms discontinued use facct researchers and policymakers,http://arxiv.org/abs/2404.13802v2,2
in the context where social media is increasingly becoming a significant platform for social movements and the formation of public opinion accurately simulating and predicting the dynamics of user opinions is of great importance for understanding social phenomena policy making and guiding public opinion however existing simulation methods face challenges in capturing the complexity and dynamics of user behavior addressing this issue this paper proposes an innovative simulation method for the dynamics of social media user opinions the fdellm algorithm which incorporates opinion dynamics and epidemic model this effectively constrains the actions and opinion evolution process of large language models llm making them more aligned with the real cyber world in particular the fdellm categorizes users into opinion leaders and followers opinion leaders are based on llm roleplaying and are constrained by the ca model while opinion followers are integrated into a dynamic system that combines the ca model with the sir model this innovative design significantly improves the accuracy and efficiency of the simulation experiments were conducted on four real weibo datasets and validated using the opensource model chatglm the results show that compared to traditional agentbased modeling abm opinion dynamics algorithms and llmbased opinion diffusion algorithms our fdellm algorithm demonstrates higher accuracy and interpretability,http://arxiv.org/abs/2409.08717v3,2
brownian motion have long been studied on a diversity of fields not only in physics of statistical mechanics but also in biological models finance and economic process and social systems in the past twenty years there has been a growing interest in studying the model in selfpropelled feature and interaction force such that the model also fits into study of social phenomenon of many individuals this article will continue with this research trend and especially investigate the model in paradigms for a quantitative description of social and economic process we mainly discuss a class of collective decision process of brownian agentparticles where the stochastic process does not exist in the fluctuation in the traditional brownian motion but in selection among several discrete choices their decisions interacts with each other in a given social topology to simply our discussion the binary choice problem is particularly discussed where each agent only takes an alternative of two choices mathematically we introduce a set of arrays to describe social relationship of agents in a quantitative manner and the arrays deduce the group social force and opinion dynamics which are useful to study complex social movement and selforganization phenomena including discretechoice activities social groups and deindividualization effect such agentbased simulation symbolizes a variety of collective activities in human society especially in the field of economics and social science,http://arxiv.org/abs/2411.09840v2,2
the potential for machine learning ml systems to amplify social inequities and unfairness is receiving increasing popular and academic attention a surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness if these tools are to have a positive impact on industry practice however it is crucial that their design be informed by an understanding of realworld needs through 35 semistructured interviews and an anonymous survey of 267 ml practitioners we conduct the first systematic investigation of commercial product teams challenges and needs for support in developing fairer ml systems we identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ml research literature based on these findings we highlight directions for future ml and hci research that will better address industry practitioners needs,http://arxiv.org/abs/1812.05239v2,2
we have two main aims in this paper first we use theories of disease spreading on networks to look at the covid19 epidemic on the basis of individual contacts these give rise to predictions which are often rather different from the homogeneous mixing approaches usually used our second aim is to look at the role of social deprivation again using networks as our basis in the spread of this epidemic we choose the city of kolkata as a case study but assert that the insights so obtained are applicable to a wide variety of urban environments which are densely populated and where social inequalities are rampant our predictions of hotspots are found to be in good agreement with those currently being identifed empirically as containment zones and provide a useful guide for identifying potential areas of concern,http://arxiv.org/abs/2005.00491v1,2
data mining revealed a cluster of economic psychological social and cultural indicators that in combination predicted corruption and wealth of european nations this prosperity syndrome of selfreliant citizens efficient division of labor a sophisticated scientific community and respect for the law was clearly distinct from that of poor countries that had a diffuse relationship between high corruption perception low gdpcapita high social inequality low scientific development reliance on family and friends and languages with many words for guilt this suggests that there are many ways for a nation to be poor but few ones to become rich supporting the existence of synergistic interactions between the components in the prosperity syndrome favoring economic growth no single feature was responsible for national prosperity focusing on synergies rather than on single features should improve our understanding of the transition from poverty and corruption to prosperity in european nations and elsewhere,http://arxiv.org/abs/1604.00283v1,2
word embeddings trained on largescale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities these embeddings are often trained in separate vector space models defined according to different attributes of interest in this paper we develop a unified dynamic embedding model that learns attributespecific word embeddings we apply our model to investigate i 20th century gender and ethnic occupation biases embedded in the corpus of historical american english coha and ii biases against refugees embedded in a novel corpus of talk radio transcripts containing 119 million words produced over one month across 83 stations and 64 cities our results shed preliminary light on scenarios when dynamic embedding models may be more suitable for representing linguistic biases than individual vector space models and viceversa,http://arxiv.org/abs/1904.03352v2,2
accompanied by the development of digital media the threat of information cocoon has become a significant issue however little is known about the measure of information cocoon as a cultural space and its relationship with social class this study addresses this problem by constructing the cultural space with word embedding models and random shuffling methods among three largescale digital media use datasets in the light of field theory of cultural production we investigate the information cocoon effect on different social classes among 979 computer users 100000 smartphone users and 159373 mobile reading application users our analysis reveals that information cocoons widely exist in the daily use of digital media moreover people of lower social class have a higher probability of getting stuck in the information cocoon filled with the entertainment content in contrast the people of higher social class have more capability to stride over the constraints of the information cocoon the results suggest that the disadvantages for vulnerable groups in acquiring knowledge may further widen social inequality,http://arxiv.org/abs/2007.10083v3,2
the form of political polarization where citizens develop strongly negative attitudes towards outparty policies and members has become increasingly prominent across many democracies economic hardship and social inequality as well as intergroup and racial conflict have been identified as important contributing factors to this phenomenon known as affective polarization such partisan animosities are exacerbated when these interests and identities become aligned with existing party cleavages in this paper we use a model of cultural evolution to study how these forces combine to generate and maintain affective political polarization we show that economic events can drive both affective polarization and sorting of group identities along party lines which in turn can magnify the effects of underlying inequality between those groups but on a more optimistic note we show that sufficiently high levels of wealth redistribution through the provision of public goods can counteract this feedback and limit the rise of polarization we test some of our key theoretical predictions using survey data on intergroup polarization sorting of racial groups and affective polarization in the united states over the past 50 years,http://arxiv.org/abs/2103.14619v1,2
cultural codeswitching concerns how we adjust our overall behaviours manners of speaking and appearance in response to a perceived change in our social environment we defend the need to investigate cultural codeswitching capacities in artificial intelligence systems we explore a series of ethical and epistemic issues that arise when bringing cultural codeswitching to bear on artificial intelligence building upon dotsons 2014 analysis of testimonial smothering we discuss how emerging technologies in ai can give rise to epistemic oppression and specifically a form of selfsilencing that we call cultural smothering by leaving the sociodynamic features of cultural codeswitching unaddressed ai systems risk negatively impacting alreadymarginalised social groups by widening opportunity gaps and further entrenching social inequalities,http://arxiv.org/abs/2112.08256v1,2
as different research works report and daily life experiences confirm learning models can result in biased outcomes the biased learned models usually replicate historical discrimination in society and typically negatively affect the less represented identities robots are equipped with these models that allow them to operate performing tasks more complex every day the learning process consists of different stages depending on human judgments moreover the resulting learned models for robot decisions rely on recorded labeled data or demonstrations therefore the robot learning process is susceptible to bias linked to human behavior in society this imposes a potential danger especially when robots operate around humans and the learning process can reflect the social unfairness present today different feminist proposals study social inequality and provide essential perspectives towards removing bias in various fields what is more feminism allowed and still allows to reconfigure numerous social dynamics and stereotypes advocating for equality across people through their diversity consequently we provide a feminist perspective on the robot learning process in this work we base our discussion on intersectional feminism community feminism decolonial feminism and pedagogy perspectives and we frame our work in a feminist robotics approach in this paper we present an initial discussion to emphasize the relevance of feminist perspectives to explore foresee en eventually correct the biased robot decisions,http://arxiv.org/abs/2201.10853v1,2
at the heart of what drives the bulk of innovation and activity in silicon valley and elsewhere is scalability this unwavering commitment to scalability to identify strategies for efficient growth is at the heart of what we refer to as scale thinking whether people are aware of it or not scale thinking is allencompassing it is not just an attribute of ones product service or company but frames how one thinks about the world what constitutes it and how it can be observed and measured its problems what is a problem worth solving versus not and the possible technological fixes for those problems this paper examines different facets of scale thinking and its implication on how we view technology and collaborative work we argue that technological solutions grounded in scale thinking are unlikely to be as liberatory or effective at deep systemic change as their purveyors imagine rather solutions which resist scale thinking are necessary to undo the social structures which lie at the heart of social inequality we draw on recent work on mutual aid networks and propose questions to ask of collaborative work systems as a means to evaluate technological solutions and guide designers in identifying sites of resistance to scale thinking,http://arxiv.org/abs/2010.08850v2,2
sociotechnical systems within cities are now equipped with machine learning algorithms in hopes to increase efficiency and functionality by modeling and predicting trends machine learning algorithms have been applied in these domains to address challenges such as balancing the distribution of bikes throughout a city and identifying demand hotspots for ride sharing drivers however these algorithms applied to challenges in sociotechnical systems have exacerbated social inequalities due to previous bias in data sets or the lack of data from marginalized communities in this paper i will address how smart mobility initiatives in cities use machine learning algorithms to address challenges i will also address how these algorithms unintentionally discriminate against features such as socioeconomic status to motivate the importance of algorithmic fairness using the bike sharing program in pittsburgh pa i will present a position on how discrimination can be eliminated from the pipeline using bayesian optimization,http://arxiv.org/abs/2011.13988v2,2
reviewing contracts is a timeconsuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it in this work we propose documentlevel natural language inference nli for contracts a novel realworld application of nli that addresses such problems in this task a system is given a set of hypotheses such as some obligations of agreement may survive termination and a contract and it is asked to classify whether each hypothesis is entailed by contradicting to or not mentioned by neutral to the contract as well as identifying evidence for the decision as spans in the contract we annotated and release the largest corpus to date consisting of 607 annotated contracts we then show that existing models fail badly on our task and introduce a strong baseline which 1 models evidence identification as multilabel classification over spans instead of trying to predict start and end tokens and 2 employs more sophisticated context segmentation for dealing with long documents we also show that linguistic characteristics of contracts such as negations by exceptions are contributing to the difficulty of this task and that there is much room for improvement,http://arxiv.org/abs/2110.01799v1,2
we introduce and discuss a system of onedimensional kinetic equations describing the influence of higher education in the social stratification of a multiagent society the system is obtained by coupling a model for knowledge formation with a kinetic description of the social climbing in which the parameters characterizing the elementary interactions leading to the formation of a social elite are assumed to depend on the degree of knowledgeeducation of the agents in addition we discuss the case in which the education level of an individual is function of the position occupied in the social ranking with this last assumption we obtain a fully coupled model in which knowledge and social status influence each other in the last part we provide several numerical experiments highlighting the role of education in reducing social inequalities and in promoting social mobility,http://arxiv.org/abs/2110.08153v1,2
social stereotypes negatively impact individuals judgements about different groups and may have a critical role in how people understand language directed toward minority social groups here we assess the role of social stereotypes in the automated detection of hateful language by examining the relation between individual annotator biases and erroneous classification of texts by hate speech classifiers specifically in study 1 we investigate the impact of novice annotators stereotypes on their hatespeechannotation behavior in study 2 we examine the effect of languageembedded stereotypes on expert annotators aggregated judgements in a large annotated corpus finally in study 3 we demonstrate how languageembedded stereotypes are associated with systematic prediction errors in a neuralnetwork hate speech classifier our results demonstrate that hate speech classifiers learn humanlike biases which can further perpetuate social inequalities when propagated at scale this framework combining social psychological and computational linguistic methods provides insights into additional sources of bias in hate speech moderation informing ongoing debates regarding fairness in machine learning,http://arxiv.org/abs/2110.14839v1,2
the psychological costs of the attention economy are often considered through the binary of harmful design and healthy use with digital wellbeing chiefly characterised as a matter of personal responsibility this article adopts an interdisciplinary approach to highlight the empirical ideological and political limits of embedding this individualised perspective in computational discourses and designs of digital wellbeing measurement we will reveal wellbeing to be a culturally specific and environmentally conditioned concept and will problematize user engagement as a universal proxy for wellbeing instead the contributing factors of user wellbeing will be located in environing social cultural and political conditions far beyond the control of individual users alone in doing so we hope to reinvigorate the issue of digital wellbeing measurement as a nexus point of political concern through which multiple disciplines can study experiences of digital ill as symptomatic of wider social inequalities and capitalist relations of power,http://arxiv.org/abs/2203.08199v1,2
recent research has shown that seemingly fair machine learning models when used to inform decisions that have an impact on peoples lives or wellbeing eg applications involving education employment and lending can inadvertently increase social inequality in the long term this is because prior fairnessaware algorithms only consider static fairness constraints such as equal opportunity or demographic parity however enforcing constraints of this type may result in models that have negative longterm impact on disadvantaged individuals and communities we introduce elf enforcing longterm fairness the first classification algorithm that provides highconfidence fairness guarantees in terms of longterm or delayed impact we prove that the probability that elf returns an unfair solution is less than a userspecified tolerance and that under mild assumptions given sufficient training data elf is able to find and return a fair solution if one exists we show experimentally that our algorithm can successfully mitigate longterm unfairness,http://arxiv.org/abs/2208.11744v1,2
based on measure transportation ideas and the related concepts of centeroutward quantile functions we propose multipleoutput centeroutward generalizations of the traditional univariate concepts of lorenz and concentration functions and the related gini and kakwani coefficients these new concepts have a natural interpretation either in terms of contributions of central middleclass regions to the expectation of some variable of interest or in terms of the physical notions of work and energy which sheds new light on the nature of economic and social inequalities importantly the proposed concepts pave the way to statistically sound definitions based on multiple variables of quantiles and quantile regions and the concept of middle class of high relevance in various socioeconomic contexts,http://arxiv.org/abs/2211.10822v1,2
a simple heuristic model including the multiple exchanges between economic agents is used to explain the mechanism of emerging and maintenance of social inequality in the market economy the model allows calculating a density function of the population distribution over income the function can be considered as a strongly deformed gauss distribution function whereas at large incomes it coincides with the pareto distribution the external in relation to the model under consideration force is necessary to maintain the strong nonequilibrium in a stationary state and this force is the nonequivalence of elementary exchanges the agent who already receives the higher income has the advantage it provokes the rich to be getting more rich and the poor to be getting pauper,http://arxiv.org/abs/2302.10751v3,2
intersectionality is a critical framework that through inquiry and praxis allows us to examine how social inequalities persist through domains of structure and discipline given ai fairness raison detre of fairness we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness through a critical review of how intersectionality is discussed in 30 papers from the ai fairness literature we deductively and inductively 1 map how intersectionality tenets operate within the ai fairness paradigm and 2 uncover gaps between the conceptualization and operationalization of intersectionality we find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups they also fail to discuss their social context and when mentioning power they mostly situate it only within the ai pipeline we 3 outline and assess the implications of these gaps for critical inquiry and praxis and 4 provide actionable recommendations for ai fairness researchers to engage with intersectionality in their work by grounding it in ai epistemology,http://arxiv.org/abs/2303.17555v2,2
while machine learning can myopically reinforce social inequalities it may also be used to dynamically seek equitable outcomes in this paper we formalize longterm fairness in the context of online reinforcement learning this formulation can accommodate dynamical control objectives such as driving equity inherent in the state of a population that cannot be incorporated into static formulations of fairness we demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing shortterm incentives to drive a classifierpopulation system towards more desirable equilibria for the proposed setting we develop an algorithm that adapts recent work in online learning we prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness as statistical regularities between demographic groups we compare our proposed algorithm to the repeated retraining of myopic classifiers as a baseline and to a deep reinforcement learning algorithm that lacks safety guarantees our experiments model human populations according to evolutionary game theory and integrate realworld datasets,http://arxiv.org/abs/2304.09362v2,2
factors contributing to social inequalities are also associated with negative mental health outcomes leading to disparities in mental wellbeing we propose a bayesian hierarchical model which can evaluate the impact of policies on population wellbeing accounting for spatialtemporal dependencies building on an interrupted time series framework our approach can evaluate how different profiles of individuals are affected in different ways whilst accounting for their uncertainty we apply the framework to assess the impact of the united kingdoms welfare reform which took place throughout the 2010s on mental wellbeing using data from the uk household longitudinal study the additional depth of knowledge is essential for effective evaluation of current policy and implementation of future policy,http://arxiv.org/abs/2306.15525v1,2
individuals sociodemographic and economic characteristics crucially shape the spread of an epidemic by largely determining the exposure level to the virus and the severity of the disease for those who got infected while the complex interplay between individual characteristics and epidemic dynamics is widely recognized traditional mathematical models often overlook these factors in this study we examine two important aspects of human behavior relevant to epidemics contact patterns and vaccination uptake using data collected during the covid19 pandemic in hungary we first identify the dimensions along which individuals exhibit the greatest variation in their contact patterns and vaccination attitudes we find that generally privileged groups of the population have higher number of contact and a higher vaccination uptake with respect to disadvantaged groups subsequently we propose a datadriven epidemiological model that incorporates these behavioral differences finally we apply our model to analyze the fourth wave of covid19 in hungary providing valuable insights into realworld scenarios by bridging the gap between individual characteristics and epidemic spread our research contributes to a more comprehensive understanding of disease dynamics and informs effective public health strategies,http://arxiv.org/abs/2307.04865v1,2
accessibility of different places such as hospitals or areas with jobs is important in understanding transportation systems urban environments and potential inequalities in what services and opportunities different people can reach often research in this area is framed around the question of whether people living in an area are able to reach certain destinations within a prespecified time frame however the cost of such journeys and whether they are affordable is often omitted or not considered to the same level here we present a python package and an associated data set which allows to analyse the cost of train journeys in great britain we present the original data set we used to construct this the python package we developed to analyse it and the output data set which we generated we envisage our work to allow researchers policy makers and other stakeholders to investigate questions around the cost of train journeys any geographical or social inequalities arising from this and how the transport system could be improved,http://arxiv.org/abs/2310.19754v1,2
intelligent software systems eg conversational agents profiling systems recruitment systems are often designed in a manner which may perpetuates antiblack racism and other forms of sociocultural discrimination this may reinforce social inequities by supporting the automation of consequential and sometimes unfair decisions that may be made by such systems and which may have an adverse impact on credit scores insurance payouts and even health evaluations just to name a few my lightning talk will therefore emphasize the need to propose a new type of nonfunctional requirements called eci emotional and cultural intelligence requirements that will aim at developing discriminationaware intelligent software systems such systems will notably be able to behave empathetically toward everyone including minoritized groups and will ensure they are treated fairly my talk will also emphasize the need to develop novel system assurance solutions to assure these eci requirements are sufficiently supported by intelligent software systems,http://arxiv.org/abs/2311.08431v2,2
in critical machine learning applications ensuring fairness is essential to avoid perpetuating social inequities in this work we address the challenges of reducing bias and improving accuracy in datascarce environments where the cost of collecting labeled data prohibits the use of large labeled datasets in such settings active learning promises to maximize marginal accuracy gains of small amounts of labeled data however existing applications of active learning for fairness fail to deliver on this typically requiring large labeled datasets or failing to ensure the desired fairness tolerance is met on the population distribution to address such limitations we introduce an innovative active learning framework that combines an exploration procedure inspired by posterior sampling with a fair classification subroutine we demonstrate that this framework performs effectively in very datascarce regimes maximizing accuracy while satisfying fairness constraints with high probability we evaluate our proposed approach using wellestablished realworld benchmark datasets and compare it against stateoftheart methods demonstrating its effectiveness in producing fair models and improvement over existing methods,http://arxiv.org/abs/2312.08559v1,2
recent conversations in the algorithmic fairness literature have raised several concerns with standard conceptions of fairness first constraining predictive algorithms to satisfy fairness benchmarks may lead to nonoptimal outcomes for disadvantaged groups second technical interventions are often ineffective by themselves especially when divorced from an understanding of structural processes that generate social inequality inspired by both these critiques we construct a common decisionmaking model using mortgage loans as a running example we show that under some conditions any choice of decision threshold will inevitably perpetuate existing disparities in financial stability unless one deviates from the pareto optimal policy then we model the effects of three different types of interventions we show how different interventions are recommended depending upon the difficulty of enacting structural change upon external parameters and depending upon the policymakers preferences for equity or efficiency counterintuitively we demonstrate that preferences for efficiency over equity may lead to recommendations for interventions that target the underresourced group finally we simulate the effects of interventions on a dataset that combines hmda and fannie mae loan data this research highlights the ways that structural inequality can be perpetuated by seemingly unbiased decision mechanisms and it shows that in many situations technical solutions must be paired with external contextaware interventions to enact social change,http://arxiv.org/abs/2406.01323v1,2
there is an ongoing debate on balancing the benefits and risks of artificial intelligence ai as ai is becoming critical to improving healthcare delivery and patient outcomes such improvements are essential in resourceconstrained settings where millions lack access to adequate healthcare services such as in africa ai in such a context can potentially improve the effectiveness efficiency and accessibility of healthcare services nevertheless the development and use of aidriven healthcare systems raise numerous ethical legal and socioeconomic issues justice is a major concern in ai that has implications for amplifying social inequities this paper discusses these implications and related justice concepts such as solidarity common good sustainability ai bias and fairness for africa to effectively benefit from ai these principles should align with the local context while balancing the risks compared to mainstream ethical debates on justice this perspective offers contextspecific considerations for equitable healthcare ai development in africa,http://arxiv.org/abs/2406.10653v1,2
the sharply increasing sizes of artificial intelligence ai models come with significant energy consumption and environmental footprints which can disproportionately impact certain often marginalized regions and hence create environmental inequity concerns moreover concerns with social inequity have also emerged as ai computing resources may not be equitably distributed across the globe and users from certain disadvantaged regions with severe resource constraints can consistently experience inferior model performance importantly the inequity concerns that encompass both social and environmental dimensions still remain unexplored and have increasingly hindered responsible ai in this paper we leverage the spatial flexibility of ai inference workloads and propose equitable geographical load balancing glb to fairly balance ais regional social and environmental costs concretely to penalize the disproportionately high social and environmental costs for equity we introduce lq norms as novel regularization terms into the optimization objective for glb decisions our empirical results based on realworld ai inference traces demonstrate that while the existing glb algorithms result in disproportionately large social and environmental costs in certain regions our proposed equitable glb can fairly balance ais negative social and environmental costs across all the regions,http://arxiv.org/abs/2407.05176v1,2
data science pipelines inform and influence many daily decisions from what we buy to who we work for and even where we live when designed incorrectly these pipelines can easily propagate social inequity and harm traditional solutions are technical in nature eg mitigating biased algorithms in this vision paper we introduce a novel lens for promoting responsible data science using theories of behavior change that emphasize not only technical solutions but also the behavioral responsibility of practitioners by integrating behavior change theories from cognitive psychology with data science workflow knowledge and ethics guidelines we present a new perspective on responsible data science we present example data science interventions in machine learning and visual data analysis contextualized in behavior change theories that could be implemented to interrupt and redirect potentially suboptimal or negligent practices while reinforcing ethically conscious behaviors we conclude with a call to action to our community to explore this new research area of behavior change interventions for responsible data science,http://arxiv.org/abs/2410.17273v1,2
liking it or not ready or not we are likely to enter a new phase of human history in which artificial intelligence ai will dominate economic production and social life the ai revolution before the actual arrival of the ai revolution it is time for us to speculate on how ai will impact the social world in this article we focus on the social impact of generative llmbased ai gellmai discussing societal factors that contribute to its technological development and its potential roles in enhancing both betweencountry and withincountry social inequality there are good indications that the us and china will lead the field and will be the main competitors for domination of ai in the world we conjecture the ai revolution will likely give rise to a postknowledge society in which knowledge per se will become less important than in todays world instead individual relationships and social identity will become more important so will soft skills,http://arxiv.org/abs/2410.21281v1,2
dialectal arabic da varieties are underserved by language technologies particularly large language models llms this trend threatens to exacerbate existing social inequalities and limits llm applications yet the research community lacks operationalized performance measurements in da we present a framework that comprehensively assesses llms da modeling capabilities across four dimensions fidelity understanding quality and diglossia we evaluate nine llms in eight da varieties and provide practical recommendations our evaluation suggests that llms do not produce da as well as they understand it not because their da fluency is poor but because they are reluctant to generate da further analysis suggests that current posttraining can contribute to bias against da that fewshot examples can overcome this deficiency and that otherwise no measurable features of input text correlate well with llm da performance,http://arxiv.org/abs/2412.04193v2,2
this paper investigates the impact of social discount rate sdr choice on intergenerational equity issues caused by publicprivate partnerships ppps projects indeed more ppps mean more debt being accumulated for future generations leading to a fiscal deficit crisis the paper draws on how the sdr level taken today distributes societies on the social welfare function swf this is done by answering two subquestions i what is the risk of ppps debts being offbalance sheet ii how do public policies based on the envisaged sdr position society within different ethical perspectives the answers are obtained from a discussion of the different sdrs applied in the uk for examples according to the merits of the pertinent ethical theories namely libertarian egalitarian utilitarian and rawlsian we find that public policymakers can manipulate the sdr to make ppps looking like a better option than the traditional financing form however this antagonises the value for money principle we also point out that public policy is not harmonised with ethical theories we find that at present in the uk the sdr is somewhere between weighted utilitarian and rawlsian societies in the tradeoff curve alas our study finds no evidence that the uk government is using a sophisticated system to keep pace with the accumulated offbalance sheet debts thus the exact prediction of the final state is hardly made because of the uncertainty factor we conclude that our study hopefully provides a good analytical framework for policymakers in order to draw on the merits of ethical theories before initiating public policies like ppps,http://arxiv.org/abs/2201.09064v1,2
the improbability scale is is proposed as a way of communicating to the general public the improbability and by implication the probability of events predicted as the result of scientific research through the use of the improbability scale the public will be able to evaluate more easily the relative risks of predicted events and draw proper conclusions when asked to support governmental and public policy decisions arising from that research,http://arxiv.org/abs/physics/0503229v2,2
besides an indicator of the gdp the central bank of venezuela generates the so called monthly economic activity general indicator the a priori knowledge of this indicator which represents and sometimes even anticipates the economys fluctuations could be helpful in developing public policies and in investment decision making the purpose of this study is forecasting the igaem through non parametric methods an approach that has proven effective in a wide variety of problems in economics and finance,http://arxiv.org/abs/0708.3463v1,2
scientific publications and other genres of research output are increasingly being cited in policy documents citations in documents of this nature could be considered a critical indicator of the significance and societal impact of the research output in this study we built classification models that predict whether a particular research work is likely to be cited in a public policy document based on the attention it received online primarily on social media platforms we evaluated the classifiers based on their accuracy precision and recall values we found that random forest and multinomial naive bayes classifiers performed better overall,http://arxiv.org/abs/1706.04140v1,2
risk including economic risk is increasingly a concern for public policy and management the possibility of dealing effectively with risk is hampered however by lack of a sound empirical basis for risk assessment and management the paper demonstrates the general point for cost and demand risks in urban rail projects the paper presents empirical evidence that allow valid economic risk assessment and management of urban rail projects including benchmarking of individual or groups of projects benchmarking of the copenhagen metro is presented as a case in point the approach developed is proposed as a model for other types of policies and projects in order to improve economic and financial risk assessment and management in policy and planning,http://arxiv.org/abs/1303.7402v1,2
public policy making has direct and indirect impacts on social behaviors however using system dynamics model alone to assess these impacts fails to consider the interactions among social elements thus may produce doubtful conclusions in this study we examine the political science theory of greed and grievance in modeling civil conflicts an agentbased model is built based on an existing rebellion model in netlogo the modifications and improvements in our model are elaborated several case studies are used to demonstrate the use of our model for investigating emergent phenomena and implications of governmental policies,http://arxiv.org/abs/1908.06883v1,2
